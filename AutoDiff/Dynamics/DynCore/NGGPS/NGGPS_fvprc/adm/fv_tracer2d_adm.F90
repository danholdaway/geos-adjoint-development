!        Generated by TAPENADE     (INRIA, Ecuador team)
!  Tapenade 3.12 (r6213) - 13 Oct 2016 10:30
!
MODULE FV_TRACER2D_MOD_B
  USE TP_CORE_MOD_B, ONLY : fv_tp_2d, fv_tp_2d_adm, fv_tp_2d, &
& fv_tp_2d_adm, copy_corners, copy_corners_adm
  USE FV_MP_MOD_B, ONLY : mp_reduce_max
  USE FV_MP_MOD_B, ONLY : ng, mp_gather, is_master
  USE FV_MP_MOD_B, ONLY : group_halo_update_type
  USE FV_MP_MOD_B, ONLY : start_group_halo_update, &
& start_group_halo_update_adm, complete_group_halo_update
  USE MPP_DOMAINS_MOD_B, ONLY : mpp_update_domains, &
& mpp_update_domains_adm, cgrid_ne, domain2d, mpp_get_boundary, &
& mpp_get_boundary_adm
  USE FV_TIMING_MOD, ONLY : timing_on, timing_off
!   use boundary_mod,      only: nested_grid_BC_apply, nested_grid_BC_apply_intT
  USE FV_ARRAYS_MOD_B, ONLY : fv_grid_type, fv_nest_type, fv_atmos_type,&
& fv_grid_bounds_type, real4, real8, 8
  USE MPP_MOD_B, ONLY : mpp_error, fatal, mpp_broadcast, mpp_send, &
& mpp_recv, mpp_sum, mpp_max
  IMPLICIT NONE
!#ifdef FLUXBCS
!
!!! nestbctype == 2: use just coarse-grid fluxes on nested grid.
!!! nestbctype == 3: use incoming fluxes: when flow is into
!!!                  nested grid, nested grid uses coarse-grid fluxes,
!!!                  and when flow is out of nested grid, coarse grid
!!!                  uses nested-grid fluxes.
!!! nestbctype == 4: use just nested-grid fluxes on coarse grid
!
!!! Note that to ensure conservation when using the mass-conserving
!!! remap BC, the vertical remapping must not change the column's
!!! tracer mass. See fv_dynamics.F90 .
!
!!! The new way (to be implemented):
!!! 1. on first split timestep, send coarse grid fluxes to nested grid.
!!! 2. Split up coarse grid fluxes onto nested-grid faces.
!!! 3. Run tracer advection on both grids, accumulating the boundary fluxes
!!!    on both grids, for all split timesteps except the last on the coarse grid.
!!! 4. If two-way BCs, send accumulated nested-grid fluxes to coarse grid
!!! 5. Do correction step on coarse grid: add difference in to fluxes taken on
!!!    last coarse-grid split timestep
!!!
!!! For two-way BCs we do not replace an incoming flux with an outgoing flux.
! !! (Should we also not INCREASE any outgoing flux?)
!
!subroutine flux_BCs(fx, fy, it, msg_split_steps, npx, npy, npz, nq, &
!     q, dp1, dp2, cx, cy, nested, neststruct, parent_grid)
!
!  integer, intent(IN) :: it, msg_split_steps, npx, npy, npz, nq
!
!  real(FVPRC), intent(INOUT) :: fx(is:ie+1,js:je  ,npz,nq)
!  real(FVPRC), intent(INOUT) :: fy(is:ie , js:je+1,npz,nq)
!  real(FVPRC), intent(INOUT) :: q(isd:ied,jsd:jed,npz,nq)
!  real(FVPRC), intent(IN) :: dp1(is:ie,js:je,npz)
!  real(FVPRC), intent(IN) :: dp2(is:ie,js:je,npz)
!  real(FVPRC), intent(IN) :: cx(is:ie+1,jsd:jed  ,npz)  ! Courant Number X-Dir
!  real(FVPRC), intent(IN) ::  cy(isd:ied,js :je +1,npz)  ! Courant Number Y-Dir
!
!  logical, intent(IN) :: nested
!
!  type(fv_nest_type), intent(INOUT), target :: neststruct
!  type(fv_atmos_type), intent(INOUT) :: parent_grid
!
!  real(FVPRC), save, allocatable, dimension(:,:,:) :: coarse_fluxes_west, coarse_fluxes_east, coarse_fluxes_south, coarse_fluxes
!_north
!  real(FVPRC), allocatable, dimension(:,:,:) :: nested_data_west, nested_data_east, nested_data_south, nested_data_north
!
!  real(FVPRC) :: ebuffer(js:je,npz,nq), wbuffer(js:je,npz,nq)
!  real(FVPRC) :: sbuffer(is:ie,npz,nq), nbuffer(is:ie,npz,nq)
!
!  !Divided fluxes for nested-grid boundary
!  real(FVPRC) :: fxWd(npy-1,npz,nq)
!  real(FVPRC) :: fxEd(npy-1,npz,nq)
!  real(FVPRC) :: fySd(npx-1,npz,nq)
!  real(FVPRC) :: fyNd(npx-1,npz,nq)
!!!$  real(FVPRC) :: fxWd(jsd:jed,npz,nq)
!!!$  real(FVPRC) :: fxEd(jsd:jed,npz,nq)
!!!$  real(FVPRC) :: fySd(isd:ied,npz,nq)
!!!$  real(FVPRC) :: fyNd(isd:ied,npz,nq)
!
!  integer n, iq, i, j, k, ic, jc
!  integer js1, je1, is1, ie1
!  real(FVPRC) fluxsplit, corr
!
!  integer, parameter :: iflux_div = 3
!
!  integer, pointer :: refinement, nestbctype, ioffset, joffset
!  logical, pointer :: do_flux_BCs, do_2way_flux_BCs
!
!  logical, dimension(:), pointer :: child_grids
!
!      integer :: is,  ie,  js,  je
!      integer :: isd, ied, jsd, jed
!
!      is  = bd%is
!      ie  = bd%ie
!      js  = bd%js
!      je  = bd%je
!      isd = bd%isd
!      ied = bd%ied
!      jsd = bd%jsd
!      jed = bd%jed
!
!  !Note that sends and receipts are only done when mod(it,msg_split_steps) == 1 or 0
!
!  !if (.not. concurrent) call mpp_error(FATAL, "Nested flux BCs can only be used with CONCURRENT nesting.")
!
!  !! Set up local pointers
!  refinement          => neststruct%refinement
!  do_flux_BCs         => neststruct%do_flux_BCs
!  do_2way_flux_BCs    => neststruct%do_2way_flux_BCs
!  nestbctype          => neststruct%nestbctype
!
!  ioffset     => neststruct%ioffset     
!  joffset     => neststruct%joffset     
!  child_grids => neststruct%child_grids 
!
!  if (.not. allocated(nest_fx_west_accum)) then
!
!     if (is == 1) then
!        allocate(nest_fx_west_accum(js:je,npz,nq))
!     else
!        allocate(nest_fx_west_accum(1,1,1))
!     endif
!     if (ie == npx-1) then
!        allocate(nest_fx_east_accum(js:je,npz,nq))
!     else
!        allocate(nest_fx_east_accum(1,1,1))
!     endif
!
!     if (js == 1) then
!        allocate(nest_fx_south_accum(is:ie,npz,nq))
!     else
!        allocate(nest_fx_south_accum(1,1,1))
!     endif
!     if (je == npy-1) then
!        allocate(nest_fx_north_accum(is:ie,npz,nq))
!     else
!        allocate(nest_fx_north_accum(1,1,1))
!     endif
!
!  end if
!
!  !NOTE: changing bounds on coarse_fluxes* to include one extra point at each end
!  !      changes answers for nestbctype == 2, but not for 3
!  if (nested .and. .not. allocated(coarse_fluxes_west) .and. nestbctype /= 4) then
!     allocate(coarse_fluxes_west(-1:npy/refinement+2,npz,nq))
!     allocate(coarse_fluxes_east(-1:npy/refinement+2,npz,nq))
!     allocate(coarse_fluxes_south(-1:npx/refinement+2,npz,nq))
!     allocate(coarse_fluxes_north(-1:npx/refinement+2,npz,nq))
!!!$     allocate(coarse_fluxes_west(0:npy/refinement+1,npz,nq))
!!!$     allocate(coarse_fluxes_east(0:npy/refinement+1,npz,nq))
!!!$     allocate(coarse_fluxes_south(0:npx/refinement+1,npz,nq))
!!!$     allocate(coarse_fluxes_north(0:npx/refinement+1,npz,nq))
!  endif
!
!!!! Flux BCs: Do transfers, if necessary
!  if ((do_flux_BCs .or. nested)) then
!
!     if (nested) then
!
!        if ( (mod(it,msg_split_steps) == 1 .or. msg_split_steps == 1) .and. nestbctype /= 4 ) then
!        !if (  it == 1 .and. nestbctype /= 4 ) then
!           coarse_fluxes_west = 0.
!           coarse_fluxes_east = 0.
!           coarse_fluxes_south = 0.
!           coarse_fluxes_north = 0.
!
!           !Receive coarse-grid fluxes; save between timesteps
!
!                        call timing_on('COMM_TOTAL')
!                            call timing_on('COMM_TRACER')
!           call mpp_sum(coarse_fluxes_west,  size(coarse_fluxes_west),  (/ parent_grid%pelist, pelist /) )
!           call mpp_sum(coarse_fluxes_east,  size(coarse_fluxes_east),  (/ parent_grid%pelist, pelist /) )
!           call mpp_sum(coarse_fluxes_south, size(coarse_fluxes_south), (/ parent_grid%pelist, pelist /) )
!           call mpp_sum(coarse_fluxes_north, size(coarse_fluxes_north), (/ parent_grid%pelist, pelist /) )
!                           call timing_off('COMM_TRACER')
!                       call timing_off('COMM_TOTAL')
!
!        endif
!
!        is1 = max(is,1)
!        ie1 = min(ie,npx-1)
!        js1 = max(js,1)
!        je1 = min(je,npy-1)
!
!        fluxsplit = 1./real(refinement*msg_split_steps)
!
!        !Divide up fluxes depending on the division method
!        select case (iflux_div)
!        case (1)
!           if (is == 1) then
!              do iq=1,nq
!              do k=1,npz
!              do j=max(js,1),min(je,npy-1)
!                 fxWd(j,k,iq) = coarse_fluxes_west((j-1)/refinement + 1,k,iq)*fluxsplit
!              enddo
!              enddo
!              enddo
!           endif
!           if (ie == npx-1) then
!              do iq=1,nq
!              do k=1,npz
!              do j=max(js,1),min(je,npy-1)
!                 fxEd(j,k,iq) = coarse_fluxes_east((j-1)/refinement + 1,k,iq)*fluxsplit
!              enddo
!              enddo
!              enddo
!           endif
!           if (js == 1) then
!              do iq=1,nq
!              do k=1,npz
!              do i=max(is,1),min(ie,npx-1)
!                 fySd(i,k,iq) = coarse_fluxes_south((i-1)/refinement + 1,k,iq)*fluxsplit
!              enddo
!              enddo
!              enddo
!           endif
!           if (je == npy-1) then
!              do iq=1,nq
!              do k=1,npz
!              do i=max(is,1),min(ie,npx-1)
!                 fyNd(i,k,iq) = coarse_fluxes_north((i-1)/refinement + 1,k,iq)*fluxsplit
!              enddo
!              enddo
!              enddo
!           endif
!
!        case (2)
!           if (is == 1) then
!              call PLM_flux_division(coarse_fluxes_west, fxWd, npy/refinement, &
!                   npy-1, npz, nq, refinement, msg_split_steps)
!           endif
!           if (ie == npx-1) then
!              call PLM_flux_division(coarse_fluxes_east, fxEd, npy/refinement, &
!                   npy-1, npz, nq, refinement, msg_split_steps)
!           endif
!           if (js == 1) then
!              call PLM_flux_division(coarse_fluxes_south, fySd, npx/refinement, &
!                   npx-1, npz, nq, refinement, msg_split_steps)
!           endif
!           if (je == npy-1) then
!              call PLM_flux_division(coarse_fluxes_north, fyNd, npx/refinement, &
!                   npx-1, npz, nq, refinement, msg_split_steps)
!           endif
!
!        case (3)
!           if (is == 1) then
!              call PPM_flux_division(coarse_fluxes_west, fxWd, npy/refinement, &
!                   npy-1, npz, nq, refinement, msg_split_steps)
!           endif
!           if (ie == npx-1) then
!              call PPM_flux_division(coarse_fluxes_east, fxEd, npy/refinement, &
!                   npy-1, npz, nq, refinement, msg_split_steps)
!           endif
!           if (js == 1) then
!              call PPM_flux_division(coarse_fluxes_south, fySd, npx/refinement, &
!                   npx-1, npz, nq, refinement, msg_split_steps)
!           endif
!           if (je == npy-1) then
!              call PPM_flux_division(coarse_fluxes_north, fyNd, npx/refinement, &
!                   npx-1, npz, nq, refinement, msg_split_steps)
!           endif
!
!        case DEFAULT
!           call mpp_error(FATAL, 'iflux_div value not implemented')
!        end select
!
!        !replace nested-grid fluxes as desired; note that grids are aligned, making this easier.
!
!        if (nestbctype == 2) then
!           if (is == 1) then
!              do iq=1,nq
!              do k=1,npz
!              do j=js1,je1
!                 fx(1,j,k,iq) = fxWd(j,k,iq) !coarse_fluxes_west((j-1)/refinement + 1,k,iq)*fluxsplit
!              enddo
!              enddo
!              enddo
!           endif
!           if (ie == npx-1) then
!              do iq=1,nq
!              do k=1,npz
!              do j=js1,je1
!                 fx(npx,j,k,iq) = fxEd(j,k,iq) !coarse_fluxes_east((j-1)/refinement + 1,k,iq)*fluxsplit
!              enddo
!              enddo
!              enddo
!           endif
!
!           if (js == 1) then
!              do iq=1,nq
!              do k=1,npz
!              do i=is1,ie1
!                 fy(i,1,k,iq) = fySd(i,k,iq) !coarse_fluxes_south((i-1)/refinement + 1,k,iq)*fluxsplit
!              enddo
!              enddo
!              enddo
!           endif
!           if (je == npy-1) then
!              do iq=1,nq
!              do k=1,npz
!              do i=is1,ie1
!                 fy(i,npy,k,iq) = fyNd(i,k,iq) !coarse_fluxes_north((i-1)/refinement + 1,k,iq)*fluxsplit
!              enddo
!              enddo
!              enddo
!           endif 
!       else if (nestbctype == 3) then
!          ! At the boundary, use the flux directed outward from the
!          ! grid it is from. In particular a cell's inward flux
!          ! (regardless of whether that cell is on the nested or the
!          ! coarse grid) should never be replaced by an outward flux,
!          ! lest positivity be violated. If BOTH fluxes are directed
!          ! inward from their grid, set the boundary flux to
!          ! zero. When both are directed outward, the choice is
!          ! arbitrary; use the flux of smaller magnitude. If both are in
!          ! the same direction we use the smaller of the fluxes.
!
!           if (is == 1) then
!              do iq=1,nq
!              do k=1,npz
!              do j=js,je
!                 if (fxWd(j,k,iq)*fx(1,j,k,iq) <= 0.) then
!                    !Two incoming fluxes => set to zero
!                    !This could most efficiently be represented by
!                    ! fx = sign(min(fxWd,-fx,0),-(fxwd+fx)) 
!                    ! but I have not yet tested this
!                    if (fxWd(j,k,iq) < 0.) then
!                       fx(1,j,k,iq) = 0.
!                    else
!                       fx(1,j,k,iq) = sign(min(fxWd(j,k,iq),-fx(1,j,k,iq)),-(fxWd(j,k,iq)+fx(1,j,k,iq)))
!                    endif
!                 else
!                    !Go with the flux of smaller magnitude
!                    fx(1,j,k,iq) = sign(min(abs(fx(1,j,k,iq)),abs(fxWd(j,k,iq))), fx(1,j,k,iq))
!                 endif
!!!$                 jc = (j-1)/refinement + 1
!!!$                 if ( cx(1,j,k) > 0. ) then
!!!$                    fx(1,j,k,iq) = fxWd(j,k,iq)
!!!$                 endif
!              enddo
!              enddo
!              enddo
!           endif
!           if (ie == npx-1) then
!              do iq=1,nq
!              do k=1,npz
!              do j=js,je
!                 if (fxEd(j,k,iq)*fx(npx,j,k,iq) <= 0.) then
!                    if (fxEd(j,k,iq) > 0.) then
!                       fx(npx,j,k,iq) = 0.
!                    else
!                       fx(npx,j,k,iq) = sign(min(-fxEd(j,k,iq),fx(npx,j,k,iq)),-(fxEd(j,k,iq)+fx(npx,j,k,iq)))
!                    endif
!                 else
!                    !Go with the flux of smaller magnitude
!                    fx(npx,j,k,iq) = sign(min(abs(fx(npx,j,k,iq)),abs(fxEd(j,k,iq))), fx(npx,j,k,iq))
!                 endif
!!!$                 jc = (j-1)/refinement + 1
!!!$                 if ( cx(npx,j,k) < 0. ) then
!!!$                    fx(npx,j,k,iq) = fxEd(j,k,iq)
!!!$                 endif
!              enddo
!              enddo
!              enddo
!           endif
!
!           if (js == 1) then
!              do iq=1,nq
!              do k=1,npz
!              do i=is,ie
!                 if (fySd(i,k,iq)*fy(i,1,k,iq) <= 0.) then
!                    if (fySd(i,k,iq) < 0.) then
!                       fy(i,1,k,iq) = 0.
!                    else
!                       fy(i,1,k,iq) = sign(min(fySd(i,k,iq),-fy(i,1,k,iq)),-(fySd(i,k,iq)+fy(i,1,k,iq)))
!                    endif
!                 else
!                    !Go with the flux of smaller magnitude
!                    fy(i,1,k,iq) = sign(min(abs(fy(i,1,k,iq)),abs(fySd(i,k,iq))), fy(i,1,k,iq))
!                 endif
!!!$                 ic = (i-1)/refinement + 1
!!!$                 if ( cy(i,1,k) > 0. ) then
!!!$                    fy(i,1,k,iq) = fySd(i,k,iq)
!!!$                 endif
!              enddo
!              enddo
!              enddo
!           endif
!           if (je == npy-1) then
!              do iq=1,nq
!              do k=1,npz
!              do i=is,ie
!                 if (fyNd(i,k,iq)*fy(i,npy,k,iq) <= 0.) then
!                    if (fyNd(i,k,iq) > 0.) then
!                       fy(i,npy,k,iq) = 0.
!                    else
!                       fy(i,npy,k,iq) = sign(min(-fyNd(i,k,iq),fy(i,npy,k,iq)),-(fyNd(i,k,iq)+fy(i,npy,k,iq)))
!                    endif
!                 else
!                    !Go with the flux of smaller magnitude
!                    fy(i,npy,k,iq) = sign(min(abs(fy(i,npy,k,iq)),abs(fyNd(i,k,iq))), fy(i,npy,k,iq))
!                 endif
!!!$                 ic = (i-1)/refinement + 1
!!!$                 if ( cy(i,npy,k) < 0. ) then
!!!$                    fy(i,npy,k,iq) = fyNd(i,k,iq)
!!!$                 endif
!              enddo
!              enddo
!              enddo
!           endif            
!        else if (nestbctype == 5) then
!           !Same as 2, except we do the same test as before: never replace an incoming flux with an outgoing flux
!           !ie. if fx(i,j,k) <0. .or. coarse(i,j,k) > 0. then...
!           !(Not fully implemented)
!           if (is == 1) then
!              do iq=1,nq
!              do k=1,npz
!                 if (js == 1) then
!                    if (fx(1,1,k,iq) < 0. .or. coarse_fluxes_west(1,k,iq) > 0.) &
!                         fx(1,1,k,iq) =  coarse_fluxes_west(1,k,iq)*fluxsplit
!                    if (fy(1,1,k,iq) < 0. .or. coarse_fluxes_south(1,k,iq) > 0.) &
!                         fy(1,1,k,iq) =  coarse_fluxes_south(1,k,iq)*fluxsplit
!                 endif
!                 if (je == npy-1) then
!                    if (fx(1,npy-1,k,iq) < 0. .or. coarse_fluxes_west(npy/refinement,k,iq) > 0.) &
!                         fx(1,npy-1,k,iq) =  coarse_fluxes_west(npy/refinement,k,iq)*fluxsplit
!                    if (fy(1,npy,k,iq) > 0. .or. coarse_fluxes_north(1,k,iq) < 0.) &
!                         fy(1,npy,k,iq) =  coarse_fluxes_north(1,k,iq)*fluxsplit
!                 endif
!              do j=js1,je1
!                 if (fx(1,j,k,iq) < 0. .or. coarse_fluxes_west((j-1)/refinement + 1,k,iq) > 0.) &
!                         fx(1,j,k,iq) = coarse_fluxes_west((j-1)/refinement + 1,k,iq)*fluxsplit
!              enddo
!              enddo
!              enddo
!           endif
!           if (ie == npx-1) then
!              do iq=1,nq
!              do k=1,npz
!                 if (js == 1) then
!                    if (fx(npx,1,k,iq) > 0. .or. coarse_fluxes_east(1,k,iq) < 0.) &
!                         fx(npx,1,k,iq) =  coarse_fluxes_east(1,k,iq)*fluxsplit
!                    if (fy(npx-1,1,k,iq) < 0. .or. coarse_fluxes_south(npx/refinement,k,iq) > 0.) &
!                         fy(npx-1,1,k,iq) =  coarse_fluxes_south(npx/refinement,k,iq)*fluxsplit
!                 endif
!                 if (je == npy-1) then
!                    if (fx(npx,npy-1,k,iq) > 0. .or. coarse_fluxes_east(npy/refinement,k,iq) < 0.) &
!                         fx(npx,npy-1,k,iq) =  coarse_fluxes_east(npy/refinement,k,iq)*fluxsplit
!                    if (fy(npx-1,npy,k,iq) > 0. .or. coarse_fluxes_north(npx/refinement,k,iq) < 0.) &
!                         fy(npx-1,npy,k,iq) =  coarse_fluxes_north(npx/refinement,k,iq)*fluxsplit
!                 endif
!              do j=js1,je1
!                 if (fx(1,j,k,iq) > 0. .or. coarse_fluxes_west((j-1)/refinement + 1,k,iq) < 0.) &
!                         fx(npx,j,k,iq) = coarse_fluxes_east((j-1)/refinement + 1,k,iq)*fluxsplit
!              enddo
!              enddo
!              enddo
!           endif
!
!           if (js == 1) then
!              do iq=1,nq
!              do k=1,npz
!              do i=is1,ie1
!                 fy(i,1,k,iq) = coarse_fluxes_south((i-1)/refinement + 1,k,iq)*fluxsplit
!              enddo
!              enddo
!              enddo
!           endif
!           if (je == npy-1) then
!              do iq=1,nq
!              do k=1,npz
!              do i=is1,ie1
!                 fy(i,npy,k,iq) = coarse_fluxes_north((i-1)/refinement + 1,k,iq)*fluxsplit
!              enddo
!              enddo
!              enddo
!           endif 
!        else
!           call mpp_error(FATAL, 'nestbctype used is not supported.')
!        endif
!
!     endif
!
!  !For TWO-WAY flux BCs:
!  !if (nested .and. (nestbctype == 3 .or. nestbctype == 4)) then
!  if ( nested ) then
!     
!     !Nested-grid: ACCUMULATE boundary fluxes at outflow points
!
!     if (mod(it,msg_split_steps) == 1 .or. msg_split_steps == 1) then
!        nest_fx_west_accum  = 0.
!        nest_fx_east_accum  = 0.
!        nest_fx_south_accum = 0.
!        nest_fx_north_accum = 0.
!     endif
!
!     if (is == 1)     then
!        do iq=1,nq
!        do k=1,npz
!        do j=js,je
!           nest_fx_west_accum(j,k,iq)  = nest_fx_west_accum(j,k,iq)  + fx(1,j,k,iq)
!        enddo
!        enddo
!        enddo
!     endif
!     if (ie == npx-1)     then
!        do iq=1,nq
!        do k=1,npz
!        do j=js,je
!           nest_fx_east_accum(j,k,iq)  = nest_fx_east_accum(j,k,iq)  + fx(npx,j,k,iq)
!        enddo
!        enddo
!        enddo
!     endif
!
!     if (js == 1)     then
!        do iq=1,nq
!        do k=1,npz
!        do i=is,ie
!           nest_fx_south_accum(i,k,iq)  = nest_fx_south_accum(i,k,iq)  + fy(i,1,k,iq)
!        enddo
!        enddo
!        enddo
!     endif
!     if (je == npy-1)     then
!        do iq=1,nq
!        do k=1,npz
!        do i=is,ie
!           nest_fx_north_accum(i,k,iq)  = nest_fx_north_accum(i,k,iq)  + fy(i,npy,k,iq)
!        enddo
!        enddo
!        enddo
!     endif
!
!  endif
!
!     !Send coarse-grid boundary fluxes to nested grid
!     if (do_flux_BCs .and.  (mod(it,msg_split_steps) == 1 .or. msg_split_steps == 1) .and. nestbctype /= 4) then
!        do n=1,size(child_grids)
!           if (child_grids(n) .and. nestbctype > 1 .and. nestbctype /= 4) &
!                call send_coarse_fluxes(fx, fy, Atm(n)%npx, Atm(n)%npy, ioffset, joffset, &
!                refinement, Atm(n)%pelist, Atm(n)%npes_this_grid, npx, npy, npz, nq, neststruct%parent_tile)
!        enddo
!     endif
!
!  endif
!
!!  if ((do_2way_flux_BCs .or. (nested .and. (nestbctype == 3 .or. nestbctype == 4))) ) then
!  if (do_2way_flux_BCs .or. nested ) then
!
!!!$     !Flux check
!!!$     if (nested) then
!!!$        sumW = 0.
!!!$        sumE = 0.
!!!$        sumS = 0.
!!!$        sumN = 0.
!!!$        if (is == 1)     sumW = sum(nest_fx_west_accum(js:je,:,1))
!!!$        if (ie == npx-1) sumE = sum(nest_fx_east_accum(js:je,:,1))
!!!$        if (js == 1)     sumS = sum(nest_fx_south_accum(is:ie,:,1))
!!!$        if (je == npy-1) sumN = sum(nest_fx_north_accum(is:ie,:,1))
!!!$        call mpp_sum(sumW)
!!!$        call mpp_sum(sumE)
!!!$        call mpp_sum(sumS)
!!!$        call mpp_sum(sumW)
!!!$        if (master) then
!!!$           print*, 'Accumulated nested fluxes:'
!!!$           print*, 'WEST: ', sumW
!!!$           print*, 'EAST: ', sumE
!!!$           print*, 'SOUTH: ', sumS
!!!$           print*, 'NORTH: ', sumN
!!!$        endif
!!!$     endif
!
!     if (do_2way_flux_BCs) then
!        do n=1,size(child_grids)
!           if (child_grids(n) .and. (nestbctype == 3 .or. nestbctype == 4)) then
!              !RECEIVE fluxes; note that this will wait until the child grid is on its last split timestep
!              !REPLACE fluxes as necessary
!              
!              call receive_nested_fluxes(fx,fy,Atm(n)%npx, Atm(n)%npy, ioffset, joffset, &
!                   refinement, Atm(n)%pelist, Atm(n)%npes_this_grid, &
!                   npz, nq, neststruct%parent_tile, nestbctype, q, dp1)
!              
!           endif
!        enddo
!
!     endif
!
!     if (ANY(child_grids)) then
!
!!!$        !Flux check
!!!$        sumW = 0.
!!!$        sumE = 0.
!!!$        sumS = 0.
!!!$        sumN = 0.
!!!$        if (is == 1)     sumW = sum(nest_fx_west_accum(js:je,:,1))
!!!$        if (ie == npx-1) sumE = sum(nest_fx_east_accum(js:je,:,1))
!!!$        if (js == 1)     sumS = sum(nest_fx_south_accum(is:ie,:,1))
!!!$        if (je == npy-1) sumN = sum(nest_fx_north_accum(is:ie,:,1))
!!!$        call mpp_sum(sumW)
!!!$        call mpp_sum(sumE)
!!!$        call mpp_sum(sumS)
!!!$        call mpp_sum(sumW)
!!!$        if (master) then
!!!$           print*, 'Ending coarse fluxes:'
!!!$           print*, 'WEST: ', sumW
!!!$           print*, 'EAST: ', sumE
!!!$           print*, 'SOUTH: ', sumS
!!!$           print*, 'NORTH: ', sumN
!!!$        endif
!
!     endif
!
!
!     if ( nested .and. (nestbctype == 3 .or. nestbctype == 4)  .and. mod(it,msg_split_steps) == 0) then
!        call send_nested_fluxes(ioffset, joffset, refinement, parent_grid%pelist, &
!             parent_grid%npes_this_grid, npx, npy, npz, nq)
!     endif
!
!  endif
!
!end subroutine flux_BCs
!
!subroutine PLM_flux_division(coarse_flux, out_flux, npts_coarse, npts_out, npz, nq, R, split_steps)
!
!  real(FVPRC), intent(IN) :: coarse_flux(-1:npts_coarse+2, npz, nq) !Will need an extra point in both directions
!  real(FVPRC), intent(OUT) :: out_flux(npts_out, npz, nq)
!  integer, intent(IN) ::  npts_coarse, npts_out, npz, nq, R, split_steps
!
!  integer :: i, n, k, iq
!
!  real(FVPRC) :: slope, B, rR, rsplit
!
!  rR = 1./real(R)
!  rsplit = 1./real(split_steps)
!  
!  !Assume evenly spaced cells
!  do iq=1,nq
!  do k=1,npz
!  do i=1,npts_coarse
!
!     !Need to limit slopes to ensure fluxes do not change sign?
!!!$     B = coarse_flux(i,k,iq)
!!!$     slope = coarse_flux(i+1,k,iq) - coarse_flux(i-1,k,iq)
!     B = coarse_flux(i,k,iq)
!     slope = min(max(coarse_flux(i+1,k,iq) - coarse_flux(i-1,k,iq),-2.*abs(B)),2.*abs(B))
!
!
!     do n=0,R-1
!        out_flux((i-1)*R+n+1,k,iq) = rsplit * rR * ( 0.5*slope*rR * real(2*n - R + 1)  + B )
!     enddo
!
!  enddo
!  enddo
!  enddo  
!
!end subroutine PLM_flux_division
!
!subroutine PPM_flux_division(coarse_flux, out_flux, npts_coarse, npts_out, npz, nq, R, split_steps)
!
!  real(FVPRC), intent(IN) :: coarse_flux(-1:npts_coarse+2, npz, nq) !Will need an extra point in both directions
!  real(FVPRC), intent(OUT) :: out_flux(npts_out, npz, nq)
!  integer, intent(IN) ::  npts_coarse, npts_out, npz, nq, R, split_steps
!
!  integer :: i, n, k, iq
!
!  real(FVPRC) :: rR, rsplit, a0, a1, a2, viol, scal, d
!  real(FVPRC) :: fhat(0:npts_coarse) !Interpolated cell-face values: fhat(i) = \hat{f}_{i+1/2}
!
!!----------------------
!! PPM volume mean form:
!!----------------------
!  real(FVPRC), parameter:: p1 =  7./12.     ! 0.58333333
!  real(FVPRC), parameter:: p2 = -1./12.
!  real(FVPRC), parameter:: r3 =  1./3.
!
!  rR = 1./real(R)
!  rsplit = 1./real(split_steps)
!  
!  !Assume evenly spaced cells
!  do iq=1,nq
!  do k=1,npz
!
!     do i=0,npts_coarse
!        fhat(i) = p1*(coarse_flux(i,k,iq)+coarse_flux(i+1,k,iq)) + p2*(coarse_flux(i-1,k,iq)+coarse_flux(i+2,k,iq))
!!!$        !Limiting process (see explanation below)
!!!$        if (coarse_flux(i,k,iq)*coarse_flux(i+1,k,iq) < 0.) then
!!!$           !If signs differ set to zero
!!!$           fhat(i) = 0.
!!!$        else if (coarse_flux(i,k,iq) > 0.) then
!!!$           fhat(i) = max(fhat(i),0.)
!!!$        else
!!!$           fhat(i) = min(fhat(i),0.)
!!!$        endif
!     enddo
!
!     do i=1,npts_coarse
!
!        !Construct integral; using my formulation where 0 = midpoint of interval [-1/2,1/2]
!        a0 = coarse_flux(i,k,iq)*1.5 - 0.25*(fhat(i)+fhat(i-1))
!        a1 = 2.*(fhat(i)-fhat(i-1))
!        a2 = 3.*(fhat(i)+fhat(i-1)) - 6.*coarse_flux(i,k,iq)
!
!        !Limit to avoid sign changes
!        !Note that this assumes that the reconstruction is zero at
!        ! the cell faces between when fluxes change sign. (Something we took care of earlier) We may want
!        ! to relax this restriction so that, instead of the entire
!        ! reconstructions being positive definite (for positive
!        ! fluxes) that merely the divided fluxes are positive
!        ! -definite
!
!        !Here, we just need to check whether the extremum is inside the cell and then scale appropriately
!
!
!        do n=0,R-1
!           d = 0.5 - real(n+1)*rR
!           out_flux((i-1)*R+n+1,k,iq) = (a0 + a1*d + a2*d**2.)*rR + 0.5*rR**2.*(a1 + 2.*a2*d) + r3*rR**3.*a2
!        enddo
!        
!     enddo
!  enddo
!  enddo  
!
!end subroutine PPM_flux_division
!
!!do_FCT
!!subroutine FCT_PD(q,fx,fy,dp1,npx,npy,npz,nq,wou)
!subroutine FCT_PD(q,fx,fy,dp1,npx,npy,npz,nq, area, domain)
!
!  real(FVPRC), intent(inOUT):: q(isd:ied, jsd:jed,npz,nq)
!  real(FVPRC), intent(INOUT) :: fx(is:ie+1,js:je  ,npz,nq)
!  real(FVPRC), intent(INOUT) :: fy(is:ie , js:je+1,npz,nq)
!  real(FVPRC), intent(IN) :: dp1(is:ie,js:je,npz)
!  integer, intent(in) :: npx, npy, npz, nq
!  real(FVPRC), intent(IN) :: area(isd:ied,jsd:jed)
!  type(domain2d), intent(INOUT) :: domain
!
!  !real(FVPRC), dimension(isd:ied,jsd:jed,npz,nq), INTENT(OUT):: wou 
!  real(FVPRC), dimension(isd:ied,jsd:jed,npz,nq) :: wou 
!  real(FVPRC), parameter:: esl = 1.E-100
!
!  integer i,j,k, n
!
!      integer :: is,  ie,  js,  je
!      integer :: isd, ied, jsd, jed
!
!      is  = bd%is
!      ie  = bd%ie
!      js  = bd%js
!      je  = bd%je
!      isd = bd%isd
!      ied = bd%ied
!      jsd = bd%jsd
!      jed = bd%jed
!  
!  wou = 1.
!
!  do n=1,nq
!     do k=1,npz  
!
!        do j=js,je
!           do i=is,ie
!              wou(i,j,k,n) = max(0.,fx(i+1,j,k,n)) - min(0.,fx(i,  j,k,n)) +   &
!                             max(0.,fy(i,j+1,k,n)) - min(0.,fy(i,  j,k,n)) + esl
!              wou(i,j,k,n) = max(0., q(i,j,k,n)*dp1(i,j,k)) / wou(i,j,k,n)*area(i,j)
!           enddo
!        enddo
!     enddo
!  enddo
!
!                        call timing_on('COMM_TOTAL')
!                            call timing_on('COMM_TRACER')
!  call mpp_update_domains(wou,domain, complete=.true.)
!                           call timing_off('COMM_TRACER')
!                       call timing_off('COMM_TOTAL')
!
!  do n=1,nq
!     do k=1,npz  
!      do j=js,je
!         do i=is,ie+1
!            if ( fx(i,j,k,n) > 0. ) then
!               fx(i,j,k,n) = max(0.,min(1., wou(i-1,j,k,n))) * fx(i,j,k,n)
!            else
!               fx(i,j,k,n) = max(0.,min(1., wou(i,j,k,n))) * fx(i,j,k,n)
!            endif
!         enddo
!      enddo
!      do j=js,je+1
!         do i=is,ie
!            if ( fy(i,j,k,n) > 0. ) then
!               fy(i,j,k,n) = max(0.,min(1., wou(i,j-1,k,n))) * fy(i,j,k,n)
!            else
!               fy(i,j,k,n) = max(0.,min(1., wou(i,j,k,n))) * fy(i,j,k,n)
!            endif
!         enddo
!      enddo
!     enddo
!  enddo
!end subroutine FCT_PD
!
!subroutine send_coarse_fluxes(fx,fy,npx_n,npy_n,ioffset,joffset,refinement, child_pelist, npes_n, npx, npy, npz, nq, tile_with_n
!est)
!  
!  real(FVPRC), intent(IN) :: fx(is:ie+1,js:je  ,npz,nq)
!  real(FVPRC), intent(IN) :: fy(is:ie , js:je+1,npz,nq)
!  integer, intent(IN) :: npx_n, npy_n, refinement, ioffset, joffset, child_pelist(npes_n), npes_n, npx, npy, npz, nq, tile_with_
!nest
!
!  real(FVPRC), allocatable, dimension(:,:,:) :: coarse_fluxes_west, coarse_fluxes_east, coarse_fluxes_south, coarse_fluxes_north
!
!  integer :: i, j, k, iq
!  integer :: instart, inend, jnstart, jnend
!
!      integer :: is,  ie,  js,  je
!      integer :: isd, ied, jsd, jed
!
!      is  = bd%is
!      ie  = bd%ie
!      js  = bd%js
!      je  = bd%je
!      isd = bd%isd
!      ied = bd%ied
!      jsd = bd%jsd
!      jed = bd%jed
!
!  !Get starting and stopping indices for nested grid
!  instart = ioffset 
!  inend   = ioffset + npx_n/refinement - 1
!  jnstart = joffset 
!  jnend   = joffset + npy_n/refinement - 1
!
!  allocate(coarse_fluxes_south(instart-2:inend+2,npz,nq))
!  allocate(coarse_fluxes_north(instart-2:inend+2,npz,nq))
!
!  allocate(coarse_fluxes_west(jnstart-2:jnend+2,npz,nq))
!  allocate(coarse_fluxes_east(jnstart-2:jnend+2,npz,nq))
!
!!!$  allocate(coarse_fluxes_south(instart-1:inend+1,npz,nq))
!!!$  allocate(coarse_fluxes_north(instart-1:inend+1,npz,nq))
!!!$
!!!$  allocate(coarse_fluxes_west(jnstart-1:jnend+1,npz,nq))
!!!$  allocate(coarse_fluxes_east(jnstart-1:jnend+1,npz,nq))
!!!$
!  coarse_fluxes_west  = 0.
!  coarse_fluxes_east  = 0.
!  coarse_fluxes_south = 0.
!  coarse_fluxes_north = 0.
!
!  if (tile == tile_with_nest) then
!
!  if (is <= instart .and. ie >= instart) then
!     do iq=1,nq
!     do k=1,npz
!     do j=max(js,jnstart-2),min(je,jnend+2)
!        coarse_fluxes_west(j,k,iq) = fx(instart,j,k,iq)
!     enddo
!     enddo
!     enddo
!  endif
!
!  if (is <= inend+1 .and. ie >= inend+1) then
!!  if (is <= inend+1 .and. (ie >= inend+1 .or. ie == npx-1)) then !Are we sure we have avoided double counting fluxes?
!     !if ie == inend+1 then on the adjacent pe is == inend+2 > inend+1 so no double counting!!
!     do iq=1,nq
!     do k=1,npz
!     do j=max(js,jnstart-2),min(je,jnend+2)
!        coarse_fluxes_east(j,k,iq) = fx(inend+1,j,k,iq)
!     enddo
!     enddo
!     enddo
!  endif
!
!
!  if (js <= jnstart .and. je >= jnstart) then
!     do iq=1,nq
!     do k=1,npz
!     do i=max(is,instart-2),min(ie,inend+2)
!        coarse_fluxes_south(i,k,iq) = fy(i,jnstart,k,iq)
!     enddo
!     enddo
!     enddo
!  endif
!  if (js <= jnend+1 .and. je >= jnend+1) then
!     do iq=1,nq
!     do k=1,npz
!     do i=max(is,instart-2),min(ie,inend+2)
!        coarse_fluxes_north(i,k,iq) = fy(i,jnend+1,k,iq)
!     enddo
!     enddo
!     enddo
!  endif
!
!endif
!
!  !Gather the fluxes
!  !Using the simplest solution right now
!
!                        call timing_on('COMM_TOTAL')
!                            call timing_on('COMM_TRACER')
!  call mpp_sum(coarse_fluxes_west,  size(coarse_fluxes_west),  (/ pelist, child_pelist /) )
!  call mpp_sum(coarse_fluxes_east,  size(coarse_fluxes_east),  (/ pelist, child_pelist /) )
!  call mpp_sum(coarse_fluxes_south, size(coarse_fluxes_south), (/ pelist, child_pelist /) )
!  call mpp_sum(coarse_fluxes_north, size(coarse_fluxes_north), (/ pelist, child_pelist /) )
!                           call timing_off('COMM_TRACER')
!                       call timing_off('COMM_TOTAL')
!
!
!  deallocate( coarse_fluxes_west, coarse_fluxes_east, coarse_fluxes_south, coarse_fluxes_north )
!
!
!end subroutine send_coarse_fluxes
!
!!Sending averages fluxes before sending them, so that we don't have to send as much data
!subroutine send_nested_fluxes(ioffset,joffset,refinement, parent_pelist, npes_n, npx, npy, npz, nq)
!  
!  integer, intent(IN) :: refinement, parent_pelist(npes_n), npes_n, npx, npy, npz, nq, ioffset, joffset
!
!  integer :: i, j, k, iq, n
!  real(FVPRC) :: val
!
!  real(FVPRC), allocatable, dimension(:,:,:) :: nested_fluxes_south, nested_fluxes_north, nested_fluxes_west, nested_fluxes_east
!
!      integer :: is,  ie,  js,  je
!      integer :: isd, ied, jsd, jed
!
!      is  = bd%is
!      ie  = bd%ie
!      js  = bd%js
!      je  = bd%je
!      isd = bd%isd
!      ied = bd%ied
!      jsd = bd%jsd
!      jed = bd%jed
!
!  allocate(nested_fluxes_south((npx-1)/refinement, npz, nq))
!  allocate(nested_fluxes_north((npx-1)/refinement, npz, nq))
!
!  allocate(nested_fluxes_east((npy-1)/refinement, npz, nq))
!  allocate(nested_fluxes_west((npy-1)/refinement, npz, nq))
!
!  nested_fluxes_west  = 0.
!  nested_fluxes_east  = 0.
!  nested_fluxes_south = 0.
!  nested_fluxes_north = 0.
!
!  if (is == 1) then
!     do iq=1,nq
!     do k=1,npz
!     do j=js,je
!        nested_fluxes_west((j-1)/refinement+1,k,iq) = nested_fluxes_west((j-1)/refinement+1,k,iq) + nest_fx_west_accum(j,k,iq)
!     enddo
!     enddo
!     enddo
!  endif
!  if (ie == npx-1 ) then
!     do iq=1,nq
!     do k=1,npz
!     do j=js,je
!        nested_fluxes_east((j-1)/refinement+1,k,iq) = nested_fluxes_east((j-1)/refinement+1,k,iq) + nest_fx_east_accum(j,k,iq)
!     enddo
!     enddo
!     enddo
!  endif
!
!  if (js == 1) then
!     do iq=1,nq
!     do k=1,npz
!     do i=is,ie
!        nested_fluxes_south((i-1)/refinement+1,k,iq) = nested_fluxes_south((i-1)/refinement+1,k,iq) + nest_fx_south_accum(i,k,iq
!)
!     enddo
!     enddo
!     enddo
!  endif
!  if (je == npy-1) then
!     do iq=1,nq
!     do k=1,npz
!     do i=is,ie
!        nested_fluxes_north((i-1)/refinement+1,k,iq) = nested_fluxes_north((i-1)/refinement+1,k,iq) + nest_fx_north_accum(i,k,iq
!)
!     enddo
!     enddo
!     enddo
!  endif
!
!  !Gather the fluxes
!  !Using the simplest solution right now
!                        call timing_on('COMM_TOTAL')
!                            call timing_on('COMM_TRACER')
!  call mpp_sum(nested_fluxes_west,  size(nested_fluxes_west),  (/ parent_pelist, pelist /) )
!  call mpp_sum(nested_fluxes_east,  size(nested_fluxes_east),  (/ parent_pelist, pelist /) )
!  call mpp_sum(nested_fluxes_south, size(nested_fluxes_south), (/ parent_pelist, pelist /) )
!  call mpp_sum(nested_fluxes_north, size(nested_fluxes_north), (/ parent_pelist, pelist /) )
!                           call timing_off('COMM_TRACER')
!                       call timing_off('COMM_TOTAL')
!
!  deallocate( nested_fluxes_south, nested_fluxes_north, nested_fluxes_west, nested_fluxes_east )
!
!end subroutine send_nested_fluxes
!
!subroutine receive_nested_fluxes(fx,fy, npx_n, npy_n, ioffset, joffset, refinement, &
!     child_pelist, npes_n, npz, nq, tile_with_nest, nestbctype_n, q, dp1)
!  
!  real(FVPRC), intent(INOUT) :: fx(is:ie+1,js:je  ,npz,nq)
!  real(FVPRC), intent(INOUT) :: fy(is:ie , js:je+1,npz,nq)
!  integer, intent(IN) :: npx_n, npy_n, refinement, ioffset, joffset, child_pelist(npes_n), &
!       npes_n, npz, nq, tile_with_nest, nestbctype_n
!  real(FVPRC), intent(IN) :: q(isd:ied,jsd:jed,npz,nq)
!  real(FVPRC), intent(IN) :: dp1(is:ie,js:je,npz)
!
!  integer :: i, j, k, iq
!  integer :: instart, inend, jnstart, jnend
!
!  real(FVPRC) :: outflux, Ry(js-1:je+1), Rx(is-1:ie+1)
!
!  real(FVPRC) :: sumN, sumS, sumE, sumW
!  real(FVPRC) :: maxN, maxS, maxE, maxW
!
!  real(FVPRC), allocatable, dimension(:,:,:) :: nested_fluxes_south, nested_fluxes_north, nested_fluxes_west, nested_fluxes_east
!
!      integer :: is,  ie,  js,  je
!      integer :: isd, ied, jsd, jed
!
!      is  = bd%is
!      ie  = bd%ie
!      js  = bd%js
!      je  = bd%je
!      isd = bd%isd
!      ied = bd%ied
!      jsd = bd%jsd
!      jed = bd%jed
!
!  allocate(nested_fluxes_south((npx_n-1)/refinement, npz, nq))
!  allocate(nested_fluxes_north((npx_n-1)/refinement, npz, nq))
!
!  allocate(nested_fluxes_east((npy_n-1)/refinement, npz, nq))
!  allocate(nested_fluxes_west((npy_n-1)/refinement, npz, nq))
!
!  nested_fluxes_west  = 0.
!  nested_fluxes_east  = 0.
!  nested_fluxes_south = 0.
!  nested_fluxes_north = 0.
!
!  !Gather the fluxes
!  !Using the simplest solution right now
!                        call timing_on('COMM_TOTAL')
!                            call timing_on('COMM_TRACER')
!  call mpp_sum(nested_fluxes_west,  size(nested_fluxes_west),  (/ pelist, child_pelist /) )
!  call mpp_sum(nested_fluxes_east,  size(nested_fluxes_east),  (/ pelist, child_pelist /) )
!  call mpp_sum(nested_fluxes_south, size(nested_fluxes_south), (/ pelist, child_pelist /) )
!  call mpp_sum(nested_fluxes_north, size(nested_fluxes_north), (/ pelist, child_pelist /) )
!                           call timing_off('COMM_TRACER')
!                       call timing_off('COMM_TOTAL')
!
!  sumN = 0.
!  sumS = 0.
!  sumE = 0.
!  sumW = 0.
!  maxN = 0.
!  maxS = 0.
!  maxE = 0.
!  maxW = 0.
!
!  if (tile == tile_with_nest) then
!
!  !Get starting and stopping indices for nested grid
!  instart = ioffset
!  inend   = ioffset + npx_n/refinement - 1
!  jnstart = joffset
!  jnend   = joffset + npy_n/refinement - 1
!
!  if (nestbctype_n == 2) then
!     
!     !I don't recall what this section of code is supposed to do.
!
!      if (is <= instart .and. ie > instart) then
!         do iq=1,nq
!         do k=1,npz
!         do j=max(js,jnstart),min(je,jnend)
!               fx(instart,j,k,iq) = fx(instart,j,k,iq) + nested_fluxes_west(j-joffset+1,k,iq)
!         enddo
!         enddo
!         enddo
!      endif
!      if (is <= inend+1 .and. ie >= inend+1) then
!         do iq=1,nq
!         do k=1,npz
!         do j=max(js,jnstart),min(je,jnend)
!               fx(inend+1,j,k,iq) = fx(inend+1,j,k,iq) + nested_fluxes_east(j-joffset+1,k,iq)
!         enddo
!         enddo
!         enddo
!      endif
!
!
!      if (js <= jnstart .and. je > jnstart) then
!         do iq=1,nq
!         do k=1,npz
!         do i=max(is,instart),min(ie,inend)
!               fy(i,jnstart,k,iq) = fy(i,jnstart,k,iq) + nested_fluxes_south(i-ioffset+1,k,iq)
!         enddo
!         enddo
!         enddo
!      endif
!      if (js <= jnend+1 .and. je >= jnend+1) then
!         do iq=1,nq
!         do k=1,npz
!         do i=max(is,instart),min(ie,inend)
!               fy(i,jnend+1,k,iq) = fy(i,jnend+1,k,iq) + nested_fluxes_north(i-ioffset+1,k,iq)
!         enddo
!         enddo
!         enddo
!      endif
!
!   else
!
!      !This outright replaces the coarse fluxes with nested-grid fluxes
!      
!    if (is <= instart .and. ie >= instart) then
!       do iq=1,nq
!       do k=1,npz
!          !Ry = 1.
!       do j=max(js,jnstart),min(je,jnend)
!          !if (iq == 1) maxW = max(maxW,(fx(instart,j,k,iq) - nested_fluxes_west(j-joffset+1,k,iq)))
!          fx(instart,j,k,iq) = nested_fluxes_west(j-joffset+1,k,iq)
!          !if (iq == 1) sumW = sumW + fx(instart,j,k,iq)
!       enddo
!       enddo
!       enddo
!    endif
!    if (is <= inend+1 .and. ie >= inend+1) then
!       do iq=1,nq
!       do k=1,npz
!          !Ry = 1.
!          i = inend+1
!       do j=max(js,jnstart),min(je,jnend)
!          !if (iq == 1) maxE = max(maxE,fx(i,j,k,iq) - nested_fluxes_east(j-joffset+1,k,iq))
!          fx(i,j,k,iq) = nested_fluxes_east(j-joffset+1,k,iq)
!          !if (iq == 1) sumE = sumE + fx(i,j,k,iq)
!       enddo
!       enddo
!       enddo
!    endif
!
!
!    if (js <= jnstart .and. je >= jnstart) then
!       do iq=1,nq
!       do k=1,npz
!          !Rx = 1.
!       do i=max(is,instart),min(ie,inend)
!          !if (iq == 1) maxS = max(maxS, fy(i,jnstart,k,iq) - nested_fluxes_south(i-ioffset+1,k,iq))
!          fy(i,jnstart,k,iq) = nested_fluxes_south(i-ioffset+1,k,iq)
!          !if (iq == 1) sumS = sumS + fy(i,jnstart,k,iq)
!       enddo
!       enddo
!       enddo
!    endif
!    if (js <= jnend+1 .and. je >= jnend+1) then
!       do iq=1,nq
!       do k=1,npz
!          j=jnend+1
!       do i=max(is,instart),min(ie,inend)
!          !if (iq == 1) maxN = max(maxN, fy(i,j,k,iq) - nested_fluxes_north(i-ioffset+1,k,iq))
!          fy(i,j,k,iq) = nested_fluxes_north(i-ioffset+1,k,iq)
!          !if (iq == 1) sumN = sumN + fy(i,j,k,iq)
!       enddo
!       enddo
!       enddo
!    endif
!
! endif
!
!
!    !Flux check
!    !if (master) then
!!!$    if (max(maxW, maxE, maxS, maxN) > 0.) then
!!!$       write(gid+100,*) ''
!!!$       if (maxW > 0) write(gid+100,*) 'WEST: ', maxW/maxval(abs(fx(instart,:,:,1)))
!!!$       if (maxE > 0) write(gid+100,*) 'EAST: ', maxE/maxval(abs(fx(inend+1,:,:,1)))
!!!$       if (maxS > 0) write(gid+100,*) 'SOUTH: ', maxS/maxval(abs(fy(:,jnstart,:,1)))
!!!$       if (maxN > 0) write(gid+100,*) 'NORTH: ', maxN/maxval(abs(fy(:,jnend+1,:,1)))
!!!$    endif
!    !endif
!
!endif
!
!  deallocate( nested_fluxes_south, nested_fluxes_north, nested_fluxes_west, nested_fluxes_east )
!
!end subroutine receive_nested_fluxes
!
!#endif FLUXBCS
  PRIVATE 
!, offline_tracer_advection
  PUBLIC tracer_2d, tracer_2d_nested, tracer_2d_1l
  PUBLIC tracer_2d_fwd, tracer_2d_bwd, tracer_2d_nested_fwd, &
& tracer_2d_nested_bwd
  REAL(fvprc), DIMENSION(:, :, :), ALLOCATABLE :: nest_fx_west_accum, &
& nest_fx_east_accum, nest_fx_south_accum, nest_fx_north_accum
  EXTERNAL TRACER_2D_ADM
  EXTERNAL TRACER_2D_NESTED_ADM

CONTAINS
!---- version number -----
!   character(len=128) :: version = '$Id: fv_tracer2d.F90,v 1.1.2.2.2.1.30.1.90.6.4.1.2.1.4.3.2.2.2.1.2.1 2017/02/16 03:47:47 aol
!oso Exp $'
!   character(len=128) :: tagname = '$Name: Heracles-UNSTABLE_ncepdyn_Feb222017 $'
!-----------------------------------------------------------------------
! !ROUTINE: Perform 2D horizontal-to-lagrangian transport
!-----------------------------------------------------------------------
  SUBROUTINE TRACER_2D_1L(q, dp0, mfx, mfy, cx, cy, gridstruct, &
&   neststruct, bd, domain, npx, npy, npz, qs, nq, hord, hord_pert, &
&   q_split, k, q3, dt, id_divg, k_split, dpa)
    IMPLICIT NONE
    TYPE(FV_GRID_BOUNDS_TYPE), INTENT(IN) :: bd
    INTEGER, INTENT(IN) :: npx, npy, npz
    INTEGER, INTENT(IN) :: k
! first tracer to advect
    INTEGER, INTENT(IN) :: qs
! number of tracers to be advected
    INTEGER, INTENT(IN) :: nq
    INTEGER, INTENT(IN) :: hord, hord_pert
    INTEGER, INTENT(IN) :: q_split, k_split
    INTEGER, INTENT(IN) :: id_divg
    REAL(fvprc), INTENT(IN) :: dt
! 2D Tracers
    REAL(fvprc), INTENT(INOUT) :: q(bd%isd:bd%ied, bd%jsd:bd%jed, nq)
! Tracers
    REAL(fvprc), INTENT(INOUT) :: q3(bd%isd:bd%ied, bd%jsd:bd%jed, npz, &
&   nq)
! DELP before dyn_core
    REAL(fvprc), INTENT(INOUT) :: dp0(bd%is:bd%ie, bd%js:bd%je)
! Mass Flux X-Dir
    REAL(fvprc), INTENT(IN) :: mfx(bd%is:bd%ie+1, bd%js:bd%je)
! Mass Flux Y-Dir
    REAL(fvprc), INTENT(IN) :: mfy(bd%is:bd%ie, bd%js:bd%je+1)
! Courant Number X-Dir
    REAL(fvprc), INTENT(IN) :: cx(bd%is:bd%ie+1, bd%jsd:bd%jed)
! Courant Number Y-Dir
    REAL(fvprc), INTENT(IN) :: cy(bd%isd:bd%ied, bd%js:bd%je+1)
    REAL(fvprc), OPTIONAL, INTENT(INOUT) :: dpa(bd%is:bd%ie, bd%js:bd%je&
&   )
    TYPE(FV_GRID_TYPE), INTENT(IN), TARGET :: gridstruct
    TYPE(FV_NEST_TYPE), INTENT(INOUT) :: neststruct
    TYPE(DOMAIN2D), INTENT(INOUT) :: domain
! Local Arrays
    REAL(fvprc) :: mfx2(bd%is:bd%ie+1, bd%js:bd%je)
    REAL(fvprc) :: mfy2(bd%is:bd%ie, bd%js:bd%je+1)
    REAL(fvprc) :: cx2(bd%is:bd%ie+1, bd%jsd:bd%jed)
    REAL(fvprc) :: cy2(bd%isd:bd%ied, bd%js:bd%je+1)
    REAL(fvprc) :: dp1(bd%is:bd%ie, bd%js:bd%je)
    REAL(fvprc) :: dp2(bd%is:bd%ie, bd%js:bd%je)
    REAL(fvprc) :: fx(bd%is:bd%ie+1, bd%js:bd%je)
    REAL(fvprc) :: fy(bd%is:bd%ie, bd%js:bd%je+1)
    REAL(fvprc) :: ra_x(bd%is:bd%ie, bd%jsd:bd%jed)
    REAL(fvprc) :: ra_y(bd%isd:bd%ied, bd%js:bd%je)
    REAL(fvprc) :: xfx(bd%is:bd%ie+1, bd%jsd:bd%jed)
    REAL(fvprc) :: yfx(bd%isd:bd%ied, bd%js:bd%je+1)
    REAL(fvprc) :: cmax
    REAL(fvprc) :: frac, rdt
    INTEGER :: nsplt
    INTEGER :: i, j, it, iq
    TYPE(GROUP_HALO_UPDATE_TYPE), SAVE :: i_pack, i_packp
! 2D Tracers
    REAL(fvprc) :: q_tj(bd%isd:bd%ied, bd%jsd:bd%jed, nq)
    REAL(fvprc) :: fx_tj(bd%is:bd%ie+1, bd%js:bd%je)
    REAL(fvprc) :: fy_tj(bd%is:bd%ie, bd%js:bd%je+1)
!      real(FVPRC), pointer, dimension(:,:) :: area, rarea, sina_u, sina_v
!      real(FVPRC), pointer, dimension(:,:) :: dxa, dya, dx, dy
!      real(FVPRC), pointer, dimension(:,:,:) :: sin_sg
    INTEGER :: is, ie, js, je
    INTEGER :: isd, ied, jsd, jed
    REAL(fvprc) :: step, split
    INTRINSIC ABS
    INTRINSIC MAX
    INTRINSIC INT
    INTRINSIC REAL
    INTRINSIC PRESENT
    REAL(fvprc) :: abs0
    REAL(fvprc) :: abs1
    REAL(fvprc) :: x1
    REAL(fvprc) :: y1
    is = bd%is
    ie = bd%ie
    js = bd%js
    je = bd%je
    isd = bd%isd
    ied = bd%ied
    jsd = bd%jsd
    jed = bd%jed
!       area => gridstruct%area
!      rarea => gridstruct%rarea
!      sina_u => gridstruct%sina_u
!      sina_v => gridstruct%sina_v
!      sin_sg => gridstruct%sin_sg
!      dxa    => gridstruct%dxa 
!      dya    => gridstruct%dya 
!      dx     => gridstruct%dx  
!      dy     => gridstruct%dy  
    CALL START_GROUP_HALO_UPDATE(i_pack, i_packp, q, domain)
    DO j=jsd,jed
      DO i=is,ie+1
        IF (cx(i, j) .GT. 0.) THEN
          xfx(i, j) = cx(i, j)*gridstruct%dxa(i-1, j)*gridstruct%dy(i, j&
&           )*gridstruct%sin_sg(i-1, j, 3)
        ELSE
          xfx(i, j) = cx(i, j)*gridstruct%dxa(i, j)*gridstruct%dy(i, j)*&
&           gridstruct%sin_sg(i, j, 1)
        END IF
      END DO
    END DO
    DO j=js,je+1
      DO i=isd,ied
        IF (cy(i, j) .GT. 0.) THEN
          yfx(i, j) = cy(i, j)*gridstruct%dya(i, j-1)*gridstruct%dx(i, j&
&           )*gridstruct%sin_sg(i, j-1, 4)
        ELSE
          yfx(i, j) = cy(i, j)*gridstruct%dya(i, j)*gridstruct%dx(i, j)*&
&           gridstruct%sin_sg(i, j, 2)
        END IF
      END DO
    END DO
    IF (q_split .EQ. 0) THEN
! Determine nsplt for tracer advection
      cmax = 0.
      DO j=js,je
        DO i=is,ie
          IF (cx(i, j) .GE. 0.) THEN
            abs0 = cx(i, j)
          ELSE
            abs0 = -cx(i, j)
          END IF
          x1 = abs0 + (1.-gridstruct%sina_u(i, j))
          IF (cy(i, j) .GE. 0.) THEN
            abs1 = cy(i, j)
          ELSE
            abs1 = -cy(i, j)
          END IF
          y1 = abs1 + (1.-gridstruct%sina_v(i, j))
          IF (x1 .LT. y1) THEN
            IF (y1 .LT. cmax) THEN
              cmax = cmax
            ELSE
              cmax = y1
            END IF
          ELSE IF (x1 .LT. cmax) THEN
            cmax = cmax
          ELSE
            cmax = x1
          END IF
        END DO
      END DO
      CALL MP_REDUCE_MAX(cmax)
      nsplt = INT(1.01 + cmax)
!if ( is_master() .and. nsplt > 5 )  write(*,*) k, 'Tracer_2d_split=', nsplt, cmax
    ELSE
      nsplt = q_split
    END IF
    frac = 1./REAL(nsplt)
    DO j=jsd,jed
      DO i=is,ie+1
        cx2(i, j) = cx(i, j)*frac
        xfx(i, j) = xfx(i, j)*frac
      END DO
    END DO
    DO j=js,je
      DO i=is,ie+1
        mfx2(i, j) = mfx(i, j)*frac
      END DO
    END DO
    DO j=js,je+1
      DO i=isd,ied
        cy2(i, j) = cy(i, j)*frac
        yfx(i, j) = yfx(i, j)*frac
      END DO
    END DO
    DO j=js,je+1
      DO i=is,ie
        mfy2(i, j) = mfy(i, j)*frac
      END DO
    END DO
    DO j=jsd,jed
      DO i=is,ie
        ra_x(i, j) = gridstruct%area(i, j) + xfx(i, j) - xfx(i+1, j)
      END DO
    END DO
    DO j=js,je
      DO i=isd,ied
        ra_y(i, j) = gridstruct%area(i, j) + yfx(i, j) - yfx(i, j+1)
      END DO
    END DO
    DO j=js,je
      DO i=is,ie
        dp1(i, j) = dp0(i, j)
      END DO
    END DO
! Start time split ...
    DO it=1,nsplt
      DO j=js,je
        DO i=is,ie
          dp2(i, j) = dp1(i, j) + (mfx2(i, j)-mfx2(i+1, j)+mfy2(i, j)-&
&           mfy2(i, j+1))*gridstruct%rarea(i, j)
        END DO
      END DO
      CALL COMPLETE_GROUP_HALO_UPDATE(i_pack, i_packp, domain)
      DO iq=qs,nq
        IF (hord .EQ. hord_pert) THEN
          CALL FV_TP_2D(q(isd:ied, jsd:jed, iq), cx2, cy2, npx, npy, &
&                    hord, fx, fy, xfx, yfx, gridstruct, bd, ra_x, ra_y&
&                    , mfx=mfx2, mfy=mfy2)
        ELSE
          q_tj(:, :, iq) = q(:, :, iq)
          fx_tj = fx
          fy_tj = fy
          CALL FV_TP_2D(q(isd:ied, jsd:jed, iq), cx2, cy2, npx, npy, &
&                 hord_pert, fx, fy, xfx, yfx, gridstruct, bd, ra_x, &
&                 ra_y, mfx=mfx2, mfy=mfy2)
        END IF
        IF (it .EQ. nsplt) THEN
          DO j=js,je
            DO i=is,ie
              q3(i, j, k, iq) = (q(i, j, iq)*dp1(i, j)+(fx(i, j)-fx(i+1&
&               , j)+fy(i, j)-fy(i, j+1))*gridstruct%rarea(i, j))/dp2(i&
&               , j)
            END DO
          END DO
        ELSE
          DO j=js,je
            DO i=is,ie
              q(i, j, iq) = (q(i, j, iq)*dp1(i, j)+(fx(i, j)-fx(i+1, j)+&
&               fy(i, j)-fy(i, j+1))*gridstruct%rarea(i, j))/dp2(i, j)
            END DO
          END DO
        END IF
      END DO
!Apply nested-grid BCs
!           if ( gridstruct%nested ) then
!
!                 step  = real(neststruct%tracer_nest_timestep)+real(nsplt*k_split)
!                 split = real(nsplt*k_split)
!                 call nested_grid_BC_apply_intT(q(isd:ied,jsd:jed,iq), &
!                      0, 0, npx, npy, bd, &
!                      step, split, &
!                      var_east_t0=neststruct%q_BC(iq)%east_t0(:,:,k), &
!                      var_west_t0=neststruct%q_BC(iq)%west_t0(:,:,k), &
!                      var_north_t0=neststruct%q_BC(iq)%north_t0(:,:,k), &
!                      var_south_t0=neststruct%q_BC(iq)%south_t0(:,:,k), &
!                      var_east_t1=neststruct%q_BC(iq)%east_t1(:,:,k), &
!                      var_west_t1=neststruct%q_BC(iq)%west_t1(:,:,k), &
!                      var_north_t1=neststruct%q_BC(iq)%north_t1(:,:,k), &
!                      var_south_t1=neststruct%q_BC(iq)%south_t1(:,:,k), &
!                      bctype=neststruct%nestbctype, &
!                      nsponge=neststruct%nsponge, s_weight=neststruct%s_weight   )
!           end if
!q-loop
      IF (it .NE. nsplt) THEN
        CALL START_GROUP_HALO_UPDATE(i_pack, i_packp, q, domain)
        DO j=js,je
          DO i=is,ie
            dp1(i, j) = dp2(i, j)
          END DO
        END DO
      END IF
    END DO
! nsplt
    IF (gridstruct%nested .AND. k .EQ. npz) neststruct%&
&     tracer_nest_timestep = neststruct%tracer_nest_timestep + 1
    IF (id_divg .GT. 0) THEN
      rdt = 1./(frac*dt)
      DO j=js,je
        DO i=is,ie
          dp0(i, j) = (xfx(i+1, j)-xfx(i, j)+yfx(i, j+1)-yfx(i, j))*&
&           gridstruct%rarea(i, j)*rdt
        END DO
      END DO
    END IF
    IF (PRESENT(dpa)) dpa = dp2
  END SUBROUTINE TRACER_2D_1L
!  Differentiation of tracer_2d in reverse (adjoint) mode, forward sweep (with options r8 split(a2b_edge_mod.a2b_ord2 a2b_edge_mo
!d.a2b_ord4 a2b_edge_mod.extrap_corner  dyn_core_mod.dyn_core dyn_core_mod.pk3_halo dyn_core_mod.pln_halo dyn_core_mod.pe_h
!alo dyn_core_mod.adv_pe dyn_core_mod.p_grad_c dyn_core_mod.nh_p_grad dyn_core_mod.split_p_grad dyn_core_mod.one_grad_p dyn_core_
!mod.grad1_p_update dyn_core_mod.mix_dp dyn_core_mod.geopk dyn_core_mod.del2_cubed fv_dynamics_mod.fv_dynamics fv_dynamics_mod.ra
!yleigh_super fv_dynamics_mod.rayleigh_friction fv_dynamics_mod.compute_aam fv_dynamics_mod.geos_to_fv3 fv_grid_utils_mod.cubed_t
!o_latlon fv_grid_utils_mod.c2l_ord4 fv_grid_utils_mod.c2l_ord2 fv_mapz_mod.lagrangian_to_eulerian fv_mapz_mod.compute_total_ener
!gy fv_mapz_mod.pkez fv_mapz_mod.remap_z fv_mapz_mod.map_scalar fv_mapz_mod.map1_ppm fv_mapz_mod.mapn_tracer fv_mapz_
!mod.map1_q2 fv_mapz_mod.map1_cubic fv_mapz_mod.scalar_profile_linear fv_mapz_mod.cs_profile_linear fv_mapz_mod.steepz f
!v_tracer2d_mod.tracer_2d fv_tracer2d_mod.tracer_2d_nested nh_core_mod.update_dz_c nh_core_mod.update_dz_d nh_core_mod.riem_solve
!r_c nh_core_mod.riem_solver3 nh_core_mod.rim_2d nh_core_mod.sim3_solver nh_core_mod.sim3p0_solver nh_core_mod.sim1_solver nh_cor
!e_mod.sim_solver nh_core_mod.edge_profile sw_core_mod.c_sw sw_core_mod.d_sw sw_core_mod.divergence_corner sw_core_mod.divergence
!_corner_nest sw_core_mod.d2a2c_vect sw_core_mod.edge_interpolate4 sw_core_mod.fill2_4corners sw_core_mod.fill_4corners sw_core_m
!od.compute_div_damping sw_core_mod.smag_corner sw_core_mod.ytp_v sw_core_mod.xtp_u tp_core_mod.fv_tp_2d tp_core_m
!od.copy_corners tp_core_mod.xtp tp_core_mod.ytp tp_core_mod.xppm0 tp_core_mod.yppm0 tp_core_mod.fxppm tp_core_
!mod.fyppm tp_core_mod.deln_flux)):
!   gradient     of useful results: q dp1 mfx mfy cx cy
!   with respect to varying inputs: q dp1 mfx mfy cx cy
  SUBROUTINE TRACER_2D_FWD(q, dp1, mfx, mfy, cx, cy, gridstruct, bd, &
&   domain, npx, npy, npz, qs, nq, hord, hord_pert, q_split, dt, id_divg&
&   , q_pack, q_packp, z_tracer, k_split, dpa)
    IMPLICIT NONE
    TYPE(FV_GRID_BOUNDS_TYPE), INTENT(IN) :: bd
    INTEGER, INTENT(IN) :: npx
    INTEGER, INTENT(IN) :: npy
    INTEGER, INTENT(IN) :: npz
! first tracer to advect
    INTEGER, INTENT(IN) :: qs
! number of tracers to be advected
    INTEGER, INTENT(IN) :: nq
    INTEGER, INTENT(IN) :: hord, hord_pert
    INTEGER, INTENT(IN) :: q_split, k_split
    INTEGER, INTENT(IN) :: id_divg
    REAL(fvprc), INTENT(IN) :: dt
    LOGICAL, INTENT(IN) :: z_tracer
    TYPE(GROUP_HALO_UPDATE_TYPE), INTENT(INOUT) :: q_pack, q_packp
! Tracers
    REAL(fvprc), INTENT(INOUT) :: q(bd%isd:bd%ied, bd%jsd:bd%jed, npz, &
&   nq)
! DELP before dyn_core
    REAL(fvprc), INTENT(INOUT) :: dp1(bd%is:bd%ie, bd%js:bd%je, npz)
! Mass Flux X-Dir
    REAL(fvprc), INTENT(INOUT) :: mfx(bd%is:bd%ie+1, bd%js:bd%je, npz)
! Mass Flux Y-Dir
    REAL(fvprc), INTENT(INOUT) :: mfy(bd%is:bd%ie, bd%js:bd%je+1, npz)
! Courant Number X-Dir
    REAL(fvprc), INTENT(INOUT) :: cx(bd%is:bd%ie+1, bd%jsd:bd%jed, npz)
! Courant Number Y-Dir
    REAL(fvprc), INTENT(INOUT) :: cy(bd%isd:bd%ied, bd%js:bd%je+1, npz)
    TYPE(FV_GRID_TYPE), INTENT(IN), TARGET :: gridstruct
    TYPE(DOMAIN2D), INTENT(INOUT) :: domain
! DELP after advection
    REAL(fvprc), OPTIONAL, INTENT(OUT) :: dpa(bd%is:bd%ie, bd%js:bd%je, &
&   npz)
! Local Arrays
    REAL(fvprc) :: dp2(bd%is:bd%ie, bd%js:bd%je, npz)
    REAL(fvprc) :: fx(bd%is:bd%ie+1, bd%js:bd%je, npz, nq)
    REAL(fvprc) :: fy(bd%is:bd%ie, bd%js:bd%je+1, npz, nq)
    REAL(fvprc) :: ra_x(bd%is:bd%ie, bd%jsd:bd%jed, npz)
    REAL(fvprc) :: ra_y(bd%isd:bd%ied, bd%js:bd%je, npz)
    REAL(fvprc) :: xfx(bd%is:bd%ie+1, bd%jsd:bd%jed, npz)
    REAL(fvprc) :: yfx(bd%isd:bd%ied, bd%js:bd%je+1, npz)
    REAL(fvprc) :: cmax(npz)
    REAL(fvprc) :: cmax_t
    REAL(fvprc) :: c_global
    REAL(fvprc) :: frac, rdt
    INTEGER :: nsplt
    INTEGER :: i, j, k, it, iq
! Tracers
    REAL(fvprc) :: q_tj(bd%isd:bd%ied, bd%jsd:bd%jed, npz, nq)
    REAL(fvprc) :: fx_tj(bd%is:bd%ie+1, bd%js:bd%je, npz, nq)
    REAL(fvprc) :: fy_tj(bd%is:bd%ie, bd%js:bd%je+1, npz, nq)
!      real(FVPRC), pointer, dimension(:,:) :: area, rarea
!      real(FVPRC), pointer, dimension(:,:,:) :: sin_sg
!      real(FVPRC), pointer, dimension(:,:) :: dxa, dya, dx, dy
    INTEGER :: is, ie, js, je
    INTEGER :: isd, ied, jsd, jed
    INTRINSIC ABS
    INTRINSIC MAX
    INTRINSIC INT
    INTRINSIC REAL
    INTRINSIC PRESENT
    REAL(fvprc) :: max1
    REAL(fvprc) :: x2
    REAL(fvprc) :: x1
    REAL(fvprc) :: y2
    REAL(fvprc) :: y1
    is = bd%is
    ie = bd%ie
    js = bd%js
    je = bd%je
    isd = bd%isd
    ied = bd%ied
    jsd = bd%jsd
    jed = bd%jed
!       area => gridstruct%area
!      rarea => gridstruct%rarea
!      sin_sg => gridstruct%sin_sg
!      dxa    => gridstruct%dxa 
!      dya    => gridstruct%dya 
!      dx     => gridstruct%dx  
!      dy     => gridstruct%dy  
!$OMP parallel do default(none) shared(is,ie,js,je,isd,ied,jsd,jed,npz,cx,xfx,dxa,dy, &
!$OMP                                  sin_sg,cy,yfx,dya,dx)
    DO k=1,npz
      DO j=jsd,jed
        DO i=is,ie+1
          IF (cx(i, j, k) .GT. 0.) THEN
            xfx(i, j, k) = cx(i, j, k)*gridstruct%dxa(i-1, j)*gridstruct&
&             %dy(i, j)*gridstruct%sin_sg(i-1, j, 3)
            CALL PUSHCONTROL1B_FV(1)
          ELSE
            xfx(i, j, k) = cx(i, j, k)*gridstruct%dxa(i, j)*gridstruct%&
&             dy(i, j)*gridstruct%sin_sg(i, j, 1)
            CALL PUSHCONTROL1B_FV(0)
          END IF
        END DO
      END DO
      DO j=js,je+1
        DO i=isd,ied
          IF (cy(i, j, k) .GT. 0.) THEN
            yfx(i, j, k) = cy(i, j, k)*gridstruct%dya(i, j-1)*gridstruct&
&             %dx(i, j)*gridstruct%sin_sg(i, j-1, 4)
            CALL PUSHCONTROL1B_FV(1)
          ELSE
            yfx(i, j, k) = cy(i, j, k)*gridstruct%dya(i, j)*gridstruct%&
&             dx(i, j)*gridstruct%sin_sg(i, j, 2)
            CALL PUSHCONTROL1B_FV(0)
          END IF
        END DO
      END DO
    END DO
!--------------------------------------------------------------------------------
    IF (q_split .EQ. 0) THEN
! Determine nsplt
!$OMP parallel do default(none) shared(is,ie,js,je,npz,cmax,cx,cy,sin_sg) &
!$OMP                          private(cmax_t )
      DO k=1,npz
        cmax(k) = 0.
        IF (k .LT. 4) THEN
! Top layers: C < max( abs(c_x), abs(c_y) )
          DO j=js,je
            DO i=is,ie
              IF (cx(i, j, k) .GE. 0.) THEN
                x1 = cx(i, j, k)
              ELSE
                x1 = -cx(i, j, k)
              END IF
              IF (cy(i, j, k) .GE. 0.) THEN
                y1 = cy(i, j, k)
              ELSE
                y1 = -cy(i, j, k)
              END IF
              IF (x1 .LT. y1) THEN
                cmax_t = y1
              ELSE
                cmax_t = x1
              END IF
              IF (cmax_t .LT. cmax(k)) THEN
                CALL PUSHCONTROL1B_FV(0)
                cmax(k) = cmax(k)
              ELSE
                CALL PUSHCONTROL1B_FV(1)
                cmax(k) = cmax_t
              END IF
            END DO
          END DO
          CALL PUSHCONTROL1B_FV(1)
        ELSE
          DO j=js,je
            DO i=is,ie
              IF (cx(i, j, k) .GE. 0.) THEN
                x2 = cx(i, j, k)
              ELSE
                x2 = -cx(i, j, k)
              END IF
              IF (cy(i, j, k) .GE. 0.) THEN
                y2 = cy(i, j, k)
              ELSE
                y2 = -cy(i, j, k)
              END IF
              IF (x2 .LT. y2) THEN
                max1 = y2
              ELSE
                max1 = x2
              END IF
              cmax_t = max1 + 1. - gridstruct%sin_sg(i, j, 5)
              IF (cmax_t .LT. cmax(k)) THEN
                CALL PUSHCONTROL1B_FV(0)
                cmax(k) = cmax(k)
              ELSE
                CALL PUSHCONTROL1B_FV(1)
                cmax(k) = cmax_t
              END IF
            END DO
          END DO
          CALL PUSHCONTROL1B_FV(0)
        END IF
      END DO
      CALL MP_REDUCE_MAX(cmax, npz)
! find global max courant number and define nsplt to scale cx,cy,mfx,mfy
      c_global = cmax(1)
      IF (npz .NE. 1) THEN
! if NOT shallow water test case
        DO k=2,npz
          IF (cmax(k) .LT. c_global) THEN
            CALL PUSHCONTROL1B_FV(0)
            c_global = c_global
          ELSE
            CALL PUSHCONTROL1B_FV(1)
            c_global = cmax(k)
          END IF
        END DO
        CALL PUSHCONTROL2B_FV(0)
      ELSE
        CALL PUSHCONTROL2B_FV(1)
      END IF
      nsplt = INT(1. + c_global)
!if ( is_master() .and. nsplt > 3 )  write(*,*) 'Tracer_2d_split=', nsplt, c_global
    ELSE
      CALL PUSHCONTROL2B_FV(2)
      nsplt = q_split
    END IF
!--------------------------------------------------------------------------------
    frac = 1./REAL(nsplt)
    IF (nsplt .NE. 1) THEN
      CALL PUSHREALARRAY(mfx, 8*(bd%ie-bd%is+2)*(bd%je-bd%js+1)*npz&
&                   /8)
      CALL PUSHREALARRAY(mfy, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+2)*npz&
&                   /8)
      CALL PUSHREALARRAY(cx, 8*(bd%ie-bd%is+2)*(bd%jed-bd%jsd+1)*&
&                   npz/8)
      CALL PUSHREALARRAY(cy, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+2)*&
&                   npz/8)
      CALL PUSHREALARRAY(frac)
!$OMP parallel do default(none) shared(is,ie,js,je,isd,ied,jsd,jed,npz,cx,frac,xfx,mfx,cy,yfx,mfy)
      DO k=1,npz
        DO j=jsd,jed
          DO i=is,ie+1
            cx(i, j, k) = cx(i, j, k)*frac
            xfx(i, j, k) = xfx(i, j, k)*frac
          END DO
        END DO
        DO j=js,je
          DO i=is,ie+1
            mfx(i, j, k) = mfx(i, j, k)*frac
          END DO
        END DO
        DO j=js,je+1
          DO i=isd,ied
            cy(i, j, k) = cy(i, j, k)*frac
            yfx(i, j, k) = yfx(i, j, k)*frac
          END DO
        END DO
        DO j=js,je+1
          DO i=is,ie
            mfy(i, j, k) = mfy(i, j, k)*frac
          END DO
        END DO
      END DO
      CALL PUSHCONTROL1B_FV(1)
    ELSE
      CALL PUSHCONTROL1B_FV(0)
    END IF
    DO it=1,nsplt
      CALL PUSHREALARRAY(dp2, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+1)*npz&
&                   /8)
      CALL PUSHREALARRAY(ra_x, 8*(bd%ie-bd%is+1)*(bd%jed-bd%jsd+1)*&
&                   npz/8)
      CALL PUSHREALARRAY(ra_y, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+1)*&
&                   npz/8)
!call timing_off('COMM_TRACER')
!call timing_off('COMM_TOTAL')
!$OMP parallel do default(none) shared(is,ie,js,je,isd,ied,jsd,jed,npz,dp1,mfx,mfy,rarea,nq, &
!$OMP                                  area,xfx,yfx,q,cx,cy,npx,npy,hord,gridstruct,bd,it,nsplt) &
!$OMP                          private(dp2, ra_x, ra_y, fx, fy)
      DO k=1,npz
        DO j=js,je
          DO i=is,ie
            dp2(i, j, k) = dp1(i, j, k) + (mfx(i, j, k)-mfx(i+1, j, k)+&
&             mfy(i, j, k)-mfy(i, j+1, k))*gridstruct%rarea(i, j)
          END DO
        END DO
        DO j=jsd,jed
          DO i=is,ie
            ra_x(i, j, k) = gridstruct%area(i, j) + xfx(i, j, k) - xfx(i&
&             +1, j, k)
          END DO
        END DO
        DO j=js,je
          DO i=isd,ied
            ra_y(i, j, k) = gridstruct%area(i, j) + yfx(i, j, k) - yfx(i&
&             , j+1, k)
          END DO
        END DO
      END DO
! npz
      DO k=1,npz
        DO iq=qs,nq
          IF (hord .EQ. hord_pert) THEN
            CALL FV_TP_2D_FWD(q(isd:ied, jsd:jed, k, iq), cx(is:ie+1&
&                          , jsd:jed, k), cy(isd:ied, js:je+1, k), npx, &
&                          npy, hord, fx(is:ie+1, js:je, k, iq), fy(is:&
&                          ie, js:je+1, k, iq), xfx(is:ie+1, jsd:jed, k)&
&                          , yfx(isd:ied, js:je+1, k), gridstruct, bd, &
&                          ra_x(is:ie, jsd:jed, k), ra_y(isd:ied, js:je&
&                          , k), mfx=mfx(is:ie+1, js:je, k), mfy=mfy(is:&
&                          ie, js:je+1, k))
            CALL PUSHCONTROL1B_FV(1)
          ELSE
            CALL PUSHREALARRAY(fy(is:ie, js:je+1, k, iq), 8*(ie-is+&
&                         1)*(je-js+2)/8)
            CALL PUSHREALARRAY(fx(is:ie+1, js:je, k, iq), 8*(ie-is+&
&                         2)*(je-js+1)/8)
            CALL PUSHREALARRAY(q(isd:ied, jsd:jed, k, iq), 8*(ied-&
&                         isd+1)*(jed-jsd+1)/8)
            CALL FV_TP_2D(q(isd:ied, jsd:jed, k, iq), cx(is:ie+1, jsd:&
&                   jed, k), cy(isd:ied, js:je+1, k), npx, npy, &
&                   hord_pert, fx(is:ie+1, js:je, k, iq), fy(is:ie, js:&
&                   je+1, k, iq), xfx(is:ie+1, jsd:jed, k), yfx(isd:ied&
&                   , js:je+1, k), gridstruct, bd, ra_x(is:ie, jsd:jed, &
&                   k), ra_y(isd:ied, js:je, k), mfx=mfx(is:ie+1, js:je&
&                   , k), mfy=mfy(is:ie, js:je+1, k))
            CALL PUSHCONTROL1B_FV(0)
          END IF
        END DO
      END DO
      CALL PUSHREALARRAY(q, 8*(bd%ied-bd%isd+1)*(bd%jed-bd%jsd+1)*&
&                   npz*nq/8)
! npz
      DO k=1,npz
        DO iq=qs,nq
          DO j=js,je
            DO i=is,ie
              q(i, j, k, iq) = (q(i, j, k, iq)*dp1(i, j, k)+(fx(i, j, k&
&               , iq)-fx(i+1, j, k, iq)+fy(i, j, k, iq)-fy(i, j+1, k, iq&
&               ))*gridstruct%rarea(i, j))/dp2(i, j, k)
            END DO
          END DO
        END DO
      END DO
! npz
      IF (it .NE. nsplt) THEN
        CALL PUSHREALARRAY(dp1, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+1)*&
&                     npz/8)
        DO k=1,npz
          DO j=js,je
            DO i=is,ie
              dp1(i, j, k) = dp2(i, j, k)
            END DO
          END DO
        END DO
        CALL PUSHCONTROL1B_FV(0)
      ELSE
        CALL PUSHCONTROL1B_FV(1)
      END IF
! npz
      IF (it .NE. nsplt) THEN
!call timing_on('COMM_TOTAL')
!call timing_on('COMM_TRACER')
        CALL PUSHREALARRAY(q, 8*(bd%ied-bd%isd+1)*(bd%jed-bd%jsd+1)&
&                     *npz*nq/8)
        CALL START_GROUP_HALO_UPDATE(q_pack, q_packp, q, domain)
!call timing_off('COMM_TRACER')
!call timing_off('COMM_TOTAL')
        CALL PUSHCONTROL1B_FV(1)
      ELSE
        CALL PUSHCONTROL1B_FV(0)
      END IF
    END DO
    IF (id_divg .GT. 0) THEN
      rdt = 1./(frac*dt)
      CALL PUSHREALARRAY(dp1, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+1)*npz&
&                   /8)
!$OMP parallel do default(none) shared(is,ie,js,je,npz,dp1,xfx,yfx,rarea,rdt)
      DO k=1,npz
        DO j=js,je
          DO i=is,ie
            dp1(i, j, k) = (xfx(i+1, j, k)-xfx(i, j, k)+yfx(i, j+1, k)-&
&             yfx(i, j, k))*gridstruct%rarea(i, j)*rdt
          END DO
        END DO
      END DO
      CALL PUSHINTEGER(js)
      CALL PUSHREALARRAY(xfx, 8*(bd%ie-bd%is+2)*(bd%jed-bd%jsd+1)*&
&                   npz/8)
      CALL PUSHREALARRAY(dp2, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+1)*npz&
&                   /8)
      CALL PUSHREALARRAY(frac)
      CALL PUSHINTEGER(ie)
      CALL PUSHREALARRAY(ra_x, 8*(bd%ie-bd%is+1)*(bd%jed-bd%jsd+1)*&
&                   npz/8)
      CALL PUSHREALARRAY(ra_y, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+1)*&
&                   npz/8)
      CALL PUSHREALARRAY(yfx, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+2)*&
&                   npz/8)
      CALL PUSHINTEGER(is)
      CALL PUSHREALARRAY(rdt)
      CALL PUSHINTEGER(nsplt)
      CALL PUSHREALARRAY(fx, 8*(bd%ie-bd%is+2)*(bd%je-bd%js+1)*npz*&
&                   nq/8)
      CALL PUSHREALARRAY(fy, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+2)*npz*&
&                   nq/8)
      CALL PUSHINTEGER(je)
      CALL PUSHCONTROL1B_FV(1)
    ELSE
      CALL PUSHINTEGER(js)
      CALL PUSHREALARRAY(xfx, 8*(bd%ie-bd%is+2)*(bd%jed-bd%jsd+1)*&
&                   npz/8)
      CALL PUSHREALARRAY(dp2, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+1)*npz&
&                   /8)
      CALL PUSHREALARRAY(frac)
      CALL PUSHINTEGER(ie)
      CALL PUSHREALARRAY(ra_x, 8*(bd%ie-bd%is+1)*(bd%jed-bd%jsd+1)*&
&                   npz/8)
      CALL PUSHREALARRAY(ra_y, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+1)*&
&                   npz/8)
      CALL PUSHREALARRAY(yfx, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+2)*&
&                   npz/8)
      CALL PUSHINTEGER(is)
      CALL PUSHINTEGER(nsplt)
      CALL PUSHREALARRAY(fx, 8*(bd%ie-bd%is+2)*(bd%je-bd%js+1)*npz*&
&                   nq/8)
      CALL PUSHREALARRAY(fy, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+2)*npz*&
&                   nq/8)
      CALL PUSHINTEGER(je)
      CALL PUSHCONTROL1B_FV(0)
    END IF
  END SUBROUTINE TRACER_2D_FWD
!  Differentiation of tracer_2d in reverse (adjoint) mode, backward sweep (with options r8 split(a2b_edge_mod.a2b_ord2 a2b_edge_m
!od.a2b_ord4 a2b_edge_mod.extrap_corner  dyn_core_mod.dyn_core dyn_core_mod.pk3_halo dyn_core_mod.pln_halo dyn_core_mod.pe_
!halo dyn_core_mod.adv_pe dyn_core_mod.p_grad_c dyn_core_mod.nh_p_grad dyn_core_mod.split_p_grad dyn_core_mod.one_grad_p dyn_core
!_mod.grad1_p_update dyn_core_mod.mix_dp dyn_core_mod.geopk dyn_core_mod.del2_cubed fv_dynamics_mod.fv_dynamics fv_dynamics_mod.r
!ayleigh_super fv_dynamics_mod.rayleigh_friction fv_dynamics_mod.compute_aam fv_dynamics_mod.geos_to_fv3 fv_grid_utils_mod.cubed_
!to_latlon fv_grid_utils_mod.c2l_ord4 fv_grid_utils_mod.c2l_ord2 fv_mapz_mod.lagrangian_to_eulerian fv_mapz_mod.compute_total_ene
!rgy fv_mapz_mod.pkez fv_mapz_mod.remap_z fv_mapz_mod.map_scalar fv_mapz_mod.map1_ppm fv_mapz_mod.mapn_tracer fv_mapz
!_mod.map1_q2 fv_mapz_mod.map1_cubic fv_mapz_mod.scalar_profile_linear fv_mapz_mod.cs_profile_linear fv_mapz_mod.steepz 
!fv_tracer2d_mod.tracer_2d fv_tracer2d_mod.tracer_2d_nested nh_core_mod.update_dz_c nh_core_mod.update_dz_d nh_core_mod.riem_solv
!er_c nh_core_mod.riem_solver3 nh_core_mod.rim_2d nh_core_mod.sim3_solver nh_core_mod.sim3p0_solver nh_core_mod.sim1_solver nh_co
!re_mod.sim_solver nh_core_mod.edge_profile sw_core_mod.c_sw sw_core_mod.d_sw sw_core_mod.divergence_corner sw_core_mod.divergenc
!e_corner_nest sw_core_mod.d2a2c_vect sw_core_mod.edge_interpolate4 sw_core_mod.fill2_4corners sw_core_mod.fill_4corners sw_core_
!mod.compute_div_damping sw_core_mod.smag_corner sw_core_mod.ytp_v sw_core_mod.xtp_u tp_core_mod.fv_tp_2d tp_core_
!mod.copy_corners tp_core_mod.xtp tp_core_mod.ytp tp_core_mod.xppm0 tp_core_mod.yppm0 tp_core_mod.fxppm tp_core
!_mod.fyppm tp_core_mod.deln_flux)):
!   gradient     of useful results: q dp1 mfx mfy cx cy
!   with respect to varying inputs: q dp1 mfx mfy cx cy
  SUBROUTINE TRACER_2D_BWD(q, q_ad, dp1, dp1_ad, mfx, mfx_ad, mfy, &
&   mfy_ad, cx, cx_ad, cy, cy_ad, gridstruct, bd, domain, npx, npy, npz&
&   , qs, nq, hord, hord_pert, q_split, dt, id_divg, q_pack, q_packp, &
&   z_tracer, k_split, dpa)
    IMPLICIT NONE
    TYPE(FV_GRID_BOUNDS_TYPE), INTENT(IN) :: bd
    INTEGER, INTENT(IN) :: npx
    INTEGER, INTENT(IN) :: npy
    INTEGER, INTENT(IN) :: npz
    INTEGER, INTENT(IN) :: qs
    INTEGER, INTENT(IN) :: nq
    INTEGER, INTENT(IN) :: hord, hord_pert
    INTEGER, INTENT(IN) :: q_split, k_split
    INTEGER, INTENT(IN) :: id_divg
    REAL(fvprc), INTENT(IN) :: dt
    LOGICAL, INTENT(IN) :: z_tracer
    TYPE(GROUP_HALO_UPDATE_TYPE), INTENT(INOUT) :: q_pack, q_packp
    REAL(fvprc), INTENT(INOUT) :: q(bd%isd:bd%ied, bd%jsd:bd%jed, npz, &
&   nq)
    REAL(fvprc), INTENT(INOUT) :: q_ad(bd%isd:bd%ied, bd%jsd:bd%jed, npz&
&   , nq)
    REAL(fvprc), INTENT(INOUT) :: dp1(bd%is:bd%ie, bd%js:bd%je, npz)
    REAL(fvprc), INTENT(INOUT) :: dp1_ad(bd%is:bd%ie, bd%js:bd%je, npz)
    REAL(fvprc), INTENT(INOUT) :: mfx(bd%is:bd%ie+1, bd%js:bd%je, npz)
    REAL(fvprc), INTENT(INOUT) :: mfx_ad(bd%is:bd%ie+1, bd%js:bd%je, npz&
&   )
    REAL(fvprc), INTENT(INOUT) :: mfy(bd%is:bd%ie, bd%js:bd%je+1, npz)
    REAL(fvprc), INTENT(INOUT) :: mfy_ad(bd%is:bd%ie, bd%js:bd%je+1, npz&
&   )
    REAL(fvprc), INTENT(INOUT) :: cx(bd%is:bd%ie+1, bd%jsd:bd%jed, npz)
    REAL(fvprc), INTENT(INOUT) :: cx_ad(bd%is:bd%ie+1, bd%jsd:bd%jed, &
&   npz)
    REAL(fvprc), INTENT(INOUT) :: cy(bd%isd:bd%ied, bd%js:bd%je+1, npz)
    REAL(fvprc), INTENT(INOUT) :: cy_ad(bd%isd:bd%ied, bd%js:bd%je+1, &
&   npz)
    TYPE(FV_GRID_TYPE), INTENT(IN), TARGET :: gridstruct
    TYPE(DOMAIN2D), INTENT(INOUT) :: domain
    REAL(fvprc), OPTIONAL, INTENT(OUT) :: dpa(bd%is:bd%ie, bd%js:bd%je, &
&   npz)
    REAL(fvprc) :: dp2(bd%is:bd%ie, bd%js:bd%je, npz)
    REAL(fvprc) :: dp2_ad(bd%is:bd%ie, bd%js:bd%je, npz)
    REAL(fvprc) :: fx(bd%is:bd%ie+1, bd%js:bd%je, npz, nq)
    REAL(fvprc) :: fx_ad(bd%is:bd%ie+1, bd%js:bd%je, npz, nq)
    REAL(fvprc) :: fy(bd%is:bd%ie, bd%js:bd%je+1, npz, nq)
    REAL(fvprc) :: fy_ad(bd%is:bd%ie, bd%js:bd%je+1, npz, nq)
    REAL(fvprc) :: ra_x(bd%is:bd%ie, bd%jsd:bd%jed, npz)
    REAL(fvprc) :: ra_x_ad(bd%is:bd%ie, bd%jsd:bd%jed, npz)
    REAL(fvprc) :: ra_y(bd%isd:bd%ied, bd%js:bd%je, npz)
    REAL(fvprc) :: ra_y_ad(bd%isd:bd%ied, bd%js:bd%je, npz)
    REAL(fvprc) :: xfx(bd%is:bd%ie+1, bd%jsd:bd%jed, npz)
    REAL(fvprc) :: xfx_ad(bd%is:bd%ie+1, bd%jsd:bd%jed, npz)
    REAL(fvprc) :: yfx(bd%isd:bd%ied, bd%js:bd%je+1, npz)
    REAL(fvprc) :: yfx_ad(bd%isd:bd%ied, bd%js:bd%je+1, npz)
    REAL(fvprc) :: cmax(npz)
    REAL(fvprc) :: cmax_t
    REAL(fvprc) :: c_global
    REAL(fvprc) :: frac, rdt
    INTEGER :: nsplt
    INTEGER :: i, j, k, it, iq
    REAL(fvprc) :: q_tj(bd%isd:bd%ied, bd%jsd:bd%jed, npz, nq)
    REAL(fvprc) :: fx_tj(bd%is:bd%ie+1, bd%js:bd%je, npz, nq)
    REAL(fvprc) :: fy_tj(bd%is:bd%ie, bd%js:bd%je+1, npz, nq)
    INTEGER :: is, ie, js, je
    INTEGER :: isd, ied, jsd, jed
    INTRINSIC ABS
    INTRINSIC MAX
    INTRINSIC INT
    INTRINSIC REAL
    INTRINSIC PRESENT
    REAL(fvprc) :: max1
    INTEGER :: branch
    REAL(fvprc) :: temp_ad
    REAL(fvprc) :: temp
    REAL(fvprc) :: temp_ad0
    REAL(fvprc) :: temp_ad1
    REAL(fvprc) :: temp_ad2
    REAL(fvprc) :: x2
    REAL(fvprc) :: x1
    REAL(fvprc) :: y2
    REAL(fvprc) :: y1
    CALL POPCONTROL1B_FV(branch)
    IF (branch .EQ. 0) THEN
      CALL POPINTEGER(je)
      CALL POPREALARRAY(fy, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+2)*npz*&
&                  nq/8)
      CALL POPREALARRAY(fx, 8*(bd%ie-bd%is+2)*(bd%je-bd%js+1)*npz*&
&                  nq/8)
      CALL POPINTEGER(nsplt)
      CALL POPINTEGER(is)
      CALL POPREALARRAY(yfx, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+2)*&
&                  npz/8)
      CALL POPREALARRAY(ra_y, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+1)*&
&                  npz/8)
      CALL POPREALARRAY(ra_x, 8*(bd%ie-bd%is+1)*(bd%jed-bd%jsd+1)*&
&                  npz/8)
      CALL POPINTEGER(ie)
      CALL POPREALARRAY(frac)
      CALL POPREALARRAY(dp2, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+1)*npz/&
&                  8)
      CALL POPREALARRAY(xfx, 8*(bd%ie-bd%is+2)*(bd%jed-bd%jsd+1)*&
&                  npz/8)
      CALL POPINTEGER(js)
      xfx_ad = 0.0_FVPRC
      yfx_ad = 0.0_FVPRC
    ELSE
      CALL POPINTEGER(je)
      CALL POPREALARRAY(fy, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+2)*npz*&
&                  nq/8)
      CALL POPREALARRAY(fx, 8*(bd%ie-bd%is+2)*(bd%je-bd%js+1)*npz*&
&                  nq/8)
      CALL POPINTEGER(nsplt)
      CALL POPREALARRAY(rdt)
      CALL POPINTEGER(is)
      CALL POPREALARRAY(yfx, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+2)*&
&                  npz/8)
      CALL POPREALARRAY(ra_y, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+1)*&
&                  npz/8)
      CALL POPREALARRAY(ra_x, 8*(bd%ie-bd%is+1)*(bd%jed-bd%jsd+1)*&
&                  npz/8)
      CALL POPINTEGER(ie)
      CALL POPREALARRAY(frac)
      CALL POPREALARRAY(dp2, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+1)*npz/&
&                  8)
      CALL POPREALARRAY(xfx, 8*(bd%ie-bd%is+2)*(bd%jed-bd%jsd+1)*&
&                  npz/8)
      CALL POPINTEGER(js)
      xfx_ad = 0.0_FVPRC
      yfx_ad = 0.0_FVPRC
      CALL POPREALARRAY(dp1, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+1)*npz/&
&                  8)
      DO k=1,npz
        DO j=je,js,-1
          DO i=ie,is,-1
            temp_ad2 = gridstruct%rarea(i, j)*rdt*dp1_ad(i, j, k)
            xfx_ad(i+1, j, k) = xfx_ad(i+1, j, k) + temp_ad2
            xfx_ad(i, j, k) = xfx_ad(i, j, k) - temp_ad2
            yfx_ad(i, j+1, k) = yfx_ad(i, j+1, k) + temp_ad2
            yfx_ad(i, j, k) = yfx_ad(i, j, k) - temp_ad2
            dp1_ad(i, j, k) = 0.0_FVPRC
          END DO
        END DO
      END DO
    END IF
    jsd = bd%jsd
    ied = bd%ied
    isd = bd%isd
    jed = bd%jed
    dp2_ad = 0.0_FVPRC
    ra_x_ad = 0.0_FVPRC
    ra_y_ad = 0.0_FVPRC
    fx_ad = 0.0_FVPRC
    fy_ad = 0.0_FVPRC
    DO it=nsplt,1,-1
      CALL POPCONTROL1B_FV(branch)
      IF (branch .NE. 0) THEN
        CALL POPREALARRAY(q, 8*(bd%ied-bd%isd+1)*(bd%jed-bd%jsd+1)*&
&                    npz*nq/8)
        CALL START_GROUP_HALO_UPDATE_ADM(q_pack, q_packp, q, q_ad, &
&                                  domain)
      END IF
      CALL POPCONTROL1B_FV(branch)
      IF (branch .EQ. 0) THEN
        CALL POPREALARRAY(dp1, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+1)*&
&                    npz/8)
        DO k=1,npz
          DO j=je,js,-1
            DO i=ie,is,-1
              dp2_ad(i, j, k) = dp2_ad(i, j, k) + dp1_ad(i, j, k)
              dp1_ad(i, j, k) = 0.0_FVPRC
            END DO
          END DO
        END DO
      END IF
      CALL POPREALARRAY(q, 8*(bd%ied-bd%isd+1)*(bd%jed-bd%jsd+1)*&
&                  npz*nq/8)
      DO k=1,npz
        DO iq=nq,qs,-1
          DO j=je,js,-1
            DO i=ie,is,-1
              temp_ad0 = q_ad(i, j, k, iq)/dp2(i, j, k)
              temp = q(i, j, k, iq)
              temp_ad1 = gridstruct%rarea(i, j)*temp_ad0
              dp1_ad(i, j, k) = dp1_ad(i, j, k) + temp*temp_ad0
              fx_ad(i, j, k, iq) = fx_ad(i, j, k, iq) + temp_ad1
              fx_ad(i+1, j, k, iq) = fx_ad(i+1, j, k, iq) - temp_ad1
              fy_ad(i, j, k, iq) = fy_ad(i, j, k, iq) + temp_ad1
              fy_ad(i, j+1, k, iq) = fy_ad(i, j+1, k, iq) - temp_ad1
              dp2_ad(i, j, k) = dp2_ad(i, j, k) - (temp*dp1(i, j, k)+&
&               gridstruct%rarea(i, j)*(fx(i, j, k, iq)-fx(i+1, j, k, iq&
&               )+fy(i, j, k, iq)-fy(i, j+1, k, iq)))*temp_ad0/dp2(i, j&
&               , k)
              q_ad(i, j, k, iq) = dp1(i, j, k)*temp_ad0
            END DO
          END DO
        END DO
      END DO
      DO k=npz,1,-1
        DO iq=nq,qs,-1
          CALL POPCONTROL1B_FV(branch)
          IF (branch .EQ. 0) THEN
            CALL POPREALARRAY(q(isd:ied, jsd:jed, k, iq), 8*(ied-&
&                        isd+1)*(jed-jsd+1)/8)
            CALL POPREALARRAY(fx(is:ie+1, js:je, k, iq), 8*(ie-is+2&
&                        )*(je-js+1)/8)
            CALL POPREALARRAY(fy(is:ie, js:je+1, k, iq), 8*(ie-is+1&
&                        )*(je-js+2)/8)
            CALL FV_TP_2D_ADM(q(isd:ied, jsd:jed, k, iq), q_ad(isd:ied, &
&                       jsd:jed, k, iq), cx(is:ie+1, jsd:jed, k), cx_ad(&
&                       is:ie+1, jsd:jed, k), cy(isd:ied, js:je+1, k), &
&                       cy_ad(isd:ied, js:je+1, k), npx, npy, hord_pert&
&                       , fx(is:ie+1, js:je, k, iq), fx_ad(is:ie+1, js:&
&                       je, k, iq), fy(is:ie, js:je+1, k, iq), fy_ad(is:&
&                       ie, js:je+1, k, iq), xfx(is:ie+1, jsd:jed, k), &
&                       xfx_ad(is:ie+1, jsd:jed, k), yfx(isd:ied, js:je+&
&                       1, k), yfx_ad(isd:ied, js:je+1, k), gridstruct, &
&                       bd, ra_x(is:ie, jsd:jed, k), ra_x_ad(is:ie, jsd:&
&                       jed, k), ra_y(isd:ied, js:je, k), ra_y_ad(isd:&
&                       ied, js:je, k), mfx(is:ie+1, js:je, k), mfx_ad(&
&                       is:ie+1, js:je, k), mfy(is:ie, js:je+1, k), &
&                       mfy_ad(is:ie, js:je+1, k))
          ELSE
            CALL FV_TP_2D_BWD(q(isd:ied, jsd:jed, k, iq), q_ad(isd:&
&                          ied, jsd:jed, k, iq), cx(is:ie+1, jsd:jed, k)&
&                          , cx_ad(is:ie+1, jsd:jed, k), cy(isd:ied, js:&
&                          je+1, k), cy_ad(isd:ied, js:je+1, k), npx, &
&                          npy, hord, fx(is:ie+1, js:je, k, iq), fx_ad(&
&                          is:ie+1, js:je, k, iq), fy(is:ie, js:je+1, k&
&                          , iq), fy_ad(is:ie, js:je+1, k, iq), xfx(is:&
&                          ie+1, jsd:jed, k), xfx_ad(is:ie+1, jsd:jed, k&
&                          ), yfx(isd:ied, js:je+1, k), yfx_ad(isd:ied, &
&                          js:je+1, k), gridstruct, bd, ra_x(is:ie, jsd:&
&                          jed, k), ra_x_ad(is:ie, jsd:jed, k), ra_y(isd&
&                          :ied, js:je, k), ra_y_ad(isd:ied, js:je, k), &
&                          mfx(is:ie+1, js:je, k), mfx_ad(is:ie+1, js:je&
&                          , k), mfy(is:ie, js:je+1, k), mfy_ad(is:ie, &
&                          js:je+1, k))
          END IF
        END DO
      END DO
      CALL POPREALARRAY(ra_y, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+1)*&
&                  npz/8)
      CALL POPREALARRAY(ra_x, 8*(bd%ie-bd%is+1)*(bd%jed-bd%jsd+1)*&
&                  npz/8)
      CALL POPREALARRAY(dp2, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+1)*npz/&
&                  8)
      DO k=1,npz
        DO j=je,js,-1
          DO i=ied,isd,-1
            yfx_ad(i, j, k) = yfx_ad(i, j, k) + ra_y_ad(i, j, k)
            yfx_ad(i, j+1, k) = yfx_ad(i, j+1, k) - ra_y_ad(i, j, k)
            ra_y_ad(i, j, k) = 0.0_FVPRC
          END DO
        END DO
        DO j=jed,jsd,-1
          DO i=ie,is,-1
            xfx_ad(i, j, k) = xfx_ad(i, j, k) + ra_x_ad(i, j, k)
            xfx_ad(i+1, j, k) = xfx_ad(i+1, j, k) - ra_x_ad(i, j, k)
            ra_x_ad(i, j, k) = 0.0_FVPRC
          END DO
        END DO
        DO j=je,js,-1
          DO i=ie,is,-1
            temp_ad = gridstruct%rarea(i, j)*dp2_ad(i, j, k)
            dp1_ad(i, j, k) = dp1_ad(i, j, k) + dp2_ad(i, j, k)
            mfx_ad(i, j, k) = mfx_ad(i, j, k) + temp_ad
            mfx_ad(i+1, j, k) = mfx_ad(i+1, j, k) - temp_ad
            mfy_ad(i, j, k) = mfy_ad(i, j, k) + temp_ad
            mfy_ad(i, j+1, k) = mfy_ad(i, j+1, k) - temp_ad
            dp2_ad(i, j, k) = 0.0_FVPRC
          END DO
        END DO
      END DO
    END DO
    CALL POPCONTROL1B_FV(branch)
    IF (branch .NE. 0) THEN
      CALL POPREALARRAY(frac)
      CALL POPREALARRAY(cy, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+2)*npz&
&                  /8)
      CALL POPREALARRAY(cx, 8*(bd%ie-bd%is+2)*(bd%jed-bd%jsd+1)*npz&
&                  /8)
      CALL POPREALARRAY(mfy, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+2)*npz/&
&                  8)
      CALL POPREALARRAY(mfx, 8*(bd%ie-bd%is+2)*(bd%je-bd%js+1)*npz/&
&                  8)
      DO k=1,npz
        DO j=je+1,js,-1
          DO i=ie,is,-1
            mfy_ad(i, j, k) = frac*mfy_ad(i, j, k)
          END DO
        END DO
        DO j=je+1,js,-1
          DO i=ied,isd,-1
            yfx_ad(i, j, k) = frac*yfx_ad(i, j, k)
            cy_ad(i, j, k) = frac*cy_ad(i, j, k)
          END DO
        END DO
        DO j=je,js,-1
          DO i=ie+1,is,-1
            mfx_ad(i, j, k) = frac*mfx_ad(i, j, k)
          END DO
        END DO
        DO j=jed,jsd,-1
          DO i=ie+1,is,-1
            xfx_ad(i, j, k) = frac*xfx_ad(i, j, k)
            cx_ad(i, j, k) = frac*cx_ad(i, j, k)
          END DO
        END DO
      END DO
    END IF
    CALL POPCONTROL2B_FV(branch)
    IF (branch .EQ. 0) THEN
      DO k=npz,2,-1
        CALL POPCONTROL1B_FV(branch)
      END DO
    ELSE IF (branch .NE. 1) THEN
      GOTO 100
    END IF
    DO k=npz,1,-1
      CALL POPCONTROL1B_FV(branch)
      IF (branch .EQ. 0) THEN
        DO j=je,js,-1
          DO i=ie,is,-1
            CALL POPCONTROL1B_FV(branch)
          END DO
        END DO
      ELSE
        DO j=je,js,-1
          DO i=ie,is,-1
            CALL POPCONTROL1B_FV(branch)
          END DO
        END DO
      END IF
    END DO
 100 DO k=npz,1,-1
      DO j=je+1,js,-1
        DO i=ied,isd,-1
          CALL POPCONTROL1B_FV(branch)
          IF (branch .EQ. 0) THEN
            cy_ad(i, j, k) = cy_ad(i, j, k) + gridstruct%dya(i, j)*&
&             gridstruct%dx(i, j)*gridstruct%sin_sg(i, j, 2)*yfx_ad(i, j&
&             , k)
            yfx_ad(i, j, k) = 0.0_FVPRC
          ELSE
            cy_ad(i, j, k) = cy_ad(i, j, k) + gridstruct%dya(i, j-1)*&
&             gridstruct%dx(i, j)*gridstruct%sin_sg(i, j-1, 4)*yfx_ad(i&
&             , j, k)
            yfx_ad(i, j, k) = 0.0_FVPRC
          END IF
        END DO
      END DO
      DO j=jed,jsd,-1
        DO i=ie+1,is,-1
          CALL POPCONTROL1B_FV(branch)
          IF (branch .EQ. 0) THEN
            cx_ad(i, j, k) = cx_ad(i, j, k) + gridstruct%dxa(i, j)*&
&             gridstruct%dy(i, j)*gridstruct%sin_sg(i, j, 1)*xfx_ad(i, j&
&             , k)
            xfx_ad(i, j, k) = 0.0_FVPRC
          ELSE
            cx_ad(i, j, k) = cx_ad(i, j, k) + gridstruct%dxa(i-1, j)*&
&             gridstruct%dy(i, j)*gridstruct%sin_sg(i-1, j, 3)*xfx_ad(i&
&             , j, k)
            xfx_ad(i, j, k) = 0.0_FVPRC
          END IF
        END DO
      END DO
    END DO
  END SUBROUTINE TRACER_2D_BWD
  SUBROUTINE TRACER_2D(q, dp1, mfx, mfy, cx, cy, gridstruct, bd, domain&
&   , npx, npy, npz, qs, nq, hord, hord_pert, q_split, dt, id_divg, &
&   q_pack, q_packp, z_tracer, k_split, dpa)
    IMPLICIT NONE
    TYPE(FV_GRID_BOUNDS_TYPE), INTENT(IN) :: bd
    INTEGER, INTENT(IN) :: npx
    INTEGER, INTENT(IN) :: npy
    INTEGER, INTENT(IN) :: npz
! first tracer to advect
    INTEGER, INTENT(IN) :: qs
! number of tracers to be advected
    INTEGER, INTENT(IN) :: nq
    INTEGER, INTENT(IN) :: hord, hord_pert
    INTEGER, INTENT(IN) :: q_split, k_split
    INTEGER, INTENT(IN) :: id_divg
    REAL(fvprc), INTENT(IN) :: dt
    LOGICAL, INTENT(IN) :: z_tracer
    TYPE(GROUP_HALO_UPDATE_TYPE), INTENT(INOUT) :: q_pack, q_packp
! Tracers
    REAL(fvprc), INTENT(INOUT) :: q(bd%isd:bd%ied, bd%jsd:bd%jed, npz, &
&   nq)
! DELP before dyn_core
    REAL(fvprc), INTENT(INOUT) :: dp1(bd%is:bd%ie, bd%js:bd%je, npz)
! Mass Flux X-Dir
    REAL(fvprc), INTENT(INOUT) :: mfx(bd%is:bd%ie+1, bd%js:bd%je, npz)
! Mass Flux Y-Dir
    REAL(fvprc), INTENT(INOUT) :: mfy(bd%is:bd%ie, bd%js:bd%je+1, npz)
! Courant Number X-Dir
    REAL(fvprc), INTENT(INOUT) :: cx(bd%is:bd%ie+1, bd%jsd:bd%jed, npz)
! Courant Number Y-Dir
    REAL(fvprc), INTENT(INOUT) :: cy(bd%isd:bd%ied, bd%js:bd%je+1, npz)
    TYPE(FV_GRID_TYPE), INTENT(IN), TARGET :: gridstruct
    TYPE(DOMAIN2D), INTENT(INOUT) :: domain
! DELP after advection
    REAL(fvprc), OPTIONAL, INTENT(OUT) :: dpa(bd%is:bd%ie, bd%js:bd%je, &
&   npz)
! Local Arrays
    REAL(fvprc) :: dp2(bd%is:bd%ie, bd%js:bd%je, npz)
    REAL(fvprc) :: fx(bd%is:bd%ie+1, bd%js:bd%je, npz, nq)
    REAL(fvprc) :: fy(bd%is:bd%ie, bd%js:bd%je+1, npz, nq)
    REAL(fvprc) :: ra_x(bd%is:bd%ie, bd%jsd:bd%jed, npz)
    REAL(fvprc) :: ra_y(bd%isd:bd%ied, bd%js:bd%je, npz)
    REAL(fvprc) :: xfx(bd%is:bd%ie+1, bd%jsd:bd%jed, npz)
    REAL(fvprc) :: yfx(bd%isd:bd%ied, bd%js:bd%je+1, npz)
    REAL(fvprc) :: cmax(npz)
    REAL(fvprc) :: cmax_t
    REAL(fvprc) :: c_global
    REAL(fvprc) :: frac, rdt
    INTEGER :: nsplt
    INTEGER :: i, j, k, it, iq
! Tracers
    REAL(fvprc) :: q_tj(bd%isd:bd%ied, bd%jsd:bd%jed, npz, nq)
    REAL(fvprc) :: fx_tj(bd%is:bd%ie+1, bd%js:bd%je, npz, nq)
    REAL(fvprc) :: fy_tj(bd%is:bd%ie, bd%js:bd%je+1, npz, nq)
!      real(FVPRC), pointer, dimension(:,:) :: area, rarea
!      real(FVPRC), pointer, dimension(:,:,:) :: sin_sg
!      real(FVPRC), pointer, dimension(:,:) :: dxa, dya, dx, dy
    INTEGER :: is, ie, js, je
    INTEGER :: isd, ied, jsd, jed
    INTRINSIC ABS
    INTRINSIC MAX
    INTRINSIC INT
    INTRINSIC REAL
    INTRINSIC PRESENT
    REAL(fvprc) :: max1
    REAL(fvprc) :: x2
    REAL(fvprc) :: x1
    REAL(fvprc) :: y2
    REAL(fvprc) :: y1
    is = bd%is
    ie = bd%ie
    js = bd%js
    je = bd%je
    isd = bd%isd
    ied = bd%ied
    jsd = bd%jsd
    jed = bd%jed
!       area => gridstruct%area
!      rarea => gridstruct%rarea
!      sin_sg => gridstruct%sin_sg
!      dxa    => gridstruct%dxa 
!      dya    => gridstruct%dya 
!      dx     => gridstruct%dx  
!      dy     => gridstruct%dy  
!$OMP parallel do default(none) shared(is,ie,js,je,isd,ied,jsd,jed,npz,cx,xfx,dxa,dy, &
!$OMP                                  sin_sg,cy,yfx,dya,dx)
    DO k=1,npz
      DO j=jsd,jed
        DO i=is,ie+1
          IF (cx(i, j, k) .GT. 0.) THEN
            xfx(i, j, k) = cx(i, j, k)*gridstruct%dxa(i-1, j)*gridstruct&
&             %dy(i, j)*gridstruct%sin_sg(i-1, j, 3)
          ELSE
            xfx(i, j, k) = cx(i, j, k)*gridstruct%dxa(i, j)*gridstruct%&
&             dy(i, j)*gridstruct%sin_sg(i, j, 1)
          END IF
        END DO
      END DO
      DO j=js,je+1
        DO i=isd,ied
          IF (cy(i, j, k) .GT. 0.) THEN
            yfx(i, j, k) = cy(i, j, k)*gridstruct%dya(i, j-1)*gridstruct&
&             %dx(i, j)*gridstruct%sin_sg(i, j-1, 4)
          ELSE
            yfx(i, j, k) = cy(i, j, k)*gridstruct%dya(i, j)*gridstruct%&
&             dx(i, j)*gridstruct%sin_sg(i, j, 2)
          END IF
        END DO
      END DO
    END DO
!--------------------------------------------------------------------------------
    IF (q_split .EQ. 0) THEN
! Determine nsplt
!$OMP parallel do default(none) shared(is,ie,js,je,npz,cmax,cx,cy,sin_sg) &
!$OMP                          private(cmax_t )
      DO k=1,npz
        cmax(k) = 0.
        IF (k .LT. 4) THEN
! Top layers: C < max( abs(c_x), abs(c_y) )
          DO j=js,je
            DO i=is,ie
              IF (cx(i, j, k) .GE. 0.) THEN
                x1 = cx(i, j, k)
              ELSE
                x1 = -cx(i, j, k)
              END IF
              IF (cy(i, j, k) .GE. 0.) THEN
                y1 = cy(i, j, k)
              ELSE
                y1 = -cy(i, j, k)
              END IF
              IF (x1 .LT. y1) THEN
                cmax_t = y1
              ELSE
                cmax_t = x1
              END IF
              IF (cmax_t .LT. cmax(k)) THEN
                cmax(k) = cmax(k)
              ELSE
                cmax(k) = cmax_t
              END IF
            END DO
          END DO
        ELSE
          DO j=js,je
            DO i=is,ie
              IF (cx(i, j, k) .GE. 0.) THEN
                x2 = cx(i, j, k)
              ELSE
                x2 = -cx(i, j, k)
              END IF
              IF (cy(i, j, k) .GE. 0.) THEN
                y2 = cy(i, j, k)
              ELSE
                y2 = -cy(i, j, k)
              END IF
              IF (x2 .LT. y2) THEN
                max1 = y2
              ELSE
                max1 = x2
              END IF
              cmax_t = max1 + 1. - gridstruct%sin_sg(i, j, 5)
              IF (cmax_t .LT. cmax(k)) THEN
                cmax(k) = cmax(k)
              ELSE
                cmax(k) = cmax_t
              END IF
            END DO
          END DO
        END IF
      END DO
      CALL MP_REDUCE_MAX(cmax, npz)
! find global max courant number and define nsplt to scale cx,cy,mfx,mfy
      c_global = cmax(1)
      IF (npz .NE. 1) THEN
! if NOT shallow water test case
        DO k=2,npz
          IF (cmax(k) .LT. c_global) THEN
            c_global = c_global
          ELSE
            c_global = cmax(k)
          END IF
        END DO
      END IF
      nsplt = INT(1. + c_global)
!if ( is_master() .and. nsplt > 3 )  write(*,*) 'Tracer_2d_split=', nsplt, c_global
    ELSE
      nsplt = q_split
    END IF
!--------------------------------------------------------------------------------
    frac = 1./REAL(nsplt)
    IF (nsplt .NE. 1) THEN
!$OMP parallel do default(none) shared(is,ie,js,je,isd,ied,jsd,jed,npz,cx,frac,xfx,mfx,cy,yfx,mfy)
      DO k=1,npz
        DO j=jsd,jed
          DO i=is,ie+1
            cx(i, j, k) = cx(i, j, k)*frac
            xfx(i, j, k) = xfx(i, j, k)*frac
          END DO
        END DO
        DO j=js,je
          DO i=is,ie+1
            mfx(i, j, k) = mfx(i, j, k)*frac
          END DO
        END DO
        DO j=js,je+1
          DO i=isd,ied
            cy(i, j, k) = cy(i, j, k)*frac
            yfx(i, j, k) = yfx(i, j, k)*frac
          END DO
        END DO
        DO j=js,je+1
          DO i=is,ie
            mfy(i, j, k) = mfy(i, j, k)*frac
          END DO
        END DO
      END DO
    END IF
    DO it=1,nsplt
!call timing_on('COMM_TOTAL')
!call timing_on('COMM_TRACER')
      CALL COMPLETE_GROUP_HALO_UPDATE(q_pack, q_packp, domain)
!call timing_off('COMM_TRACER')
!call timing_off('COMM_TOTAL')
!$OMP parallel do default(none) shared(is,ie,js,je,isd,ied,jsd,jed,npz,dp1,mfx,mfy,rarea,nq, &
!$OMP                                  area,xfx,yfx,q,cx,cy,npx,npy,hord,gridstruct,bd,it,nsplt) &
!$OMP                          private(dp2, ra_x, ra_y, fx, fy)
      DO k=1,npz
        DO j=js,je
          DO i=is,ie
            dp2(i, j, k) = dp1(i, j, k) + (mfx(i, j, k)-mfx(i+1, j, k)+&
&             mfy(i, j, k)-mfy(i, j+1, k))*gridstruct%rarea(i, j)
          END DO
        END DO
        DO j=jsd,jed
          DO i=is,ie
            ra_x(i, j, k) = gridstruct%area(i, j) + xfx(i, j, k) - xfx(i&
&             +1, j, k)
          END DO
        END DO
        DO j=js,je
          DO i=isd,ied
            ra_y(i, j, k) = gridstruct%area(i, j) + yfx(i, j, k) - yfx(i&
&             , j+1, k)
          END DO
        END DO
      END DO
! npz
      DO k=1,npz
        DO iq=qs,nq
          IF (hord .EQ. hord_pert) THEN
            CALL FV_TP_2D(q(isd:ied, jsd:jed, k, iq), cx(is:ie+1, jsd&
&                      :jed, k), cy(isd:ied, js:je+1, k), npx, npy, hord&
&                      , fx(is:ie+1, js:je, k, iq), fy(is:ie, js:je+1, k&
&                      , iq), xfx(is:ie+1, jsd:jed, k), yfx(isd:ied, js:&
&                      je+1, k), gridstruct, bd, ra_x(is:ie, jsd:jed, k)&
&                      , ra_y(isd:ied, js:je, k), mfx=mfx(is:ie+1, js:je&
&                      , k), mfy=mfy(is:ie, js:je+1, k))
          ELSE
            q_tj(:, :, k, iq) = q(:, :, k, iq)
            fx_tj(:, :, k, iq) = fx(:, :, k, iq)
            fy_tj(:, :, k, iq) = fy(:, :, k, iq)
            CALL FV_TP_2D(q(isd:ied, jsd:jed, k, iq), cx(is:ie+1, jsd:&
&                   jed, k), cy(isd:ied, js:je+1, k), npx, npy, &
&                   hord_pert, fx(is:ie+1, js:je, k, iq), fy(is:ie, js:&
&                   je+1, k, iq), xfx(is:ie+1, jsd:jed, k), yfx(isd:ied&
&                   , js:je+1, k), gridstruct, bd, ra_x(is:ie, jsd:jed, &
&                   k), ra_y(isd:ied, js:je, k), mfx=mfx(is:ie+1, js:je&
&                   , k), mfy=mfy(is:ie, js:je+1, k))
          END IF
        END DO
      END DO
! npz
      DO k=1,npz
        DO iq=qs,nq
          DO j=js,je
            DO i=is,ie
              q(i, j, k, iq) = (q(i, j, k, iq)*dp1(i, j, k)+(fx(i, j, k&
&               , iq)-fx(i+1, j, k, iq)+fy(i, j, k, iq)-fy(i, j+1, k, iq&
&               ))*gridstruct%rarea(i, j))/dp2(i, j, k)
            END DO
          END DO
        END DO
      END DO
! npz
      IF (it .NE. nsplt) THEN
        DO k=1,npz
          DO j=js,je
            DO i=is,ie
              dp1(i, j, k) = dp2(i, j, k)
            END DO
          END DO
        END DO
      END IF
! npz
      IF (it .NE. nsplt) CALL START_GROUP_HALO_UPDATE(q_pack, q_packp, q&
&                                               , domain)
!call timing_on('COMM_TOTAL')
!call timing_on('COMM_TRACER')
!call timing_off('COMM_TRACER')
!call timing_off('COMM_TOTAL')
    END DO
! nsplt
    IF (PRESENT(dpa)) dpa = dp1
    IF (id_divg .GT. 0) THEN
      rdt = 1./(frac*dt)
!$OMP parallel do default(none) shared(is,ie,js,je,npz,dp1,xfx,yfx,rarea,rdt)
      DO k=1,npz
        DO j=js,je
          DO i=is,ie
            dp1(i, j, k) = (xfx(i+1, j, k)-xfx(i, j, k)+yfx(i, j+1, k)-&
&             yfx(i, j, k))*gridstruct%rarea(i, j)*rdt
          END DO
        END DO
      END DO
    END IF
  END SUBROUTINE TRACER_2D
!  Differentiation of tracer_2d_nested in reverse (adjoint) mode, forward sweep (with options r8 split(a2b_edge_mod.a2b_ord2 a2b_
!edge_mod.a2b_ord4 a2b_edge_mod.extrap_corner  dyn_core_mod.dyn_core dyn_core_mod.pk3_halo dyn_core_mod.pln_halo dyn_core_m
!od.pe_halo dyn_core_mod.adv_pe dyn_core_mod.p_grad_c dyn_core_mod.nh_p_grad dyn_core_mod.split_p_grad dyn_core_mod.one_grad_p dy
!n_core_mod.grad1_p_update dyn_core_mod.mix_dp dyn_core_mod.geopk dyn_core_mod.del2_cubed fv_dynamics_mod.fv_dynamics fv_dynamics
!_mod.rayleigh_super fv_dynamics_mod.rayleigh_friction fv_dynamics_mod.compute_aam fv_dynamics_mod.geos_to_fv3 fv_grid_utils_mod.
!cubed_to_latlon fv_grid_utils_mod.c2l_ord4 fv_grid_utils_mod.c2l_ord2 fv_mapz_mod.lagrangian_to_eulerian fv_mapz_mod.compute_tot
!al_energy fv_mapz_mod.pkez fv_mapz_mod.remap_z fv_mapz_mod.map_scalar fv_mapz_mod.map1_ppm fv_mapz_mod.mapn_tracer f
!v_mapz_mod.map1_q2 fv_mapz_mod.map1_cubic fv_mapz_mod.scalar_profile_linear fv_mapz_mod.cs_profile_linear fv_mapz_mod.s
!teepz fv_tracer2d_mod.tracer_2d fv_tracer2d_mod.tracer_2d_nested nh_core_mod.update_dz_c nh_core_mod.update_dz_d nh_core_mod.rie
!m_solver_c nh_core_mod.riem_solver3 nh_core_mod.rim_2d nh_core_mod.sim3_solver nh_core_mod.sim3p0_solver nh_core_mod.sim1_solver
! nh_core_mod.sim_solver nh_core_mod.edge_profile sw_core_mod.c_sw sw_core_mod.d_sw sw_core_mod.divergence_corner sw_core_mod.div
!ergence_corner_nest sw_core_mod.d2a2c_vect sw_core_mod.edge_interpolate4 sw_core_mod.fill2_4corners sw_core_mod.fill_4corners sw
!_core_mod.compute_div_damping sw_core_mod.smag_corner sw_core_mod.ytp_v sw_core_mod.xtp_u tp_core_mod.fv_tp_2d tp
!_core_mod.copy_corners tp_core_mod.xtp tp_core_mod.ytp tp_core_mod.xppm0 tp_core_mod.yppm0 tp_core_mod.fxppm t
!p_core_mod.fyppm tp_core_mod.deln_flux)):
!   gradient     of useful results: q dp1 mfx mfy cx cy
!   with respect to varying inputs: q dp1 mfx mfy cx cy
!subroutine offline_tracer_advection(q, ple0, ple1, mfx, mfy, cx, cy, &
!                                    gridstruct, neststruct, bd, domain, &
!                                    ak, bk, ptop, npx, npy, npz,   &
!                                    nq, hord, kord, q_split, k_split, dt, z_tracer, fill)
!
!      use fv_mapz_mod,        only: map1_q2
!      use fv_fill_mod,        only: fillz
!
!      integer, intent(IN) :: npx
!      integer, intent(IN) :: npy
!      integer, intent(IN) :: npz
!      integer, intent(IN) :: nq    ! number of tracers to be advected
!      integer, intent(IN) :: hord
!      integer, intent(IN) :: kord
!      integer, intent(IN) :: q_split
!      integer, intent(IN) :: k_split
!      logical, intent(IN) :: z_tracer
!      logical, intent(IN) :: fill
!      type(fv_grid_bounds_type), intent(IN   ) :: bd
!      type(fv_nest_type), intent(INOUT) :: neststruct
!      type(fv_grid_type), intent(IN), target :: gridstruct
!      type(domain2D), intent(INOUT) :: domain
!
!      real(FVPRC), intent(IN   ) :: dt
!      real(FVPRC), intent(IN   ) ::ple0(bd%is:bd%ie,bd%js:bd%je,npz+1)      ! DELP before dyn_core
!      real(FVPRC), intent(INOUT) ::ple1(bd%is:bd%ie,bd%js:bd%je,npz+1)      ! DELP after dyn_core
!      real(FVPRC), intent(IN   ) ::  cx(bd%is:bd%ie,bd%js:bd%je,npz)        ! Courant Number X-Dir
!      real(FVPRC), intent(IN   ) ::  cy(bd%is:bd%ie,bd%js:bd%je,npz)        ! Courant Number Y-Dir
!      real(FVPRC), intent(IN   ) :: mfx(bd%is:bd%ie,bd%js:bd%je,npz)        ! Mass Flux X-Dir
!      real(FVPRC), intent(IN   ) :: mfy(bd%is:bd%ie,bd%js:bd%je,npz)        ! Mass Flux Y-Dir
!      real(FVPRC), intent(INOUT) ::   q(bd%is:bd%ie,bd%js:bd%je,npz,nq)     ! Tracers
!      real(FVPRC), intent(IN   ) ::  ak(npz+1)                  ! AK for remapping
!      real(FVPRC), intent(IN   ) ::  bk(npz+1)                  ! BK for remapping
!      real(FVPRC), intent(IN   ) :: ptop
!! Local Arrays
!      real(FVPRC) ::   xL(bd%isd:bd%ied+1,bd%jsd:bd%jed  ,npz)  ! X-Dir for MPP Updates
!      real(FVPRC) ::   yL(bd%isd:bd%ied  ,bd%jsd:bd%jed+1,npz)  ! Y-Dir for MPP Updates
!      real(FVPRC) ::  cxL(bd%is :bd%ie +1,bd%jsd:bd%jed  ,npz)  ! Courant Number X-Dir
!      real(FVPRC) ::  cyL(bd%isd:bd%ied  ,bd%js :bd%je +1,npz)  ! Courant Number Y-Dir
!      real(FVPRC) :: mfxL(bd%is :bd%ie +1,bd%js :bd%je   ,npz)  ! Mass Flux X-Dir
!      real(FVPRC) :: mfyL(bd%is :bd%ie   ,bd%js :bd%je +1,npz)  ! Mass Flux Y-Dir
!      real(FVPRC) ::  dpL(bd%is :bd%ie   ,bd%js :bd%je   ,npz)  ! Pressure Thickness
!      real(FVPRC) ::  dpA(bd%is :bd%ie   ,bd%js :bd%je   ,npz)  ! Pressure Thickness
!! Local Tracer Arrays
!      real(FVPRC) ::   q1(bd%is:bd%ie  ,bd%js:bd%je, npz   )! 2D Tracers
!      real(FVPRC) ::   q2(bd%isd:bd%ied  ,bd%jsd:bd%jed     ,nq)! 2D Tracers
!      real(FVPRC) ::   q3(bd%isd:bd%ied  ,bd%jsd:bd%jed, npz,nq)! 3D Tracers
!! Local Buffer Arrarys
!      real(FVPRC) :: wbuffer(bd%js:bd%je,npz)
!      real(FVPRC) :: sbuffer(bd%is:bd%ie,npz)
!      real(FVPRC) :: ebuffer(bd%js:bd%je,npz)
!      real(FVPRC) :: nbuffer(bd%is:bd%ie,npz)
!! Local Remap Arrays
!      real(FVPRC)  pe1(bd%is:bd%ie,npz+1)
!      real(FVPRC)  pe2(bd%is:bd%ie,npz+1)
!      real(FVPRC)  dp2(bd%is:bd%ie,bd%js:bd%je,npz)
!
!! Local indices
!      integer     :: i,j,k,n,iq
!      real(FVPRC) :: dtR8
!
!      real(FVPRC) :: scalingFactor
!
!      type(group_halo_update_type), save :: i_pack
!
!      integer :: is,  ie,  js,  je
!      integer :: isd, ied, jsd, jed
!
!      is  = bd%is
!      ie  = bd%ie
!      js  = bd%js
!      je  = bd%je
!      isd = bd%isd
!      ied = bd%ied
!      jsd = bd%jsd
!      jed = bd%jed
!
!! Time-step
!    dtR8=dt
!! Fill CX/CY C-Grid boundaries and update ghost regions
!    xL(is:ie,js:je,:) = cx(:,:,:)
!    yL(is:ie,js:je,:) = cy(:,:,:)
!    call mpp_get_boundary(xL, yL, domain, &
!                          wbufferx=wbuffer, ebufferx=ebuffer, &
!                          sbuffery=sbuffer, nbuffery=nbuffer, &
!                          gridtype=CGRID_NE )
!    xL(ie+1,js:je,:) = ebuffer
!    yL(is:ie,je+1,:) = nbuffer
!    call mpp_update_domains( xL, yL, domain, gridtype=CGRID_NE, complete=.true.)
!    cxL(is:ie+1,jsd:jed,:) = xL(is:ie+1,jsd:jed,:)
!    cyL(isd:ied,js:je+1,:) = yL(isd:ied,js:je+1,:)
!
!! Fill MFX/MFY C-Grid boundaries
!    xL(is:ie,js:je,:) = mfx(:,:,:)
!    yL(is:ie,js:je,:) = mfy(:,:,:)
!    call mpp_get_boundary(xL, yL, domain, &
!                          wbufferx=wbuffer, ebufferx=ebuffer, &
!                          sbuffery=sbuffer, nbuffery=nbuffer, &
!                          gridtype=CGRID_NE )
!    xL(ie+1,js:je,:) = ebuffer
!    yL(is:ie,je+1,:) = nbuffer
!    mfxL(is:ie+1,js:je,:) = xL(is:ie+1,js:je,:)
!    mfyL(is:ie,js:je+1,:) = yL(is:ie,js:je+1,:)
!
!! Fill local tracers and pressure thickness
!    dpL(:,:,:) = ple0(:,:,2:npz+1) - ple0(:,:,1:npz)
!    q3(is:ie,js:je,:,:) = q(is:ie,js:je,:,:)
!
!    call start_group_halo_update(i_pack, q3, domain)
!
!    if ( z_tracer ) then
!!$omp parallel do default(shared) private(q2)
!       do k=1,npz
!         do iq=1,nq
!            do j=js,je
!               do i=is,ie                   ! To_do list:
!                  q2(i,j,iq) = q3(i,j,k,iq) ! The data copying can be avoided if q is
!                                            ! re-dimensioned as q(i,j,nq,k)
!               enddo
!            enddo
!         enddo
!         call tracer_2d_1L(q2, dpL(is,js,k), mfxL(is,js,k), mfyL(is,js,k), &
!                           cxL(is,jsd,k),  cyL(isd,js,k), gridstruct, neststruct, bd, domain, npx, npy, npz,   &
!                           1, nq, hord, q_split, k, q3, dtR8, 0, k_split, dpA=dpA)
!       enddo
!    else
!         call tracer_2d(q3, dpL, mfxL, mfyL, cxL, cyL, gridstruct, bd, domain, npx, npy, npz, 1, nq, &
!                        hord, q_split, dtR8, 0, i_pack, .false., k_split, dpA=dpA)
!    endif
!
!!------------------------------------------------------------------
!! Re-Map constituents
!! Do remapping one tracer at a time; seems to be faster
!! It requires less memory than mapn_ppm
!!------------------------------------------------------------------
!
!       do iq=1,nq
!          do j=js,je
!           ! pressures mapping from (dpA is new delp after tracer_2d)
!             pe1(:,1) = ptop
!             do k=2,npz+1
!               pe1(:,k) = pe1(:,k-1) + dpA(:,j,k-1)
!             enddo
!           ! pressures mapping to
!             pe2(:,1) = ptop
!             pe2(:,npz+1) = pe1(:,npz+1)
!             do k=2,npz
!                 pe2(:  ,k) = ak(k) + bk(k)*pe1(:,npz+1)
!             enddo
!             do k=1,npz
!                dp2(:,j,k) = pe2(:,k+1) - pe2(:,k)
!             enddo
!             call map1_q2(npz, pe1, q3(isd,jsd,1,iq),      &
!                          npz, pe2, q1(:,j,:), dp2(:,j,:), &
!                          is, ie, 0, kord, j,              &
!                          isd, ied, jsd, jed, 0._FVPRC) 
!             if (fill) call fillz(ie-is+1, npz, 1, q1(:,j,:), dp2(:,j,:))
!          enddo
!          ! Rescale tracers based on ple1 at destination timestep
!          !------------------------------------------------------
!
!          scalingFactor = calcScalingFactor(q1, dp2, ple1, npx, npy, npz, gridstruct, bd)
!          !scalingFactors = computeScalingFactors(q1, dp2, ple1, npx, npy, npz)
!
!          ! Return tracers
!          !---------------
!          q(is:ie,js:je,1:npz,iq) = q1(is:ie,js:je,1:npz) * scalingFactor
!          !do k =1,npz
!             !do j = js,je
!                !do i = is,ie
!                   !q(i,j,k,iq) = q1(i,j,k)
!                !enddo
!             !enddo
!          !enddo
!
!       enddo
!
!end subroutine offline_tracer_advection
!------------------------------------------------------------------------------------
!         function calcScalingFactor(q1, dp2, ple1, npx, npy, npz, gridstruct, bd) result(scaling)
!         use mpp_mod, only: mpp_sum
!         integer, intent(in) :: npx
!         integer, intent(in) :: npy
!         integer, intent(in) :: npz
!         real(FVPRC), intent(in) :: q1(:,:,:)
!         real(FVPRC), intent(in) :: dp2(:,:,:)
!         real(FVPRC), intent(in) :: ple1(:,:,:)
!         type(fv_grid_type), intent(IN   ) :: gridstruct
!         type(fv_grid_bounds_type), intent(IN   ) :: bd
!         real(FVPRC) :: scaling
!
!         integer :: k
!         real(FVPRC) :: partialSums(2,npz), globalSums(2)
!         real(FVPRC), parameter :: TINY_DENOMINATOR = tiny(1.0)
!
!         !-------
!         ! Compute partial sum on local array first to minimize communication.
!         ! This algorithm will not be strongly repdroducible under changes do domain
!         ! decomposition, but uses far less communication bandwidth (and memory BW)
!         ! then the preceding implementation.
!         !-------
!         do k = 1, npz
!            ! numerator
!            partialSums(1,k) = sum(q1(:,:,k)*dp2(:,:,k)*gridstruct%area(bd%is:bd%ie,bd%js:bd%je))
!            ! denominator
!            partialSums(2,k) = sum(q1(:,:,k)*(ple1(:,:,k+1)-ple1(:,:,k))*gridstruct%area(bd%is:bd%ie,bd%js:bd%je))
!         end do
!
!         globalSums(1) = sum(partialSums(1,:))
!         globalSums(2) = sum(partialSums(2,:))
!
!         call mpp_sum(globalSums, 2)
!
!         if (globalSums(2) > TINY_DENOMINATOR) then
!            scaling =  globalSums(1) / globalSums(2)
!            !#################################################################
!            ! This line was added to ensure strong reproducibility of the code
!            !#################################################################
!            scaling = REAL(scaling, KIND=REAL4)
!         else
!            scaling = 1.d0
!         end if
!
!         end function calcScalingFactor
  SUBROUTINE TRACER_2D_NESTED_FWD(q, dp1, mfx, mfy, cx, cy, gridstruct, &
&   bd, domain, npx, npy, npz, nq, hord, hord_pert, q_split, dt, id_divg&
&   , q_pack, q_packp, z_tracer, k_split, neststruct, parent_grid)
    IMPLICIT NONE
    TYPE(FV_GRID_BOUNDS_TYPE), INTENT(IN) :: bd
    INTEGER, INTENT(IN) :: npx
    INTEGER, INTENT(IN) :: npy
    INTEGER, INTENT(IN) :: npz
! number of tracers to be advected
    INTEGER, INTENT(IN) :: nq
    INTEGER, INTENT(IN) :: hord, hord_pert
    INTEGER, INTENT(IN) :: q_split, k_split
    INTEGER, INTENT(IN) :: id_divg
    REAL(fvprc), INTENT(IN) :: dt
    LOGICAL, INTENT(IN) :: z_tracer
    TYPE(GROUP_HALO_UPDATE_TYPE), INTENT(INOUT) :: q_pack, q_packp
! Tracers
    REAL(fvprc), INTENT(INOUT) :: q(bd%isd:bd%ied, bd%jsd:bd%jed, npz, &
&   nq)
! DELP before dyn_core
    REAL(fvprc), INTENT(INOUT) :: dp1(bd%is:bd%ie, bd%js:bd%je, npz)
! Mass Flux X-Dir
    REAL(fvprc), INTENT(INOUT) :: mfx(bd%is:bd%ie+1, bd%js:bd%je, npz)
! Mass Flux Y-Dir
    REAL(fvprc), INTENT(INOUT) :: mfy(bd%is:bd%ie, bd%js:bd%je+1, npz)
! Courant Number X-Dir
    REAL(fvprc), INTENT(INOUT) :: cx(bd%is:bd%ie+1, bd%jsd:bd%jed, npz)
! Courant Number Y-Dir
    REAL(fvprc), INTENT(INOUT) :: cy(bd%isd:bd%ied, bd%js:bd%je+1, npz)
    TYPE(FV_GRID_TYPE), INTENT(IN), TARGET :: gridstruct
    TYPE(FV_NEST_TYPE), INTENT(INOUT) :: neststruct
    TYPE(FV_ATMOS_TYPE), INTENT(INOUT) :: parent_grid
    TYPE(DOMAIN2D), INTENT(INOUT) :: domain
! Local Arrays
    REAL(fvprc) :: dp2(bd%is:bd%ie, bd%js:bd%je, npz)
!#ifdef FLUXBCS
!      real(FVPRC) :: fx(bd%is:bd%ie+1,bd%js:bd%je  ,npz,nq)
!      real(FVPRC) :: fy(bd%is:bd%ie , bd%js:bd%je+1,npz,nq)
!#else
    REAL(fvprc) :: fx(bd%is:bd%ie+1, bd%js:bd%je)
    REAL(fvprc) :: fy(bd%is:bd%ie, bd%js:bd%je+1)
!#endif
    REAL(fvprc) :: ra_x(bd%is:bd%ie, bd%jsd:bd%jed)
    REAL(fvprc) :: ra_y(bd%isd:bd%ied, bd%js:bd%je)
    REAL(fvprc) :: xfx(bd%is:bd%ie+1, bd%jsd:bd%jed, npz)
    REAL(fvprc) :: yfx(bd%isd:bd%ied, bd%js:bd%je+1, npz)
    REAL(fvprc) :: cmax(npz)
    REAL(fvprc) :: cmax_t
    REAL(fvprc) :: c_global
    REAL(fvprc) :: frac, rdt
    REAL(fvprc), PARAMETER :: esl=1.e-24
    INTEGER :: nsplt, nsplt_parent
    INTEGER, SAVE :: msg_split_steps=1
    INTEGER :: i, j, k, it, iq, n
! Tracers
    REAL(fvprc) :: q_tj(bd%isd:bd%ied, bd%jsd:bd%jed, npz, nq)
    REAL(fvprc) :: fx_tj(bd%is:bd%ie+1, bd%js:bd%je)
    REAL(fvprc) :: fy_tj(bd%is:bd%ie, bd%js:bd%je+1)
!      real(FVPRC), pointer, dimension(:,:) :: area, rarea
!      real(FVPRC), pointer, dimension(:,:,:) :: sin_sg
!      real(FVPRC), pointer, dimension(:,:) :: dxa, dya, dx, dy
    INTEGER :: is, ie, js, je
    INTEGER :: isd, ied, jsd, jed
    REAL(fvprc) :: step, split
    INTRINSIC ABS
    INTRINSIC MAX
    INTRINSIC INT
    INTRINSIC REAL
    REAL(fvprc) :: max1
    REAL(fvprc) :: x2
    REAL(fvprc) :: x1
    REAL(fvprc) :: y2
    REAL(fvprc) :: y1
    is = bd%is
    ie = bd%ie
    js = bd%js
    je = bd%je
    isd = bd%isd
    ied = bd%ied
    jsd = bd%jsd
    jed = bd%jed
!       area => gridstruct%area
!      rarea => gridstruct%rarea
!      sin_sg => gridstruct%sin_sg
!      dxa    => gridstruct%dxa 
!      dya    => gridstruct%dya 
!      dx     => gridstruct%dx  
!      dy     => gridstruct%dy  
!$OMP parallel do default(none) shared(is,ie,js,je,isd,ied,jsd,jed,npz,cx,cy,xfx,yfx, &
!$OMP                                  dxa,dya,dx,dy,sin_sg)
    DO k=1,npz
      DO j=jsd,jed
        DO i=is,ie+1
          IF (cx(i, j, k) .GT. 0.) THEN
            xfx(i, j, k) = cx(i, j, k)*gridstruct%dxa(i-1, j)*gridstruct&
&             %dy(i, j)*gridstruct%sin_sg(i-1, j, 3)
            CALL PUSHCONTROL1B_FV(1)
          ELSE
            xfx(i, j, k) = cx(i, j, k)*gridstruct%dxa(i, j)*gridstruct%&
&             dy(i, j)*gridstruct%sin_sg(i, j, 1)
            CALL PUSHCONTROL1B_FV(0)
          END IF
        END DO
      END DO
      DO j=js,je+1
        DO i=isd,ied
          IF (cy(i, j, k) .GT. 0.) THEN
            yfx(i, j, k) = cy(i, j, k)*gridstruct%dya(i, j-1)*gridstruct&
&             %dx(i, j)*gridstruct%sin_sg(i, j-1, 4)
            CALL PUSHCONTROL1B_FV(1)
          ELSE
            yfx(i, j, k) = cy(i, j, k)*gridstruct%dya(i, j)*gridstruct%&
&             dx(i, j)*gridstruct%sin_sg(i, j, 2)
            CALL PUSHCONTROL1B_FV(0)
          END IF
        END DO
      END DO
    END DO
!--------------------------------------------------------------------------------
    IF (q_split .EQ. 0) THEN
! Determine nsplt
!$OMP parallel do default(none) shared(is,ie,js,je,npz,cmax,cx,cy,sin_sg) &
!$OMP                          private(cmax_t )
      DO k=1,npz
        cmax(k) = 0.
        IF (k .LT. 4) THEN
! Top layers: C < max( abs(c_x), abs(c_y) )
          DO j=js,je
            DO i=is,ie
              IF (cx(i, j, k) .GE. 0.) THEN
                x1 = cx(i, j, k)
              ELSE
                x1 = -cx(i, j, k)
              END IF
              IF (cy(i, j, k) .GE. 0.) THEN
                y1 = cy(i, j, k)
              ELSE
                y1 = -cy(i, j, k)
              END IF
              IF (x1 .LT. y1) THEN
                cmax_t = y1
              ELSE
                cmax_t = x1
              END IF
              IF (cmax_t .LT. cmax(k)) THEN
                CALL PUSHCONTROL1B_FV(0)
                cmax(k) = cmax(k)
              ELSE
                CALL PUSHCONTROL1B_FV(1)
                cmax(k) = cmax_t
              END IF
            END DO
          END DO
          CALL PUSHCONTROL1B_FV(1)
        ELSE
          DO j=js,je
            DO i=is,ie
              IF (cx(i, j, k) .GE. 0.) THEN
                x2 = cx(i, j, k)
              ELSE
                x2 = -cx(i, j, k)
              END IF
              IF (cy(i, j, k) .GE. 0.) THEN
                y2 = cy(i, j, k)
              ELSE
                y2 = -cy(i, j, k)
              END IF
              IF (x2 .LT. y2) THEN
                max1 = y2
              ELSE
                max1 = x2
              END IF
              cmax_t = max1 + 1. - gridstruct%sin_sg(i, j, 5)
              IF (cmax_t .LT. cmax(k)) THEN
                CALL PUSHCONTROL1B_FV(0)
                cmax(k) = cmax(k)
              ELSE
                CALL PUSHCONTROL1B_FV(1)
                cmax(k) = cmax_t
              END IF
            END DO
          END DO
          CALL PUSHCONTROL1B_FV(0)
        END IF
      END DO
      CALL MP_REDUCE_MAX(cmax, npz)
! find global max courant number and define nsplt to scale cx,cy,mfx,mfy
      c_global = cmax(1)
      IF (npz .NE. 1) THEN
! if NOT shallow water test case
        DO k=2,npz
          IF (cmax(k) .LT. c_global) THEN
            CALL PUSHCONTROL1B_FV(0)
            c_global = c_global
          ELSE
            CALL PUSHCONTROL1B_FV(1)
            c_global = cmax(k)
          END IF
        END DO
        CALL PUSHCONTROL2B_FV(0)
      ELSE
        CALL PUSHCONTROL2B_FV(1)
      END IF
      nsplt = INT(1. + c_global)
!#ifdef FLUXBCS
!      !!*****NOTE*****
!      !! If setting the FLUXBCS directive do note that the current
!      !!  version of the code requires that Atm, tile, and pelist be
!      !!  brought in through use statements. This code will need re
!      !! -writing to avoid this.
!
!      !If using flux BCs, nested grid nsplit must be an
!      !even multiple of that on the coarse grid
!      if (neststruct%do_flux_BCs .or. (gridstruct%nested .and. neststruct%nestbctype > 1)) then
!         !Receive from all parent grids
!         if (gridstruct%nested) then
!            !!NOTE about mpp_recv/mpp_send and scalars:
!            !! When passing a scalar, the second argument is not SIZE (which is known to be 1) but a process ID
!            call mpp_recv(nsplt_parent,parent_grid%pelist(1))
!            nsplt = ceiling(real(nsplt)/real(nsplt_parent))*nsplt
!            nsplt = max(nsplt,nsplt_parent)
!            msg_split_steps = nsplt/nsplt_parent
!         endif
!      endif
!#endif
!if ( master )  write(*,*) 'Tracer_2d_split=', nsplt, c_global 
!if ( is_master() .and. nsplt > 3 )  write(*,*) 'Tracer_2d_split=', nsplt, c_global 
    ELSE
      nsplt = q_split
      CALL PUSHCONTROL2B_FV(2)
    END IF
!#ifdef FLUXBCS
!   !Make sure to send to any nested grids which might be expecting a coarse-grid nsplit.
!   !(This is outside the if statement since it could be that the coarse grid uses
!   !q_split > 0 but the nested grid has q_split = 0)
!   if (neststruct%do_flux_BCs .or. (gridstruct%nested .and. neststruct%nestbctype > 1)) then
!      if (ANY(neststruct%child_grids) .and. is_master()) then
!         do n=1,size(neststruct%child_grids)
!            if (neststruct%child_grids(n) .and. Atm(n)%flagstruct%q_split == 0) then
!               do i=1,Atm(n)%npes_this_grid
!                  call mpp_send(nsplt,Atm(n)%pelist(i))
!               enddo
!            endif
!         enddo
!      endif
!   endif
!#endif FLUXBCS
!--------------------------------------------------------------------------------
    frac = 1./REAL(nsplt)
    IF (nsplt .NE. 1) THEN
!$OMP parallel do default(none) shared(is,ie,js,je,isd,ied,jsd,jed,npz,cx,cy,xfx,yfx, &
!$OMP                                  mfx,mfy,frac)
      DO k=1,npz
        DO j=jsd,jed
          DO i=is,ie+1
            CALL PUSHREALARRAY(cx(i, j, k))
            cx(i, j, k) = cx(i, j, k)*frac
            xfx(i, j, k) = xfx(i, j, k)*frac
          END DO
        END DO
        DO j=js,je
          DO i=is,ie+1
            CALL PUSHREALARRAY(mfx(i, j, k))
            mfx(i, j, k) = mfx(i, j, k)*frac
          END DO
        END DO
        DO j=js,je+1
          DO i=isd,ied
            CALL PUSHREALARRAY(cy(i, j, k))
            cy(i, j, k) = cy(i, j, k)*frac
            yfx(i, j, k) = yfx(i, j, k)*frac
          END DO
        END DO
        DO j=js,je+1
          DO i=is,ie
            CALL PUSHREALARRAY(mfy(i, j, k))
            mfy(i, j, k) = mfy(i, j, k)*frac
          END DO
        END DO
      END DO
      CALL PUSHCONTROL1B_FV(1)
    ELSE
      CALL PUSHCONTROL1B_FV(0)
    END IF
!$OMP parallel do default(none) shared(is,ie,js,je,npz,dp2,dp1,mfx,mfy,rarea)
    DO k=1,npz
      DO j=js,je
        DO i=is,ie
          dp2(i, j, k) = dp1(i, j, k) + (mfx(i, j, k)-mfx(i+1, j, k)+mfy&
&           (i, j, k)-mfy(i, j+1, k))*gridstruct%rarea(i, j)
        END DO
      END DO
    END DO
    DO it=1,nsplt
!call timing_off('COMM_TRACER')
!call timing_off('COMM_TOTAL')
!      if (gridstruct%nested) then
!            step = real(neststruct%tracer_nest_timestep)+real(nsplt*k_split)
!            split = real(nsplt*k_split)
!            do iq=1,nq
!                 call nested_grid_BC_apply_intT(q(isd:ied,jsd:jed,:,iq), &
!                      !0, 0, npx, npy, npz, real(tracer_nest_timestep), real(nsplt), &
!                      0, 0, npx, npy, npz, bd, &
!                      step, split, &
!                      var_east_t0=neststruct%q_BC(iq)%east_t0, &
!                      var_west_t0=neststruct%q_BC(iq)%west_t0, &
!                      var_north_t0=neststruct%q_BC(iq)%north_t0, &
!                      var_south_t0=neststruct%q_BC(iq)%south_t0, &
!                      var_east_t1=neststruct%q_BC(iq)%east_t1, &
!                      var_west_t1=neststruct%q_BC(iq)%west_t1, &
!                      var_north_t1=neststruct%q_BC(iq)%north_t1, &
!                      var_south_t1=neststruct%q_BC(iq)%south_t1, &
!                      bctype=neststruct%nestbctype, &
!                      nsponge=neststruct%nsponge, s_weight=neststruct%s_weight   )
!           enddo
!      endif
!#ifdef FLUXBCS
!
!!$OMP parallel do default(none) shared(is,ie,js,je,isd,ied,jsd,jed,npz,nq,area,xfx,yfx,q, &
!!$OMP                                  cx,cy,npx,npy,hord,fx,fy,gridstruct,bd,mfx,mfy) &
!!$OMP                                  private(ra_x, ra_y)
!      do k=1,npz
!
!         do j=jsd,jed
!            do i=is,ie
!               ra_x(i,j) = gridstruct%area(i,j) + xfx(i,j,k) - xfx(i+1,j,k)
!            enddo
!         enddo
!         do j=js,je
!            do i=isd,ied
!               ra_y(i,j) = gridstruct%area(i,j) + yfx(i,j,k) - yfx(i,j+1,k)
!            enddo
!         enddo
!
!         do iq=1,nq
!            call fv_tp_2d(q(isd,jsd,k,iq), cx(is,jsd,k), cy(isd,js,k), &
!                          npx, npy, hord, fx(is,js,k,iq), fy(is,js,k,iq), xfx(is,jsd,k), yfx(isd,js,k), &
!                          gridstruct, bd, ra_x, ra_y, mfx=mfx(is,js,k), mfy=mfy(is,js,k))
!         enddo
!      enddo
!
!      if (neststruct%do_flux_BCs .or. (gridstruct%nested .and. neststruct%nestbctype > 1) ) then
!
!         !call FCT_PD(q,fx,fy,dp1,npx,npy,npz,nq,gridstruct%area, domain)
!
!         call flux_BCs(fx, fy, it, msg_split_steps, npx, npy, npz, nq, q, dp1, dp2, cx, cy, gridstruct%nested, neststruct, paren
!t_grid)
!         !call flux_BCs(fx, fy, it, nsplt, npx, npy, npz, nq, q, dp1, dp2, cx, cy)
!
!      endif
!!$OMP parallel do default(none) shared(is,ie,js,je,npz,nq,q,dp1,fx,fy,rarea,dp2)
!      do k=1,npz
!         do iq=1,nq
!
!            do j=js,je
!               do i=is,ie
!                  q(i,j,k,iq) = ( q(i,j,k,iq)*dp1(i,j,k) + &
!                                (fx(i,j,k,iq)-fx(i+1,j,k,iq)+fy(i,j,k,iq)-fy(i,j+1,k,iq))*gridstruct%rarea(i,j) )/dp2(i,j,k)
!               enddo
!            enddo
!
!         enddo
!      enddo ! npz
!
!#else
!$OMP parallel do default(none) shared(is,ie,js,je,isd,ied,jsd,jed,npz,gridstruct%area,xfx,  &
!$OMP                                  yfx,q,iq,cx,cy,npx,npy,hord,nq,gridstruct, &
!$OMP                                  mfx,mfy,dp1,dp2,rarea,bd)                  &
!$OMP                            private(ra_x, ra_y, fx, fy)
      DO k=1,npz
        DO j=jsd,jed
          DO i=is,ie
            CALL PUSHREALARRAY(ra_x(i, j))
            ra_x(i, j) = gridstruct%area(i, j) + xfx(i, j, k) - xfx(i+1&
&             , j, k)
          END DO
        END DO
        DO j=js,je
          DO i=isd,ied
            CALL PUSHREALARRAY(ra_y(i, j))
            ra_y(i, j) = gridstruct%area(i, j) + yfx(i, j, k) - yfx(i, j&
&             +1, k)
          END DO
        END DO
        DO iq=1,nq
          IF (hord .EQ. hord_pert) THEN
            CALL FV_TP_2D_FWD(q(isd:ied, jsd:jed, k, iq), cx(is:ie+1&
&                          , jsd:jed, k), cy(isd:ied, js:je+1, k), npx, &
&                          npy, hord, fx, fy, xfx(is:ie+1, jsd:jed, k), &
&                          yfx(isd:ied, js:je+1, k), gridstruct, bd, &
&                          ra_x, ra_y, mfx=mfx(is:ie+1, js:je, k), mfy=&
&                          mfy(is:ie, js:je+1, k))
            CALL PUSHCONTROL1B_FV(1)
          ELSE
            CALL PUSHREALARRAY(fy, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+2&
&                         )/8)
            CALL PUSHREALARRAY(fx, 8*(bd%ie-bd%is+2)*(bd%je-bd%js+1&
&                         )/8)
            CALL PUSHREALARRAY(q(isd:ied, jsd:jed, k, iq), 8*(ied-&
&                         isd+1)*(jed-jsd+1)/8)
            CALL FV_TP_2D(q(isd:ied, jsd:jed, k, iq), cx(is:ie+1, jsd:&
&                   jed, k), cy(isd:ied, js:je+1, k), npx, npy, &
&                   hord_pert, fx, fy, xfx(is:ie+1, jsd:jed, k), yfx(isd&
&                   :ied, js:je+1, k), gridstruct, bd, ra_x, ra_y, mfx=&
&                   mfx(is:ie+1, js:je, k), mfy=mfy(is:ie, js:je+1, k))
            CALL PUSHCONTROL1B_FV(0)
          END IF
          DO j=js,je
            DO i=is,ie
              CALL PUSHREALARRAY(q(i, j, k, iq))
              q(i, j, k, iq) = (q(i, j, k, iq)*dp1(i, j, k)+(fx(i, j)-fx&
&               (i+1, j)+fy(i, j)-fy(i, j+1))*gridstruct%rarea(i, j))/&
&               dp2(i, j, k)
            END DO
          END DO
        END DO
      END DO
! npz
!#endif
      IF (it .NE. nsplt) THEN
!call timing_on('COMM_TOTAL')
!call timing_on('COMM_TRACER')
        CALL PUSHREALARRAY(q, 8*(bd%ied-bd%isd+1)*(bd%jed-bd%jsd+1)&
&                     *npz*nq/8)
        CALL START_GROUP_HALO_UPDATE(q_pack, q_packp, q, domain)
!call timing_off('COMM_TRACER')
!call timing_off('COMM_TOTAL')
!$OMP parallel do default(none) shared(is,ie,js,je,npz,dp1,dp2,mfx,mfy,rarea)
        DO k=1,npz
          DO j=js,je
            DO i=is,ie
              CALL PUSHREALARRAY(dp1(i, j, k))
              dp1(i, j, k) = dp2(i, j, k)
              CALL PUSHREALARRAY(dp2(i, j, k))
              dp2(i, j, k) = dp1(i, j, k) + (mfx(i, j, k)-mfx(i+1, j, k)&
&               +mfy(i, j, k)-mfy(i, j+1, k))*gridstruct%rarea(i, j)
            END DO
          END DO
        END DO
        CALL PUSHCONTROL1B_FV(1)
      ELSE
        CALL PUSHCONTROL1B_FV(0)
      END IF
    END DO
!Apply nested-grid BCs
!           if ( gridstruct%nested ) then
!              step  = real(neststruct%tracer_nest_timestep)
!              split = real(nsplt*k_split)
!              do iq=1,nq
!
!
!                 call nested_grid_BC_apply_intT(q(isd:ied,jsd:jed,:,iq), &
!                      0, 0, npx, npy, npz, bd, &
!                      step, split, &
!                      !0, 0, npx, npy, npz, real(tracer_nest_timestep)+real(nsplt), real(nsplt), &
!                      var_east_t0=neststruct%q_BC(iq)%east_t0, &
!                      var_west_t0=neststruct%q_BC(iq)%west_t0, &
!                      var_north_t0=neststruct%q_BC(iq)%north_t0, &
!                      var_south_t0=neststruct%q_BC(iq)%south_t0, &
!                      var_east_t1=neststruct%q_BC(iq)%east_t1, &
!                      var_west_t1=neststruct%q_BC(iq)%west_t1, &
!                      var_north_t1=neststruct%q_BC(iq)%north_t1, &
!                      var_south_t1=neststruct%q_BC(iq)%south_t1, &
!                      bctype=neststruct%nestbctype, &
!                      nsponge=neststruct%nsponge, s_weight=neststruct%s_weight   )
!
!              end do
!           end if
! nsplt
    IF (id_divg .GT. 0) THEN
      rdt = 1./(frac*dt)
!$OMP parallel do default(none) shared(is,ie,js,je,npz,dp1,xfx,yfx,rarea,rdt)
      DO k=1,npz
        DO j=js,je
          DO i=is,ie
            CALL PUSHREALARRAY(dp1(i, j, k))
            dp1(i, j, k) = (xfx(i+1, j, k)-xfx(i, j, k)+yfx(i, j+1, k)-&
&             yfx(i, j, k))*gridstruct%rarea(i, j)*rdt
          END DO
        END DO
      END DO
      CALL PUSHINTEGER(js)
      CALL PUSHREALARRAY(xfx, 8*(bd%ie-bd%is+2)*(bd%jed-bd%jsd+1)*&
&                   npz/8)
      CALL PUSHREALARRAY(dp2, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+1)*npz&
&                   /8)
      CALL PUSHREALARRAY(frac)
      CALL PUSHINTEGER(ie)
      CALL PUSHREALARRAY(ra_x, 8*(bd%ie-bd%is+1)*(bd%jed-bd%jsd+1)/&
&                   8)
      CALL PUSHREALARRAY(ra_y, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+1)/&
&                   8)
      CALL PUSHREALARRAY(yfx, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+2)*&
&                   npz/8)
      CALL PUSHINTEGER(is)
      CALL PUSHREALARRAY(rdt)
      CALL PUSHINTEGER(nsplt)
      CALL PUSHREALARRAY(fx, 8*(bd%ie-bd%is+2)*(bd%je-bd%js+1)/8)
      CALL PUSHREALARRAY(fy, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+2)/8)
      CALL PUSHINTEGER(je)
      CALL PUSHCONTROL1B_FV(1)
    ELSE
      CALL PUSHINTEGER(js)
      CALL PUSHREALARRAY(xfx, 8*(bd%ie-bd%is+2)*(bd%jed-bd%jsd+1)*&
&                   npz/8)
      CALL PUSHREALARRAY(dp2, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+1)*npz&
&                   /8)
      CALL PUSHREALARRAY(frac)
      CALL PUSHINTEGER(ie)
      CALL PUSHREALARRAY(ra_x, 8*(bd%ie-bd%is+1)*(bd%jed-bd%jsd+1)/&
&                   8)
      CALL PUSHREALARRAY(ra_y, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+1)/&
&                   8)
      CALL PUSHREALARRAY(yfx, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+2)*&
&                   npz/8)
      CALL PUSHINTEGER(is)
      CALL PUSHINTEGER(nsplt)
      CALL PUSHREALARRAY(fx, 8*(bd%ie-bd%is+2)*(bd%je-bd%js+1)/8)
      CALL PUSHREALARRAY(fy, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+2)/8)
      CALL PUSHINTEGER(je)
      CALL PUSHCONTROL1B_FV(0)
    END IF
  END SUBROUTINE TRACER_2D_NESTED_FWD
!  Differentiation of tracer_2d_nested in reverse (adjoint) mode, backward sweep (with options r8 split(a2b_edge_mod.a2b_ord2 a2b
!_edge_mod.a2b_ord4 a2b_edge_mod.extrap_corner  dyn_core_mod.dyn_core dyn_core_mod.pk3_halo dyn_core_mod.pln_halo dyn_core_
!mod.pe_halo dyn_core_mod.adv_pe dyn_core_mod.p_grad_c dyn_core_mod.nh_p_grad dyn_core_mod.split_p_grad dyn_core_mod.one_grad_p d
!yn_core_mod.grad1_p_update dyn_core_mod.mix_dp dyn_core_mod.geopk dyn_core_mod.del2_cubed fv_dynamics_mod.fv_dynamics fv_dynamic
!s_mod.rayleigh_super fv_dynamics_mod.rayleigh_friction fv_dynamics_mod.compute_aam fv_dynamics_mod.geos_to_fv3 fv_grid_utils_mod
!.cubed_to_latlon fv_grid_utils_mod.c2l_ord4 fv_grid_utils_mod.c2l_ord2 fv_mapz_mod.lagrangian_to_eulerian fv_mapz_mod.compute_to
!tal_energy fv_mapz_mod.pkez fv_mapz_mod.remap_z fv_mapz_mod.map_scalar fv_mapz_mod.map1_ppm fv_mapz_mod.mapn_tracer 
!fv_mapz_mod.map1_q2 fv_mapz_mod.map1_cubic fv_mapz_mod.scalar_profile_linear fv_mapz_mod.cs_profile_linear fv_mapz_mod.
!steepz fv_tracer2d_mod.tracer_2d fv_tracer2d_mod.tracer_2d_nested nh_core_mod.update_dz_c nh_core_mod.update_dz_d nh_core_mod.ri
!em_solver_c nh_core_mod.riem_solver3 nh_core_mod.rim_2d nh_core_mod.sim3_solver nh_core_mod.sim3p0_solver nh_core_mod.sim1_solve
!r nh_core_mod.sim_solver nh_core_mod.edge_profile sw_core_mod.c_sw sw_core_mod.d_sw sw_core_mod.divergence_corner sw_core_mod.di
!vergence_corner_nest sw_core_mod.d2a2c_vect sw_core_mod.edge_interpolate4 sw_core_mod.fill2_4corners sw_core_mod.fill_4corners s
!w_core_mod.compute_div_damping sw_core_mod.smag_corner sw_core_mod.ytp_v sw_core_mod.xtp_u tp_core_mod.fv_tp_2d t
!p_core_mod.copy_corners tp_core_mod.xtp tp_core_mod.ytp tp_core_mod.xppm0 tp_core_mod.yppm0 tp_core_mod.fxppm 
!tp_core_mod.fyppm tp_core_mod.deln_flux)):
!   gradient     of useful results: q dp1 mfx mfy cx cy
!   with respect to varying inputs: q dp1 mfx mfy cx cy
!subroutine offline_tracer_advection(q, ple0, ple1, mfx, mfy, cx, cy, &
!                                    gridstruct, neststruct, bd, domain, &
!                                    ak, bk, ptop, npx, npy, npz,   &
!                                    nq, hord, kord, q_split, k_split, dt, z_tracer, fill)
!
!      use fv_mapz_mod,        only: map1_q2
!      use fv_fill_mod,        only: fillz
!
!      integer, intent(IN) :: npx
!      integer, intent(IN) :: npy
!      integer, intent(IN) :: npz
!      integer, intent(IN) :: nq    ! number of tracers to be advected
!      integer, intent(IN) :: hord
!      integer, intent(IN) :: kord
!      integer, intent(IN) :: q_split
!      integer, intent(IN) :: k_split
!      logical, intent(IN) :: z_tracer
!      logical, intent(IN) :: fill
!      type(fv_grid_bounds_type), intent(IN   ) :: bd
!      type(fv_nest_type), intent(INOUT) :: neststruct
!      type(fv_grid_type), intent(IN), target :: gridstruct
!      type(domain2D), intent(INOUT) :: domain
!
!      real(FVPRC), intent(IN   ) :: dt
!      real(FVPRC), intent(IN   ) ::ple0(bd%is:bd%ie,bd%js:bd%je,npz+1)      ! DELP before dyn_core
!      real(FVPRC), intent(INOUT) ::ple1(bd%is:bd%ie,bd%js:bd%je,npz+1)      ! DELP after dyn_core
!      real(FVPRC), intent(IN   ) ::  cx(bd%is:bd%ie,bd%js:bd%je,npz)        ! Courant Number X-Dir
!      real(FVPRC), intent(IN   ) ::  cy(bd%is:bd%ie,bd%js:bd%je,npz)        ! Courant Number Y-Dir
!      real(FVPRC), intent(IN   ) :: mfx(bd%is:bd%ie,bd%js:bd%je,npz)        ! Mass Flux X-Dir
!      real(FVPRC), intent(IN   ) :: mfy(bd%is:bd%ie,bd%js:bd%je,npz)        ! Mass Flux Y-Dir
!      real(FVPRC), intent(INOUT) ::   q(bd%is:bd%ie,bd%js:bd%je,npz,nq)     ! Tracers
!      real(FVPRC), intent(IN   ) ::  ak(npz+1)                  ! AK for remapping
!      real(FVPRC), intent(IN   ) ::  bk(npz+1)                  ! BK for remapping
!      real(FVPRC), intent(IN   ) :: ptop
!! Local Arrays
!      real(FVPRC) ::   xL(bd%isd:bd%ied+1,bd%jsd:bd%jed  ,npz)  ! X-Dir for MPP Updates
!      real(FVPRC) ::   yL(bd%isd:bd%ied  ,bd%jsd:bd%jed+1,npz)  ! Y-Dir for MPP Updates
!      real(FVPRC) ::  cxL(bd%is :bd%ie +1,bd%jsd:bd%jed  ,npz)  ! Courant Number X-Dir
!      real(FVPRC) ::  cyL(bd%isd:bd%ied  ,bd%js :bd%je +1,npz)  ! Courant Number Y-Dir
!      real(FVPRC) :: mfxL(bd%is :bd%ie +1,bd%js :bd%je   ,npz)  ! Mass Flux X-Dir
!      real(FVPRC) :: mfyL(bd%is :bd%ie   ,bd%js :bd%je +1,npz)  ! Mass Flux Y-Dir
!      real(FVPRC) ::  dpL(bd%is :bd%ie   ,bd%js :bd%je   ,npz)  ! Pressure Thickness
!      real(FVPRC) ::  dpA(bd%is :bd%ie   ,bd%js :bd%je   ,npz)  ! Pressure Thickness
!! Local Tracer Arrays
!      real(FVPRC) ::   q1(bd%is:bd%ie  ,bd%js:bd%je, npz   )! 2D Tracers
!      real(FVPRC) ::   q2(bd%isd:bd%ied  ,bd%jsd:bd%jed     ,nq)! 2D Tracers
!      real(FVPRC) ::   q3(bd%isd:bd%ied  ,bd%jsd:bd%jed, npz,nq)! 3D Tracers
!! Local Buffer Arrarys
!      real(FVPRC) :: wbuffer(bd%js:bd%je,npz)
!      real(FVPRC) :: sbuffer(bd%is:bd%ie,npz)
!      real(FVPRC) :: ebuffer(bd%js:bd%je,npz)
!      real(FVPRC) :: nbuffer(bd%is:bd%ie,npz)
!! Local Remap Arrays
!      real(FVPRC)  pe1(bd%is:bd%ie,npz+1)
!      real(FVPRC)  pe2(bd%is:bd%ie,npz+1)
!      real(FVPRC)  dp2(bd%is:bd%ie,bd%js:bd%je,npz)
!
!! Local indices
!      integer     :: i,j,k,n,iq
!      real(FVPRC) :: dtR8
!
!      real(FVPRC) :: scalingFactor
!
!      type(group_halo_update_type), save :: i_pack
!
!      integer :: is,  ie,  js,  je
!      integer :: isd, ied, jsd, jed
!
!      is  = bd%is
!      ie  = bd%ie
!      js  = bd%js
!      je  = bd%je
!      isd = bd%isd
!      ied = bd%ied
!      jsd = bd%jsd
!      jed = bd%jed
!
!! Time-step
!    dtR8=dt
!! Fill CX/CY C-Grid boundaries and update ghost regions
!    xL(is:ie,js:je,:) = cx(:,:,:)
!    yL(is:ie,js:je,:) = cy(:,:,:)
!    call mpp_get_boundary(xL, yL, domain, &
!                          wbufferx=wbuffer, ebufferx=ebuffer, &
!                          sbuffery=sbuffer, nbuffery=nbuffer, &
!                          gridtype=CGRID_NE )
!    xL(ie+1,js:je,:) = ebuffer
!    yL(is:ie,je+1,:) = nbuffer
!    call mpp_update_domains( xL, yL, domain, gridtype=CGRID_NE, complete=.true.)
!    cxL(is:ie+1,jsd:jed,:) = xL(is:ie+1,jsd:jed,:)
!    cyL(isd:ied,js:je+1,:) = yL(isd:ied,js:je+1,:)
!
!! Fill MFX/MFY C-Grid boundaries
!    xL(is:ie,js:je,:) = mfx(:,:,:)
!    yL(is:ie,js:je,:) = mfy(:,:,:)
!    call mpp_get_boundary(xL, yL, domain, &
!                          wbufferx=wbuffer, ebufferx=ebuffer, &
!                          sbuffery=sbuffer, nbuffery=nbuffer, &
!                          gridtype=CGRID_NE )
!    xL(ie+1,js:je,:) = ebuffer
!    yL(is:ie,je+1,:) = nbuffer
!    mfxL(is:ie+1,js:je,:) = xL(is:ie+1,js:je,:)
!    mfyL(is:ie,js:je+1,:) = yL(is:ie,js:je+1,:)
!
!! Fill local tracers and pressure thickness
!    dpL(:,:,:) = ple0(:,:,2:npz+1) - ple0(:,:,1:npz)
!    q3(is:ie,js:je,:,:) = q(is:ie,js:je,:,:)
!
!    call start_group_halo_update(i_pack, q3, domain)
!
!    if ( z_tracer ) then
!!$omp parallel do default(shared) private(q2)
!       do k=1,npz
!         do iq=1,nq
!            do j=js,je
!               do i=is,ie                   ! To_do list:
!                  q2(i,j,iq) = q3(i,j,k,iq) ! The data copying can be avoided if q is
!                                            ! re-dimensioned as q(i,j,nq,k)
!               enddo
!            enddo
!         enddo
!         call tracer_2d_1L(q2, dpL(is,js,k), mfxL(is,js,k), mfyL(is,js,k), &
!                           cxL(is,jsd,k),  cyL(isd,js,k), gridstruct, neststruct, bd, domain, npx, npy, npz,   &
!                           1, nq, hord, q_split, k, q3, dtR8, 0, k_split, dpA=dpA)
!       enddo
!    else
!         call tracer_2d(q3, dpL, mfxL, mfyL, cxL, cyL, gridstruct, bd, domain, npx, npy, npz, 1, nq, &
!                        hord, q_split, dtR8, 0, i_pack, .false., k_split, dpA=dpA)
!    endif
!
!!------------------------------------------------------------------
!! Re-Map constituents
!! Do remapping one tracer at a time; seems to be faster
!! It requires less memory than mapn_ppm
!!------------------------------------------------------------------
!
!       do iq=1,nq
!          do j=js,je
!           ! pressures mapping from (dpA is new delp after tracer_2d)
!             pe1(:,1) = ptop
!             do k=2,npz+1
!               pe1(:,k) = pe1(:,k-1) + dpA(:,j,k-1)
!             enddo
!           ! pressures mapping to
!             pe2(:,1) = ptop
!             pe2(:,npz+1) = pe1(:,npz+1)
!             do k=2,npz
!                 pe2(:  ,k) = ak(k) + bk(k)*pe1(:,npz+1)
!             enddo
!             do k=1,npz
!                dp2(:,j,k) = pe2(:,k+1) - pe2(:,k)
!             enddo
!             call map1_q2(npz, pe1, q3(isd,jsd,1,iq),      &
!                          npz, pe2, q1(:,j,:), dp2(:,j,:), &
!                          is, ie, 0, kord, j,              &
!                          isd, ied, jsd, jed, 0._FVPRC) 
!             if (fill) call fillz(ie-is+1, npz, 1, q1(:,j,:), dp2(:,j,:))
!          enddo
!          ! Rescale tracers based on ple1 at destination timestep
!          !------------------------------------------------------
!
!          scalingFactor = calcScalingFactor(q1, dp2, ple1, npx, npy, npz, gridstruct, bd)
!          !scalingFactors = computeScalingFactors(q1, dp2, ple1, npx, npy, npz)
!
!          ! Return tracers
!          !---------------
!          q(is:ie,js:je,1:npz,iq) = q1(is:ie,js:je,1:npz) * scalingFactor
!          !do k =1,npz
!             !do j = js,je
!                !do i = is,ie
!                   !q(i,j,k,iq) = q1(i,j,k)
!                !enddo
!             !enddo
!          !enddo
!
!       enddo
!
!end subroutine offline_tracer_advection
!------------------------------------------------------------------------------------
!         function calcScalingFactor(q1, dp2, ple1, npx, npy, npz, gridstruct, bd) result(scaling)
!         use mpp_mod, only: mpp_sum
!         integer, intent(in) :: npx
!         integer, intent(in) :: npy
!         integer, intent(in) :: npz
!         real(FVPRC), intent(in) :: q1(:,:,:)
!         real(FVPRC), intent(in) :: dp2(:,:,:)
!         real(FVPRC), intent(in) :: ple1(:,:,:)
!         type(fv_grid_type), intent(IN   ) :: gridstruct
!         type(fv_grid_bounds_type), intent(IN   ) :: bd
!         real(FVPRC) :: scaling
!
!         integer :: k
!         real(FVPRC) :: partialSums(2,npz), globalSums(2)
!         real(FVPRC), parameter :: TINY_DENOMINATOR = tiny(1.0)
!
!         !-------
!         ! Compute partial sum on local array first to minimize communication.
!         ! This algorithm will not be strongly repdroducible under changes do domain
!         ! decomposition, but uses far less communication bandwidth (and memory BW)
!         ! then the preceding implementation.
!         !-------
!         do k = 1, npz
!            ! numerator
!            partialSums(1,k) = sum(q1(:,:,k)*dp2(:,:,k)*gridstruct%area(bd%is:bd%ie,bd%js:bd%je))
!            ! denominator
!            partialSums(2,k) = sum(q1(:,:,k)*(ple1(:,:,k+1)-ple1(:,:,k))*gridstruct%area(bd%is:bd%ie,bd%js:bd%je))
!         end do
!
!         globalSums(1) = sum(partialSums(1,:))
!         globalSums(2) = sum(partialSums(2,:))
!
!         call mpp_sum(globalSums, 2)
!
!         if (globalSums(2) > TINY_DENOMINATOR) then
!            scaling =  globalSums(1) / globalSums(2)
!            !#################################################################
!            ! This line was added to ensure strong reproducibility of the code
!            !#################################################################
!            scaling = REAL(scaling, KIND=REAL4)
!         else
!            scaling = 1.d0
!         end if
!
!         end function calcScalingFactor
  SUBROUTINE TRACER_2D_NESTED_BWD(q, q_ad, dp1, dp1_ad, mfx, mfx_ad, mfy&
&   , mfy_ad, cx, cx_ad, cy, cy_ad, gridstruct, bd, domain, npx, npy, &
&   npz, nq, hord, hord_pert, q_split, dt, id_divg, q_pack, q_packp, &
&   z_tracer, k_split, neststruct, parent_grid)
    IMPLICIT NONE
    TYPE(FV_GRID_BOUNDS_TYPE), INTENT(IN) :: bd
    INTEGER, INTENT(IN) :: npx
    INTEGER, INTENT(IN) :: npy
    INTEGER, INTENT(IN) :: npz
    INTEGER, INTENT(IN) :: nq
    INTEGER, INTENT(IN) :: hord, hord_pert
    INTEGER, INTENT(IN) :: q_split, k_split
    INTEGER, INTENT(IN) :: id_divg
    REAL(fvprc), INTENT(IN) :: dt
    LOGICAL, INTENT(IN) :: z_tracer
    TYPE(GROUP_HALO_UPDATE_TYPE), INTENT(INOUT) :: q_pack, q_packp
    REAL(fvprc), INTENT(INOUT) :: q(bd%isd:bd%ied, bd%jsd:bd%jed, npz, &
&   nq)
    REAL(fvprc), INTENT(INOUT) :: q_ad(bd%isd:bd%ied, bd%jsd:bd%jed, npz&
&   , nq)
    REAL(fvprc), INTENT(INOUT) :: dp1(bd%is:bd%ie, bd%js:bd%je, npz)
    REAL(fvprc), INTENT(INOUT) :: dp1_ad(bd%is:bd%ie, bd%js:bd%je, npz)
    REAL(fvprc), INTENT(INOUT) :: mfx(bd%is:bd%ie+1, bd%js:bd%je, npz)
    REAL(fvprc), INTENT(INOUT) :: mfx_ad(bd%is:bd%ie+1, bd%js:bd%je, npz&
&   )
    REAL(fvprc), INTENT(INOUT) :: mfy(bd%is:bd%ie, bd%js:bd%je+1, npz)
    REAL(fvprc), INTENT(INOUT) :: mfy_ad(bd%is:bd%ie, bd%js:bd%je+1, npz&
&   )
    REAL(fvprc), INTENT(INOUT) :: cx(bd%is:bd%ie+1, bd%jsd:bd%jed, npz)
    REAL(fvprc), INTENT(INOUT) :: cx_ad(bd%is:bd%ie+1, bd%jsd:bd%jed, &
&   npz)
    REAL(fvprc), INTENT(INOUT) :: cy(bd%isd:bd%ied, bd%js:bd%je+1, npz)
    REAL(fvprc), INTENT(INOUT) :: cy_ad(bd%isd:bd%ied, bd%js:bd%je+1, &
&   npz)
    TYPE(FV_GRID_TYPE), INTENT(IN), TARGET :: gridstruct
    TYPE(FV_NEST_TYPE), INTENT(INOUT) :: neststruct
    TYPE(FV_ATMOS_TYPE), INTENT(INOUT) :: parent_grid
    TYPE(DOMAIN2D), INTENT(INOUT) :: domain
    REAL(fvprc) :: dp2(bd%is:bd%ie, bd%js:bd%je, npz)
    REAL(fvprc) :: dp2_ad(bd%is:bd%ie, bd%js:bd%je, npz)
    REAL(fvprc) :: fx(bd%is:bd%ie+1, bd%js:bd%je)
    REAL(fvprc) :: fx_ad(bd%is:bd%ie+1, bd%js:bd%je)
    REAL(fvprc) :: fy(bd%is:bd%ie, bd%js:bd%je+1)
    REAL(fvprc) :: fy_ad(bd%is:bd%ie, bd%js:bd%je+1)
    REAL(fvprc) :: ra_x(bd%is:bd%ie, bd%jsd:bd%jed)
    REAL(fvprc) :: ra_x_ad(bd%is:bd%ie, bd%jsd:bd%jed)
    REAL(fvprc) :: ra_y(bd%isd:bd%ied, bd%js:bd%je)
    REAL(fvprc) :: ra_y_ad(bd%isd:bd%ied, bd%js:bd%je)
    REAL(fvprc) :: xfx(bd%is:bd%ie+1, bd%jsd:bd%jed, npz)
    REAL(fvprc) :: xfx_ad(bd%is:bd%ie+1, bd%jsd:bd%jed, npz)
    REAL(fvprc) :: yfx(bd%isd:bd%ied, bd%js:bd%je+1, npz)
    REAL(fvprc) :: yfx_ad(bd%isd:bd%ied, bd%js:bd%je+1, npz)
    REAL(fvprc) :: cmax(npz)
    REAL(fvprc) :: cmax_t
    REAL(fvprc) :: c_global
    REAL(fvprc) :: frac, rdt
    REAL(fvprc), PARAMETER :: esl=1.e-24
    INTEGER :: nsplt, nsplt_parent
    INTEGER, SAVE :: msg_split_steps=1
    INTEGER :: i, j, k, it, iq, n
    REAL(fvprc) :: q_tj(bd%isd:bd%ied, bd%jsd:bd%jed, npz, nq)
    REAL(fvprc) :: fx_tj(bd%is:bd%ie+1, bd%js:bd%je)
    REAL(fvprc) :: fy_tj(bd%is:bd%ie, bd%js:bd%je+1)
    INTEGER :: is, ie, js, je
    INTEGER :: isd, ied, jsd, jed
    REAL(fvprc) :: step, split
    INTRINSIC ABS
    INTRINSIC MAX
    INTRINSIC INT
    INTRINSIC REAL
    REAL(fvprc) :: max1
    REAL(fvprc) :: temp_ad
    REAL(fvprc) :: temp
    REAL(fvprc) :: temp_ad0
    REAL(fvprc) :: temp_ad1
    REAL(fvprc) :: temp_ad2
    REAL(fvprc) :: temp_ad3
    INTEGER :: branch
    REAL(fvprc) :: x2
    REAL(fvprc) :: x1
    REAL(fvprc) :: y2
    REAL(fvprc) :: y1
    CALL POPCONTROL1B_FV(branch)
    IF (branch .EQ. 0) THEN
      CALL POPINTEGER(je)
      CALL POPREALARRAY(fy, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+2)/8)
      CALL POPREALARRAY(fx, 8*(bd%ie-bd%is+2)*(bd%je-bd%js+1)/8)
      CALL POPINTEGER(nsplt)
      CALL POPINTEGER(is)
      CALL POPREALARRAY(yfx, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+2)*&
&                  npz/8)
      CALL POPREALARRAY(ra_y, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+1)/8&
&                 )
      CALL POPREALARRAY(ra_x, 8*(bd%ie-bd%is+1)*(bd%jed-bd%jsd+1)/8&
&                 )
      CALL POPINTEGER(ie)
      CALL POPREALARRAY(frac)
      CALL POPREALARRAY(dp2, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+1)*npz/&
&                  8)
      CALL POPREALARRAY(xfx, 8*(bd%ie-bd%is+2)*(bd%jed-bd%jsd+1)*&
&                  npz/8)
      CALL POPINTEGER(js)
      xfx_ad = 0.0_FVPRC
      yfx_ad = 0.0_FVPRC
    ELSE
      CALL POPINTEGER(je)
      CALL POPREALARRAY(fy, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+2)/8)
      CALL POPREALARRAY(fx, 8*(bd%ie-bd%is+2)*(bd%je-bd%js+1)/8)
      CALL POPINTEGER(nsplt)
      CALL POPREALARRAY(rdt)
      CALL POPINTEGER(is)
      CALL POPREALARRAY(yfx, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+2)*&
&                  npz/8)
      CALL POPREALARRAY(ra_y, 8*(bd%ied-bd%isd+1)*(bd%je-bd%js+1)/8&
&                 )
      CALL POPREALARRAY(ra_x, 8*(bd%ie-bd%is+1)*(bd%jed-bd%jsd+1)/8&
&                 )
      CALL POPINTEGER(ie)
      CALL POPREALARRAY(frac)
      CALL POPREALARRAY(dp2, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+1)*npz/&
&                  8)
      CALL POPREALARRAY(xfx, 8*(bd%ie-bd%is+2)*(bd%jed-bd%jsd+1)*&
&                  npz/8)
      CALL POPINTEGER(js)
      xfx_ad = 0.0_FVPRC
      yfx_ad = 0.0_FVPRC
      DO k=npz,1,-1
        DO j=je,js,-1
          DO i=ie,is,-1
            CALL POPREALARRAY(dp1(i, j, k))
            temp_ad3 = gridstruct%rarea(i, j)*rdt*dp1_ad(i, j, k)
            xfx_ad(i+1, j, k) = xfx_ad(i+1, j, k) + temp_ad3
            xfx_ad(i, j, k) = xfx_ad(i, j, k) - temp_ad3
            yfx_ad(i, j+1, k) = yfx_ad(i, j+1, k) + temp_ad3
            yfx_ad(i, j, k) = yfx_ad(i, j, k) - temp_ad3
            dp1_ad(i, j, k) = 0.0_FVPRC
          END DO
        END DO
      END DO
    END IF
    jsd = bd%jsd
    ied = bd%ied
    isd = bd%isd
    jed = bd%jed
    dp2_ad = 0.0_FVPRC
    ra_x_ad = 0.0_FVPRC
    ra_y_ad = 0.0_FVPRC
    fx_ad = 0.0_FVPRC
    fy_ad = 0.0_FVPRC
    DO it=nsplt,1,-1
      CALL POPCONTROL1B_FV(branch)
      IF (branch .NE. 0) THEN
        DO k=npz,1,-1
          DO j=je,js,-1
            DO i=ie,is,-1
              CALL POPREALARRAY(dp2(i, j, k))
              temp_ad2 = gridstruct%rarea(i, j)*dp2_ad(i, j, k)
              dp1_ad(i, j, k) = dp1_ad(i, j, k) + dp2_ad(i, j, k)
              mfx_ad(i, j, k) = mfx_ad(i, j, k) + temp_ad2
              mfx_ad(i+1, j, k) = mfx_ad(i+1, j, k) - temp_ad2
              mfy_ad(i, j, k) = mfy_ad(i, j, k) + temp_ad2
              mfy_ad(i, j+1, k) = mfy_ad(i, j+1, k) - temp_ad2
              dp2_ad(i, j, k) = dp1_ad(i, j, k)
              CALL POPREALARRAY(dp1(i, j, k))
              dp1_ad(i, j, k) = 0.0_FVPRC
            END DO
          END DO
        END DO
        CALL POPREALARRAY(q, 8*(bd%ied-bd%isd+1)*(bd%jed-bd%jsd+1)*&
&                    npz*nq/8)
        CALL START_GROUP_HALO_UPDATE_ADM(q_pack, q_packp, q, q_ad, &
&                                  domain)
      END IF
      DO k=npz,1,-1
        DO iq=nq,1,-1
          DO j=je,js,-1
            DO i=ie,is,-1
              CALL POPREALARRAY(q(i, j, k, iq))
              temp_ad0 = q_ad(i, j, k, iq)/dp2(i, j, k)
              temp = q(i, j, k, iq)
              temp_ad1 = gridstruct%rarea(i, j)*temp_ad0
              dp1_ad(i, j, k) = dp1_ad(i, j, k) + temp*temp_ad0
              fx_ad(i, j) = fx_ad(i, j) + temp_ad1
              fx_ad(i+1, j) = fx_ad(i+1, j) - temp_ad1
              fy_ad(i, j) = fy_ad(i, j) + temp_ad1
              fy_ad(i, j+1) = fy_ad(i, j+1) - temp_ad1
              dp2_ad(i, j, k) = dp2_ad(i, j, k) - (temp*dp1(i, j, k)+&
&               gridstruct%rarea(i, j)*(fx(i, j)-fx(i+1, j)+fy(i, j)-fy(&
&               i, j+1)))*temp_ad0/dp2(i, j, k)
              q_ad(i, j, k, iq) = dp1(i, j, k)*temp_ad0
            END DO
          END DO
          CALL POPCONTROL1B_FV(branch)
          IF (branch .EQ. 0) THEN
            CALL POPREALARRAY(q(isd:ied, jsd:jed, k, iq), 8*(ied-&
&                        isd+1)*(jed-jsd+1)/8)
            CALL POPREALARRAY(fx, 8*(bd%ie-bd%is+2)*(bd%je-bd%js+1)&
&                        /8)
            CALL POPREALARRAY(fy, 8*(bd%ie-bd%is+1)*(bd%je-bd%js+2)&
&                        /8)
            CALL FV_TP_2D_ADM(q(isd:ied, jsd:jed, k, iq), q_ad(isd:ied, &
&                       jsd:jed, k, iq), cx(is:ie+1, jsd:jed, k), cx_ad(&
&                       is:ie+1, jsd:jed, k), cy(isd:ied, js:je+1, k), &
&                       cy_ad(isd:ied, js:je+1, k), npx, npy, hord_pert&
&                       , fx, fx_ad, fy, fy_ad, xfx(is:ie+1, jsd:jed, k)&
&                       , xfx_ad(is:ie+1, jsd:jed, k), yfx(isd:ied, js:&
&                       je+1, k), yfx_ad(isd:ied, js:je+1, k), &
&                       gridstruct, bd, ra_x, ra_x_ad, ra_y, ra_y_ad, &
&                       mfx(is:ie+1, js:je, k), mfx_ad(is:ie+1, js:je, k&
&                       ), mfy(is:ie, js:je+1, k), mfy_ad(is:ie, js:je+1&
&                       , k))
          ELSE
            CALL FV_TP_2D_BWD(q(isd:ied, jsd:jed, k, iq), q_ad(isd:&
&                          ied, jsd:jed, k, iq), cx(is:ie+1, jsd:jed, k)&
&                          , cx_ad(is:ie+1, jsd:jed, k), cy(isd:ied, js:&
&                          je+1, k), cy_ad(isd:ied, js:je+1, k), npx, &
&                          npy, hord, fx, fx_ad, fy, fy_ad, xfx(is:ie+1&
&                          , jsd:jed, k), xfx_ad(is:ie+1, jsd:jed, k), &
&                          yfx(isd:ied, js:je+1, k), yfx_ad(isd:ied, js:&
&                          je+1, k), gridstruct, bd, ra_x, ra_x_ad, ra_y&
&                          , ra_y_ad, mfx(is:ie+1, js:je, k), mfx_ad(is:&
&                          ie+1, js:je, k), mfy(is:ie, js:je+1, k), &
&                          mfy_ad(is:ie, js:je+1, k))
          END IF
        END DO
        DO j=je,js,-1
          DO i=ied,isd,-1
            CALL POPREALARRAY(ra_y(i, j))
            yfx_ad(i, j, k) = yfx_ad(i, j, k) + ra_y_ad(i, j)
            yfx_ad(i, j+1, k) = yfx_ad(i, j+1, k) - ra_y_ad(i, j)
            ra_y_ad(i, j) = 0.0_FVPRC
          END DO
        END DO
        DO j=jed,jsd,-1
          DO i=ie,is,-1
            CALL POPREALARRAY(ra_x(i, j))
            xfx_ad(i, j, k) = xfx_ad(i, j, k) + ra_x_ad(i, j)
            xfx_ad(i+1, j, k) = xfx_ad(i+1, j, k) - ra_x_ad(i, j)
            ra_x_ad(i, j) = 0.0_FVPRC
          END DO
        END DO
      END DO
    END DO
    DO k=npz,1,-1
      DO j=je,js,-1
        DO i=ie,is,-1
          temp_ad = gridstruct%rarea(i, j)*dp2_ad(i, j, k)
          dp1_ad(i, j, k) = dp1_ad(i, j, k) + dp2_ad(i, j, k)
          mfx_ad(i, j, k) = mfx_ad(i, j, k) + temp_ad
          mfx_ad(i+1, j, k) = mfx_ad(i+1, j, k) - temp_ad
          mfy_ad(i, j, k) = mfy_ad(i, j, k) + temp_ad
          mfy_ad(i, j+1, k) = mfy_ad(i, j+1, k) - temp_ad
          dp2_ad(i, j, k) = 0.0_FVPRC
        END DO
      END DO
    END DO
    CALL POPCONTROL1B_FV(branch)
    IF (branch .NE. 0) THEN
      DO k=npz,1,-1
        DO j=je+1,js,-1
          DO i=ie,is,-1
            CALL POPREALARRAY(mfy(i, j, k))
            mfy_ad(i, j, k) = frac*mfy_ad(i, j, k)
          END DO
        END DO
        DO j=je+1,js,-1
          DO i=ied,isd,-1
            yfx_ad(i, j, k) = frac*yfx_ad(i, j, k)
            CALL POPREALARRAY(cy(i, j, k))
            cy_ad(i, j, k) = frac*cy_ad(i, j, k)
          END DO
        END DO
        DO j=je,js,-1
          DO i=ie+1,is,-1
            CALL POPREALARRAY(mfx(i, j, k))
            mfx_ad(i, j, k) = frac*mfx_ad(i, j, k)
          END DO
        END DO
        DO j=jed,jsd,-1
          DO i=ie+1,is,-1
            xfx_ad(i, j, k) = frac*xfx_ad(i, j, k)
            CALL POPREALARRAY(cx(i, j, k))
            cx_ad(i, j, k) = frac*cx_ad(i, j, k)
          END DO
        END DO
      END DO
    END IF
    CALL POPCONTROL2B_FV(branch)
    IF (branch .EQ. 0) THEN
      DO k=npz,2,-1
        CALL POPCONTROL1B_FV(branch)
      END DO
    ELSE IF (branch .NE. 1) THEN
      GOTO 100
    END IF
    DO k=npz,1,-1
      CALL POPCONTROL1B_FV(branch)
      IF (branch .EQ. 0) THEN
        DO j=je,js,-1
          DO i=ie,is,-1
            CALL POPCONTROL1B_FV(branch)
          END DO
        END DO
      ELSE
        DO j=je,js,-1
          DO i=ie,is,-1
            CALL POPCONTROL1B_FV(branch)
          END DO
        END DO
      END IF
    END DO
 100 DO k=npz,1,-1
      DO j=je+1,js,-1
        DO i=ied,isd,-1
          CALL POPCONTROL1B_FV(branch)
          IF (branch .EQ. 0) THEN
            cy_ad(i, j, k) = cy_ad(i, j, k) + gridstruct%dya(i, j)*&
&             gridstruct%dx(i, j)*gridstruct%sin_sg(i, j, 2)*yfx_ad(i, j&
&             , k)
            yfx_ad(i, j, k) = 0.0_FVPRC
          ELSE
            cy_ad(i, j, k) = cy_ad(i, j, k) + gridstruct%dya(i, j-1)*&
&             gridstruct%dx(i, j)*gridstruct%sin_sg(i, j-1, 4)*yfx_ad(i&
&             , j, k)
            yfx_ad(i, j, k) = 0.0_FVPRC
          END IF
        END DO
      END DO
      DO j=jed,jsd,-1
        DO i=ie+1,is,-1
          CALL POPCONTROL1B_FV(branch)
          IF (branch .EQ. 0) THEN
            cx_ad(i, j, k) = cx_ad(i, j, k) + gridstruct%dxa(i, j)*&
&             gridstruct%dy(i, j)*gridstruct%sin_sg(i, j, 1)*xfx_ad(i, j&
&             , k)
            xfx_ad(i, j, k) = 0.0_FVPRC
          ELSE
            cx_ad(i, j, k) = cx_ad(i, j, k) + gridstruct%dxa(i-1, j)*&
&             gridstruct%dy(i, j)*gridstruct%sin_sg(i-1, j, 3)*xfx_ad(i&
&             , j, k)
            xfx_ad(i, j, k) = 0.0_FVPRC
          END IF
        END DO
      END DO
    END DO
  END SUBROUTINE TRACER_2D_NESTED_BWD
!subroutine offline_tracer_advection(q, ple0, ple1, mfx, mfy, cx, cy, &
!                                    gridstruct, neststruct, bd, domain, &
!                                    ak, bk, ptop, npx, npy, npz,   &
!                                    nq, hord, kord, q_split, k_split, dt, z_tracer, fill)
!
!      use fv_mapz_mod,        only: map1_q2
!      use fv_fill_mod,        only: fillz
!
!      integer, intent(IN) :: npx
!      integer, intent(IN) :: npy
!      integer, intent(IN) :: npz
!      integer, intent(IN) :: nq    ! number of tracers to be advected
!      integer, intent(IN) :: hord
!      integer, intent(IN) :: kord
!      integer, intent(IN) :: q_split
!      integer, intent(IN) :: k_split
!      logical, intent(IN) :: z_tracer
!      logical, intent(IN) :: fill
!      type(fv_grid_bounds_type), intent(IN   ) :: bd
!      type(fv_nest_type), intent(INOUT) :: neststruct
!      type(fv_grid_type), intent(IN), target :: gridstruct
!      type(domain2D), intent(INOUT) :: domain
!
!      real(FVPRC), intent(IN   ) :: dt
!      real(FVPRC), intent(IN   ) ::ple0(bd%is:bd%ie,bd%js:bd%je,npz+1)      ! DELP before dyn_core
!      real(FVPRC), intent(INOUT) ::ple1(bd%is:bd%ie,bd%js:bd%je,npz+1)      ! DELP after dyn_core
!      real(FVPRC), intent(IN   ) ::  cx(bd%is:bd%ie,bd%js:bd%je,npz)        ! Courant Number X-Dir
!      real(FVPRC), intent(IN   ) ::  cy(bd%is:bd%ie,bd%js:bd%je,npz)        ! Courant Number Y-Dir
!      real(FVPRC), intent(IN   ) :: mfx(bd%is:bd%ie,bd%js:bd%je,npz)        ! Mass Flux X-Dir
!      real(FVPRC), intent(IN   ) :: mfy(bd%is:bd%ie,bd%js:bd%je,npz)        ! Mass Flux Y-Dir
!      real(FVPRC), intent(INOUT) ::   q(bd%is:bd%ie,bd%js:bd%je,npz,nq)     ! Tracers
!      real(FVPRC), intent(IN   ) ::  ak(npz+1)                  ! AK for remapping
!      real(FVPRC), intent(IN   ) ::  bk(npz+1)                  ! BK for remapping
!      real(FVPRC), intent(IN   ) :: ptop
!! Local Arrays
!      real(FVPRC) ::   xL(bd%isd:bd%ied+1,bd%jsd:bd%jed  ,npz)  ! X-Dir for MPP Updates
!      real(FVPRC) ::   yL(bd%isd:bd%ied  ,bd%jsd:bd%jed+1,npz)  ! Y-Dir for MPP Updates
!      real(FVPRC) ::  cxL(bd%is :bd%ie +1,bd%jsd:bd%jed  ,npz)  ! Courant Number X-Dir
!      real(FVPRC) ::  cyL(bd%isd:bd%ied  ,bd%js :bd%je +1,npz)  ! Courant Number Y-Dir
!      real(FVPRC) :: mfxL(bd%is :bd%ie +1,bd%js :bd%je   ,npz)  ! Mass Flux X-Dir
!      real(FVPRC) :: mfyL(bd%is :bd%ie   ,bd%js :bd%je +1,npz)  ! Mass Flux Y-Dir
!      real(FVPRC) ::  dpL(bd%is :bd%ie   ,bd%js :bd%je   ,npz)  ! Pressure Thickness
!      real(FVPRC) ::  dpA(bd%is :bd%ie   ,bd%js :bd%je   ,npz)  ! Pressure Thickness
!! Local Tracer Arrays
!      real(FVPRC) ::   q1(bd%is:bd%ie  ,bd%js:bd%je, npz   )! 2D Tracers
!      real(FVPRC) ::   q2(bd%isd:bd%ied  ,bd%jsd:bd%jed     ,nq)! 2D Tracers
!      real(FVPRC) ::   q3(bd%isd:bd%ied  ,bd%jsd:bd%jed, npz,nq)! 3D Tracers
!! Local Buffer Arrarys
!      real(FVPRC) :: wbuffer(bd%js:bd%je,npz)
!      real(FVPRC) :: sbuffer(bd%is:bd%ie,npz)
!      real(FVPRC) :: ebuffer(bd%js:bd%je,npz)
!      real(FVPRC) :: nbuffer(bd%is:bd%ie,npz)
!! Local Remap Arrays
!      real(FVPRC)  pe1(bd%is:bd%ie,npz+1)
!      real(FVPRC)  pe2(bd%is:bd%ie,npz+1)
!      real(FVPRC)  dp2(bd%is:bd%ie,bd%js:bd%je,npz)
!
!! Local indices
!      integer     :: i,j,k,n,iq
!      real(FVPRC) :: dtR8
!
!      real(FVPRC) :: scalingFactor
!
!      type(group_halo_update_type), save :: i_pack
!
!      integer :: is,  ie,  js,  je
!      integer :: isd, ied, jsd, jed
!
!      is  = bd%is
!      ie  = bd%ie
!      js  = bd%js
!      je  = bd%je
!      isd = bd%isd
!      ied = bd%ied
!      jsd = bd%jsd
!      jed = bd%jed
!
!! Time-step
!    dtR8=dt
!! Fill CX/CY C-Grid boundaries and update ghost regions
!    xL(is:ie,js:je,:) = cx(:,:,:)
!    yL(is:ie,js:je,:) = cy(:,:,:)
!    call mpp_get_boundary(xL, yL, domain, &
!                          wbufferx=wbuffer, ebufferx=ebuffer, &
!                          sbuffery=sbuffer, nbuffery=nbuffer, &
!                          gridtype=CGRID_NE )
!    xL(ie+1,js:je,:) = ebuffer
!    yL(is:ie,je+1,:) = nbuffer
!    call mpp_update_domains( xL, yL, domain, gridtype=CGRID_NE, complete=.true.)
!    cxL(is:ie+1,jsd:jed,:) = xL(is:ie+1,jsd:jed,:)
!    cyL(isd:ied,js:je+1,:) = yL(isd:ied,js:je+1,:)
!
!! Fill MFX/MFY C-Grid boundaries
!    xL(is:ie,js:je,:) = mfx(:,:,:)
!    yL(is:ie,js:je,:) = mfy(:,:,:)
!    call mpp_get_boundary(xL, yL, domain, &
!                          wbufferx=wbuffer, ebufferx=ebuffer, &
!                          sbuffery=sbuffer, nbuffery=nbuffer, &
!                          gridtype=CGRID_NE )
!    xL(ie+1,js:je,:) = ebuffer
!    yL(is:ie,je+1,:) = nbuffer
!    mfxL(is:ie+1,js:je,:) = xL(is:ie+1,js:je,:)
!    mfyL(is:ie,js:je+1,:) = yL(is:ie,js:je+1,:)
!
!! Fill local tracers and pressure thickness
!    dpL(:,:,:) = ple0(:,:,2:npz+1) - ple0(:,:,1:npz)
!    q3(is:ie,js:je,:,:) = q(is:ie,js:je,:,:)
!
!    call start_group_halo_update(i_pack, q3, domain)
!
!    if ( z_tracer ) then
!!$omp parallel do default(shared) private(q2)
!       do k=1,npz
!         do iq=1,nq
!            do j=js,je
!               do i=is,ie                   ! To_do list:
!                  q2(i,j,iq) = q3(i,j,k,iq) ! The data copying can be avoided if q is
!                                            ! re-dimensioned as q(i,j,nq,k)
!               enddo
!            enddo
!         enddo
!         call tracer_2d_1L(q2, dpL(is,js,k), mfxL(is,js,k), mfyL(is,js,k), &
!                           cxL(is,jsd,k),  cyL(isd,js,k), gridstruct, neststruct, bd, domain, npx, npy, npz,   &
!                           1, nq, hord, q_split, k, q3, dtR8, 0, k_split, dpA=dpA)
!       enddo
!    else
!         call tracer_2d(q3, dpL, mfxL, mfyL, cxL, cyL, gridstruct, bd, domain, npx, npy, npz, 1, nq, &
!                        hord, q_split, dtR8, 0, i_pack, .false., k_split, dpA=dpA)
!    endif
!
!!------------------------------------------------------------------
!! Re-Map constituents
!! Do remapping one tracer at a time; seems to be faster
!! It requires less memory than mapn_ppm
!!------------------------------------------------------------------
!
!       do iq=1,nq
!          do j=js,je
!           ! pressures mapping from (dpA is new delp after tracer_2d)
!             pe1(:,1) = ptop
!             do k=2,npz+1
!               pe1(:,k) = pe1(:,k-1) + dpA(:,j,k-1)
!             enddo
!           ! pressures mapping to
!             pe2(:,1) = ptop
!             pe2(:,npz+1) = pe1(:,npz+1)
!             do k=2,npz
!                 pe2(:  ,k) = ak(k) + bk(k)*pe1(:,npz+1)
!             enddo
!             do k=1,npz
!                dp2(:,j,k) = pe2(:,k+1) - pe2(:,k)
!             enddo
!             call map1_q2(npz, pe1, q3(isd,jsd,1,iq),      &
!                          npz, pe2, q1(:,j,:), dp2(:,j,:), &
!                          is, ie, 0, kord, j,              &
!                          isd, ied, jsd, jed, 0._FVPRC) 
!             if (fill) call fillz(ie-is+1, npz, 1, q1(:,j,:), dp2(:,j,:))
!          enddo
!          ! Rescale tracers based on ple1 at destination timestep
!          !------------------------------------------------------
!
!          scalingFactor = calcScalingFactor(q1, dp2, ple1, npx, npy, npz, gridstruct, bd)
!          !scalingFactors = computeScalingFactors(q1, dp2, ple1, npx, npy, npz)
!
!          ! Return tracers
!          !---------------
!          q(is:ie,js:je,1:npz,iq) = q1(is:ie,js:je,1:npz) * scalingFactor
!          !do k =1,npz
!             !do j = js,je
!                !do i = is,ie
!                   !q(i,j,k,iq) = q1(i,j,k)
!                !enddo
!             !enddo
!          !enddo
!
!       enddo
!
!end subroutine offline_tracer_advection
!------------------------------------------------------------------------------------
!         function calcScalingFactor(q1, dp2, ple1, npx, npy, npz, gridstruct, bd) result(scaling)
!         use mpp_mod, only: mpp_sum
!         integer, intent(in) :: npx
!         integer, intent(in) :: npy
!         integer, intent(in) :: npz
!         real(FVPRC), intent(in) :: q1(:,:,:)
!         real(FVPRC), intent(in) :: dp2(:,:,:)
!         real(FVPRC), intent(in) :: ple1(:,:,:)
!         type(fv_grid_type), intent(IN   ) :: gridstruct
!         type(fv_grid_bounds_type), intent(IN   ) :: bd
!         real(FVPRC) :: scaling
!
!         integer :: k
!         real(FVPRC) :: partialSums(2,npz), globalSums(2)
!         real(FVPRC), parameter :: TINY_DENOMINATOR = tiny(1.0)
!
!         !-------
!         ! Compute partial sum on local array first to minimize communication.
!         ! This algorithm will not be strongly repdroducible under changes do domain
!         ! decomposition, but uses far less communication bandwidth (and memory BW)
!         ! then the preceding implementation.
!         !-------
!         do k = 1, npz
!            ! numerator
!            partialSums(1,k) = sum(q1(:,:,k)*dp2(:,:,k)*gridstruct%area(bd%is:bd%ie,bd%js:bd%je))
!            ! denominator
!            partialSums(2,k) = sum(q1(:,:,k)*(ple1(:,:,k+1)-ple1(:,:,k))*gridstruct%area(bd%is:bd%ie,bd%js:bd%je))
!         end do
!
!         globalSums(1) = sum(partialSums(1,:))
!         globalSums(2) = sum(partialSums(2,:))
!
!         call mpp_sum(globalSums, 2)
!
!         if (globalSums(2) > TINY_DENOMINATOR) then
!            scaling =  globalSums(1) / globalSums(2)
!            !#################################################################
!            ! This line was added to ensure strong reproducibility of the code
!            !#################################################################
!            scaling = REAL(scaling, KIND=REAL4)
!         else
!            scaling = 1.d0
!         end if
!
!         end function calcScalingFactor
  SUBROUTINE TRACER_2D_NESTED(q, dp1, mfx, mfy, cx, cy, gridstruct, bd, &
&   domain, npx, npy, npz, nq, hord, hord_pert, q_split, dt, id_divg, &
&   q_pack, q_packp, z_tracer, k_split, neststruct, parent_grid)
    IMPLICIT NONE
    TYPE(FV_GRID_BOUNDS_TYPE), INTENT(IN) :: bd
    INTEGER, INTENT(IN) :: npx
    INTEGER, INTENT(IN) :: npy
    INTEGER, INTENT(IN) :: npz
! number of tracers to be advected
    INTEGER, INTENT(IN) :: nq
    INTEGER, INTENT(IN) :: hord, hord_pert
    INTEGER, INTENT(IN) :: q_split, k_split
    INTEGER, INTENT(IN) :: id_divg
    REAL(fvprc), INTENT(IN) :: dt
    LOGICAL, INTENT(IN) :: z_tracer
    TYPE(GROUP_HALO_UPDATE_TYPE), INTENT(INOUT) :: q_pack, q_packp
! Tracers
    REAL(fvprc), INTENT(INOUT) :: q(bd%isd:bd%ied, bd%jsd:bd%jed, npz, &
&   nq)
! DELP before dyn_core
    REAL(fvprc), INTENT(INOUT) :: dp1(bd%is:bd%ie, bd%js:bd%je, npz)
! Mass Flux X-Dir
    REAL(fvprc), INTENT(INOUT) :: mfx(bd%is:bd%ie+1, bd%js:bd%je, npz)
! Mass Flux Y-Dir
    REAL(fvprc), INTENT(INOUT) :: mfy(bd%is:bd%ie, bd%js:bd%je+1, npz)
! Courant Number X-Dir
    REAL(fvprc), INTENT(INOUT) :: cx(bd%is:bd%ie+1, bd%jsd:bd%jed, npz)
! Courant Number Y-Dir
    REAL(fvprc), INTENT(INOUT) :: cy(bd%isd:bd%ied, bd%js:bd%je+1, npz)
    TYPE(FV_GRID_TYPE), INTENT(IN), TARGET :: gridstruct
    TYPE(FV_NEST_TYPE), INTENT(INOUT) :: neststruct
    TYPE(FV_ATMOS_TYPE), INTENT(INOUT) :: parent_grid
    TYPE(DOMAIN2D), INTENT(INOUT) :: domain
! Local Arrays
    REAL(fvprc) :: dp2(bd%is:bd%ie, bd%js:bd%je, npz)
!#ifdef FLUXBCS
!      real(FVPRC) :: fx(bd%is:bd%ie+1,bd%js:bd%je  ,npz,nq)
!      real(FVPRC) :: fy(bd%is:bd%ie , bd%js:bd%je+1,npz,nq)
!#else
    REAL(fvprc) :: fx(bd%is:bd%ie+1, bd%js:bd%je)
    REAL(fvprc) :: fy(bd%is:bd%ie, bd%js:bd%je+1)
!#endif
    REAL(fvprc) :: ra_x(bd%is:bd%ie, bd%jsd:bd%jed)
    REAL(fvprc) :: ra_y(bd%isd:bd%ied, bd%js:bd%je)
    REAL(fvprc) :: xfx(bd%is:bd%ie+1, bd%jsd:bd%jed, npz)
    REAL(fvprc) :: yfx(bd%isd:bd%ied, bd%js:bd%je+1, npz)
    REAL(fvprc) :: cmax(npz)
    REAL(fvprc) :: cmax_t
    REAL(fvprc) :: c_global
    REAL(fvprc) :: frac, rdt
    REAL(fvprc), PARAMETER :: esl=1.e-24
    INTEGER :: nsplt, nsplt_parent
    INTEGER, SAVE :: msg_split_steps=1
    INTEGER :: i, j, k, it, iq, n
! Tracers
    REAL(fvprc) :: q_tj(bd%isd:bd%ied, bd%jsd:bd%jed, npz, nq)
    REAL(fvprc) :: fx_tj(bd%is:bd%ie+1, bd%js:bd%je)
    REAL(fvprc) :: fy_tj(bd%is:bd%ie, bd%js:bd%je+1)
!      real(FVPRC), pointer, dimension(:,:) :: area, rarea
!      real(FVPRC), pointer, dimension(:,:,:) :: sin_sg
!      real(FVPRC), pointer, dimension(:,:) :: dxa, dya, dx, dy
    INTEGER :: is, ie, js, je
    INTEGER :: isd, ied, jsd, jed
    REAL(fvprc) :: step, split
    INTRINSIC ABS
    INTRINSIC MAX
    INTRINSIC INT
    INTRINSIC REAL
    REAL(fvprc) :: max1
    REAL(fvprc) :: x2
    REAL(fvprc) :: x1
    REAL(fvprc) :: y2
    REAL(fvprc) :: y1
    is = bd%is
    ie = bd%ie
    js = bd%js
    je = bd%je
    isd = bd%isd
    ied = bd%ied
    jsd = bd%jsd
    jed = bd%jed
!       area => gridstruct%area
!      rarea => gridstruct%rarea
!      sin_sg => gridstruct%sin_sg
!      dxa    => gridstruct%dxa 
!      dya    => gridstruct%dya 
!      dx     => gridstruct%dx  
!      dy     => gridstruct%dy  
!$OMP parallel do default(none) shared(is,ie,js,je,isd,ied,jsd,jed,npz,cx,cy,xfx,yfx, &
!$OMP                                  dxa,dya,dx,dy,sin_sg)
    DO k=1,npz
      DO j=jsd,jed
        DO i=is,ie+1
          IF (cx(i, j, k) .GT. 0.) THEN
            xfx(i, j, k) = cx(i, j, k)*gridstruct%dxa(i-1, j)*gridstruct&
&             %dy(i, j)*gridstruct%sin_sg(i-1, j, 3)
          ELSE
            xfx(i, j, k) = cx(i, j, k)*gridstruct%dxa(i, j)*gridstruct%&
&             dy(i, j)*gridstruct%sin_sg(i, j, 1)
          END IF
        END DO
      END DO
      DO j=js,je+1
        DO i=isd,ied
          IF (cy(i, j, k) .GT. 0.) THEN
            yfx(i, j, k) = cy(i, j, k)*gridstruct%dya(i, j-1)*gridstruct&
&             %dx(i, j)*gridstruct%sin_sg(i, j-1, 4)
          ELSE
            yfx(i, j, k) = cy(i, j, k)*gridstruct%dya(i, j)*gridstruct%&
&             dx(i, j)*gridstruct%sin_sg(i, j, 2)
          END IF
        END DO
      END DO
    END DO
!--------------------------------------------------------------------------------
    IF (q_split .EQ. 0) THEN
! Determine nsplt
!$OMP parallel do default(none) shared(is,ie,js,je,npz,cmax,cx,cy,sin_sg) &
!$OMP                          private(cmax_t )
      DO k=1,npz
        cmax(k) = 0.
        IF (k .LT. 4) THEN
! Top layers: C < max( abs(c_x), abs(c_y) )
          DO j=js,je
            DO i=is,ie
              IF (cx(i, j, k) .GE. 0.) THEN
                x1 = cx(i, j, k)
              ELSE
                x1 = -cx(i, j, k)
              END IF
              IF (cy(i, j, k) .GE. 0.) THEN
                y1 = cy(i, j, k)
              ELSE
                y1 = -cy(i, j, k)
              END IF
              IF (x1 .LT. y1) THEN
                cmax_t = y1
              ELSE
                cmax_t = x1
              END IF
              IF (cmax_t .LT. cmax(k)) THEN
                cmax(k) = cmax(k)
              ELSE
                cmax(k) = cmax_t
              END IF
            END DO
          END DO
        ELSE
          DO j=js,je
            DO i=is,ie
              IF (cx(i, j, k) .GE. 0.) THEN
                x2 = cx(i, j, k)
              ELSE
                x2 = -cx(i, j, k)
              END IF
              IF (cy(i, j, k) .GE. 0.) THEN
                y2 = cy(i, j, k)
              ELSE
                y2 = -cy(i, j, k)
              END IF
              IF (x2 .LT. y2) THEN
                max1 = y2
              ELSE
                max1 = x2
              END IF
              cmax_t = max1 + 1. - gridstruct%sin_sg(i, j, 5)
              IF (cmax_t .LT. cmax(k)) THEN
                cmax(k) = cmax(k)
              ELSE
                cmax(k) = cmax_t
              END IF
            END DO
          END DO
        END IF
      END DO
      CALL MP_REDUCE_MAX(cmax, npz)
! find global max courant number and define nsplt to scale cx,cy,mfx,mfy
      c_global = cmax(1)
      IF (npz .NE. 1) THEN
! if NOT shallow water test case
        DO k=2,npz
          IF (cmax(k) .LT. c_global) THEN
            c_global = c_global
          ELSE
            c_global = cmax(k)
          END IF
        END DO
      END IF
      nsplt = INT(1. + c_global)
!#ifdef FLUXBCS
!      !!*****NOTE*****
!      !! If setting the FLUXBCS directive do note that the current
!      !!  version of the code requires that Atm, tile, and pelist be
!      !!  brought in through use statements. This code will need re
!      !! -writing to avoid this.
!
!      !If using flux BCs, nested grid nsplit must be an
!      !even multiple of that on the coarse grid
!      if (neststruct%do_flux_BCs .or. (gridstruct%nested .and. neststruct%nestbctype > 1)) then
!         !Receive from all parent grids
!         if (gridstruct%nested) then
!            !!NOTE about mpp_recv/mpp_send and scalars:
!            !! When passing a scalar, the second argument is not SIZE (which is known to be 1) but a process ID
!            call mpp_recv(nsplt_parent,parent_grid%pelist(1))
!            nsplt = ceiling(real(nsplt)/real(nsplt_parent))*nsplt
!            nsplt = max(nsplt,nsplt_parent)
!            msg_split_steps = nsplt/nsplt_parent
!         endif
!      endif
!#endif
!if ( master )  write(*,*) 'Tracer_2d_split=', nsplt, c_global 
!if ( is_master() .and. nsplt > 3 )  write(*,*) 'Tracer_2d_split=', nsplt, c_global 
    ELSE
      nsplt = q_split
      IF (gridstruct%nested .AND. neststruct%nestbctype .GT. 1) THEN
        IF (q_split/parent_grid%flagstruct%q_split .LT. 1) THEN
          msg_split_steps = 1
        ELSE
          msg_split_steps = q_split/parent_grid%flagstruct%q_split
        END IF
      END IF
    END IF
!#ifdef FLUXBCS
!   !Make sure to send to any nested grids which might be expecting a coarse-grid nsplit.
!   !(This is outside the if statement since it could be that the coarse grid uses
!   !q_split > 0 but the nested grid has q_split = 0)
!   if (neststruct%do_flux_BCs .or. (gridstruct%nested .and. neststruct%nestbctype > 1)) then
!      if (ANY(neststruct%child_grids) .and. is_master()) then
!         do n=1,size(neststruct%child_grids)
!            if (neststruct%child_grids(n) .and. Atm(n)%flagstruct%q_split == 0) then
!               do i=1,Atm(n)%npes_this_grid
!                  call mpp_send(nsplt,Atm(n)%pelist(i))
!               enddo
!            endif
!         enddo
!      endif
!   endif
!#endif FLUXBCS
!--------------------------------------------------------------------------------
    frac = 1./REAL(nsplt)
    IF (nsplt .NE. 1) THEN
!$OMP parallel do default(none) shared(is,ie,js,je,isd,ied,jsd,jed,npz,cx,cy,xfx,yfx, &
!$OMP                                  mfx,mfy,frac)
      DO k=1,npz
        DO j=jsd,jed
          DO i=is,ie+1
            cx(i, j, k) = cx(i, j, k)*frac
            xfx(i, j, k) = xfx(i, j, k)*frac
          END DO
        END DO
        DO j=js,je
          DO i=is,ie+1
            mfx(i, j, k) = mfx(i, j, k)*frac
          END DO
        END DO
        DO j=js,je+1
          DO i=isd,ied
            cy(i, j, k) = cy(i, j, k)*frac
            yfx(i, j, k) = yfx(i, j, k)*frac
          END DO
        END DO
        DO j=js,je+1
          DO i=is,ie
            mfy(i, j, k) = mfy(i, j, k)*frac
          END DO
        END DO
      END DO
    END IF
!$OMP parallel do default(none) shared(is,ie,js,je,npz,dp2,dp1,mfx,mfy,rarea)
    DO k=1,npz
      DO j=js,je
        DO i=is,ie
          dp2(i, j, k) = dp1(i, j, k) + (mfx(i, j, k)-mfx(i+1, j, k)+mfy&
&           (i, j, k)-mfy(i, j+1, k))*gridstruct%rarea(i, j)
        END DO
      END DO
    END DO
    DO it=1,nsplt
      IF (gridstruct%nested) neststruct%tracer_nest_timestep = &
&         neststruct%tracer_nest_timestep + 1
!call timing_on('COMM_TOTAL')
!call timing_on('COMM_TRACER')
      CALL COMPLETE_GROUP_HALO_UPDATE(q_pack, q_packp, domain)
!call timing_off('COMM_TRACER')
!call timing_off('COMM_TOTAL')
!      if (gridstruct%nested) then
!            step = real(neststruct%tracer_nest_timestep)+real(nsplt*k_split)
!            split = real(nsplt*k_split)
!            do iq=1,nq
!                 call nested_grid_BC_apply_intT(q(isd:ied,jsd:jed,:,iq), &
!                      !0, 0, npx, npy, npz, real(tracer_nest_timestep), real(nsplt), &
!                      0, 0, npx, npy, npz, bd, &
!                      step, split, &
!                      var_east_t0=neststruct%q_BC(iq)%east_t0, &
!                      var_west_t0=neststruct%q_BC(iq)%west_t0, &
!                      var_north_t0=neststruct%q_BC(iq)%north_t0, &
!                      var_south_t0=neststruct%q_BC(iq)%south_t0, &
!                      var_east_t1=neststruct%q_BC(iq)%east_t1, &
!                      var_west_t1=neststruct%q_BC(iq)%west_t1, &
!                      var_north_t1=neststruct%q_BC(iq)%north_t1, &
!                      var_south_t1=neststruct%q_BC(iq)%south_t1, &
!                      bctype=neststruct%nestbctype, &
!                      nsponge=neststruct%nsponge, s_weight=neststruct%s_weight   )
!           enddo
!      endif
!#ifdef FLUXBCS
!
!!$OMP parallel do default(none) shared(is,ie,js,je,isd,ied,jsd,jed,npz,nq,area,xfx,yfx,q, &
!!$OMP                                  cx,cy,npx,npy,hord,fx,fy,gridstruct,bd,mfx,mfy) &
!!$OMP                                  private(ra_x, ra_y)
!      do k=1,npz
!
!         do j=jsd,jed
!            do i=is,ie
!               ra_x(i,j) = gridstruct%area(i,j) + xfx(i,j,k) - xfx(i+1,j,k)
!            enddo
!         enddo
!         do j=js,je
!            do i=isd,ied
!               ra_y(i,j) = gridstruct%area(i,j) + yfx(i,j,k) - yfx(i,j+1,k)
!            enddo
!         enddo
!
!         do iq=1,nq
!            call fv_tp_2d(q(isd,jsd,k,iq), cx(is,jsd,k), cy(isd,js,k), &
!                          npx, npy, hord, fx(is,js,k,iq), fy(is,js,k,iq), xfx(is,jsd,k), yfx(isd,js,k), &
!                          gridstruct, bd, ra_x, ra_y, mfx=mfx(is,js,k), mfy=mfy(is,js,k))
!         enddo
!      enddo
!
!      if (neststruct%do_flux_BCs .or. (gridstruct%nested .and. neststruct%nestbctype > 1) ) then
!
!         !call FCT_PD(q,fx,fy,dp1,npx,npy,npz,nq,gridstruct%area, domain)
!
!         call flux_BCs(fx, fy, it, msg_split_steps, npx, npy, npz, nq, q, dp1, dp2, cx, cy, gridstruct%nested, neststruct, paren
!t_grid)
!         !call flux_BCs(fx, fy, it, nsplt, npx, npy, npz, nq, q, dp1, dp2, cx, cy)
!
!      endif
!!$OMP parallel do default(none) shared(is,ie,js,je,npz,nq,q,dp1,fx,fy,rarea,dp2)
!      do k=1,npz
!         do iq=1,nq
!
!            do j=js,je
!               do i=is,ie
!                  q(i,j,k,iq) = ( q(i,j,k,iq)*dp1(i,j,k) + &
!                                (fx(i,j,k,iq)-fx(i+1,j,k,iq)+fy(i,j,k,iq)-fy(i,j+1,k,iq))*gridstruct%rarea(i,j) )/dp2(i,j,k)
!               enddo
!            enddo
!
!         enddo
!      enddo ! npz
!
!#else
!$OMP parallel do default(none) shared(is,ie,js,je,isd,ied,jsd,jed,npz,gridstruct%area,xfx,  &
!$OMP                                  yfx,q,iq,cx,cy,npx,npy,hord,nq,gridstruct, &
!$OMP                                  mfx,mfy,dp1,dp2,rarea,bd)                  &
!$OMP                            private(ra_x, ra_y, fx, fy)
      DO k=1,npz
        DO j=jsd,jed
          DO i=is,ie
            ra_x(i, j) = gridstruct%area(i, j) + xfx(i, j, k) - xfx(i+1&
&             , j, k)
          END DO
        END DO
        DO j=js,je
          DO i=isd,ied
            ra_y(i, j) = gridstruct%area(i, j) + yfx(i, j, k) - yfx(i, j&
&             +1, k)
          END DO
        END DO
        DO iq=1,nq
          IF (hord .EQ. hord_pert) THEN
            CALL FV_TP_2D(q(isd:ied, jsd:jed, k, iq), cx(is:ie+1, jsd&
&                      :jed, k), cy(isd:ied, js:je+1, k), npx, npy, hord&
&                      , fx, fy, xfx(is:ie+1, jsd:jed, k), yfx(isd:ied, &
&                      js:je+1, k), gridstruct, bd, ra_x, ra_y, mfx=mfx(&
&                      is:ie+1, js:je, k), mfy=mfy(is:ie, js:je+1, k))
          ELSE
            q_tj(:, :, k, iq) = q(:, :, k, iq)
            fx_tj = fx
            fy_tj = fy
            CALL FV_TP_2D(q(isd:ied, jsd:jed, k, iq), cx(is:ie+1, jsd:&
&                   jed, k), cy(isd:ied, js:je+1, k), npx, npy, &
&                   hord_pert, fx, fy, xfx(is:ie+1, jsd:jed, k), yfx(isd&
&                   :ied, js:je+1, k), gridstruct, bd, ra_x, ra_y, mfx=&
&                   mfx(is:ie+1, js:je, k), mfy=mfy(is:ie, js:je+1, k))
          END IF
          DO j=js,je
            DO i=is,ie
              q(i, j, k, iq) = (q(i, j, k, iq)*dp1(i, j, k)+(fx(i, j)-fx&
&               (i+1, j)+fy(i, j)-fy(i, j+1))*gridstruct%rarea(i, j))/&
&               dp2(i, j, k)
            END DO
          END DO
        END DO
      END DO
! npz
!#endif
      IF (it .NE. nsplt) THEN
!call timing_on('COMM_TOTAL')
!call timing_on('COMM_TRACER')
        CALL START_GROUP_HALO_UPDATE(q_pack, q_packp, q, domain)
!call timing_off('COMM_TRACER')
!call timing_off('COMM_TOTAL')
!$OMP parallel do default(none) shared(is,ie,js,je,npz,dp1,dp2,mfx,mfy,rarea)
        DO k=1,npz
          DO j=js,je
            DO i=is,ie
              dp1(i, j, k) = dp2(i, j, k)
              dp2(i, j, k) = dp1(i, j, k) + (mfx(i, j, k)-mfx(i+1, j, k)&
&               +mfy(i, j, k)-mfy(i, j+1, k))*gridstruct%rarea(i, j)
            END DO
          END DO
        END DO
      END IF
    END DO
!Apply nested-grid BCs
!           if ( gridstruct%nested ) then
!              step  = real(neststruct%tracer_nest_timestep)
!              split = real(nsplt*k_split)
!              do iq=1,nq
!
!
!                 call nested_grid_BC_apply_intT(q(isd:ied,jsd:jed,:,iq), &
!                      0, 0, npx, npy, npz, bd, &
!                      step, split, &
!                      !0, 0, npx, npy, npz, real(tracer_nest_timestep)+real(nsplt), real(nsplt), &
!                      var_east_t0=neststruct%q_BC(iq)%east_t0, &
!                      var_west_t0=neststruct%q_BC(iq)%west_t0, &
!                      var_north_t0=neststruct%q_BC(iq)%north_t0, &
!                      var_south_t0=neststruct%q_BC(iq)%south_t0, &
!                      var_east_t1=neststruct%q_BC(iq)%east_t1, &
!                      var_west_t1=neststruct%q_BC(iq)%west_t1, &
!                      var_north_t1=neststruct%q_BC(iq)%north_t1, &
!                      var_south_t1=neststruct%q_BC(iq)%south_t1, &
!                      bctype=neststruct%nestbctype, &
!                      nsponge=neststruct%nsponge, s_weight=neststruct%s_weight   )
!
!              end do
!           end if
! nsplt
    IF (id_divg .GT. 0) THEN
      rdt = 1./(frac*dt)
!$OMP parallel do default(none) shared(is,ie,js,je,npz,dp1,xfx,yfx,rarea,rdt)
      DO k=1,npz
        DO j=js,je
          DO i=is,ie
            dp1(i, j, k) = (xfx(i+1, j, k)-xfx(i, j, k)+yfx(i, j+1, k)-&
&             yfx(i, j, k))*gridstruct%rarea(i, j)*rdt
          END DO
        END DO
      END DO
    END IF
  END SUBROUTINE TRACER_2D_NESTED
END MODULE FV_TRACER2D_MOD_B
